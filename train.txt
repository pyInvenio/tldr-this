This program takes in a Code segment and coding Language and outputs the Code Tokens and Docstring for the code:

Code:def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    """\n    Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        ├── <person1>/\n        │   ├── <somename1>.jpeg\n        │   ├── <somename2>.jpeg\n        │   ├── ...\n        ├── <person2>/\n        │   ├── <somename1>.jpeg\n        │   └── <somename2>.jpeg\n        └── ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.\n    """\n    X = []\n    y = []\n\n    # Loop through each person in the training set\n    for class_dir in os.listdir(train_dir):\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n            continue\n\n        # Loop through each training image for the current person\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n\n            if len(face_bounding_boxes) != 1:\n                # If there are no people (or too many people) in a training image, skip the image.\n                if verbose:\n                    print("Image {} not suitable for training: {}".format(img_path, "Didn't find a face" if len(face_bounding_boxes) < 1 else "Found more than one face"))\n            else:\n                # Add face encoding for current image to the training set\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n                y.append(class_dir)\n\n    # Determine how many neighbors to use for weighting in the KNN classifier\n    if n_neighbors is None:\n        n_neighbors = int(round(math.sqrt(len(X))))\n        if verbose:\n            print("Chose n_neighbors automatically:", n_neighbors)\n\n    # Create and train the KNN classifier\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n    knn_clf.fit(X, y)\n\n    # Save the trained KNN classifier\n    if model_save_path is not None:\n        with open(model_save_path, 'wb') as f:\n            pickle.dump(knn_clf, f)\n\n    return knn_clf
Language: python
Code Tokens: ['def', 'train', '(', 'train_dir', ',', 'model_save_path', '=', 'None', ',', 'n_neighbors', '=', 'None', ',', 'knn_algo', '=', "'ball_tree'", ',', 'verbose', '=', 'False', ')', ':', 'X', '=', '[', ']', 'y', '=', '[', ']', '# Loop through each person in the training set', 'for', 'class_dir', 'in', 'os', '.', 'listdir', '(', 'train_dir', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'os', '.', 'path', '.', 'join', '(', 'train_dir', ',', 'class_dir', ')', ')', ':', 'continue', '# Loop through each training image for the current person', 'for', 'img_path', 'in', 'image_files_in_folder', '(', 'os', '.', 'path', '.', 'join', '(', 'train_dir', ',', 'class_dir', ')', ')', ':', 'image', '=', 'face_recognition', '.', 'load_image_file', '(', 'img_path', ')', 'face_bounding_boxes', '=', 'face_recognition', '.', 'face_locations', '(', 'image', ')', 'if', 'len', '(', 'face_bounding_boxes', ')', '!=', '1', ':', '# If there are no people (or too many people) in a training image, skip the image.', 'if', 'verbose', ':', 'print', '(', '"Image {} not suitable for training: {}"', '.', 'format', '(', 'img_path', ',', '"Didn\'t find a face"', 'if', 'len', '(', 'face_bounding_boxes', ')', '<', '1', 'else', '"Found more than one face"', ')', ')', 'else', ':', '# Add face encoding for current image to the training set', 'X', '.', 'append', '(', 'face_recognition', '.', 'face_encodings', '(', 'image', ',', 'known_face_locations', '=', 'face_bounding_boxes', ')', '[', '0', ']', ')', 'y', '.', 'append', '(', 'class_dir', ')', '# Determine how many neighbors to use for weighting in the KNN classifier', 'if', 'n_neighbors', 'is', 'None', ':', 'n_neighbors', '=', 'int', '(', 'round', '(', 'math', '.', 'sqrt', '(', 'len', '(', 'X', ')', ')', ')', ')', 'if', 'verbose', ':', 'print', '(', '"Chose n_neighbors automatically:"', ',', 'n_neighbors', ')', '# Create and train the KNN classifier', 'knn_clf', '=', 'neighbors', '.', 'KNeighborsClassifier', '(', 'n_neighbors', '=', 'n_neighbors', ',', 'algorithm', '=', 'knn_algo', ',', 'weights', '=', "'distance'", ')', 'knn_clf', '.', 'fit', '(', 'X', ',', 'y', ')', '# Save the trained KNN classifier', 'if', 'model_save_path', 'is', 'not', 'None', ':', 'with', 'open', '(', 'model_save_path', ',', "'wb'", ')', 'as', 'f', ':', 'pickle', '.', 'dump', '(', 'knn_clf', ',', 'f', ')', 'return', 'knn_clf']
Docstring: Trains a k-nearest neighbors classifier for face recognition.    :param train_dir: directory that contains a sub-directory for each known person, with its name.     (View in source code to see train_dir example tree structure)     Structure:        <train_dir>/        ├── <person1>/        │   ├── <somename1>.jpeg        │   ├── <somename2>.jpeg        │   ├── ...        ├── <person2>/        │   ├── <somename1>.jpeg        │   └── <somename2>.jpeg        └── ...    :param model_save_path: (optional) path to save model on disk    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree    :param verbose: verbosity of training    :return: returns knn classifier that was trained on the given data.
*******__*******
Code:def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n    """\n    Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.\n    """\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n        raise Exception("Invalid image path: {}".format(X_img_path))\n\n    if knn_clf is None and model_path is None:\n        raise Exception("Must supply knn classifier either thourgh knn_clf or model_path")\n\n    # Load a trained KNN model (if one was passed in)\n    if knn_clf is None:\n        with open(model_path, 'rb') as f:\n            knn_clf = pickle.load(f)\n\n    # Load image file and find face locations\n    X_img = face_recognition.load_image_file(X_img_path)\n    X_face_locations = face_recognition.face_locations(X_img)\n\n    # If no faces are found in the image, return an empty result.\n    if len(X_face_locations) == 0:\n        return []\n\n    # Find encodings for faces in the test iamge\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n\n    # Use the KNN model to find the best matches for the test face\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n\n    # Predict classes and remove classifications that aren't within the threshold\n    return [(pred, loc) if rec else ("unknown", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]
Language: python
Code Tokens: ['def', 'predict', '(', 'X_img_path', ',', 'knn_clf', '=', 'None', ',', 'model_path', '=', 'None', ',', 'distance_threshold', '=', '0.6', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'X_img_path', ')', 'or', 'os', '.', 'path', '.', 'splitext', '(', 'X_img_path', ')', '[', '1', ']', '[', '1', ':', ']', 'not', 'in', 'ALLOWED_EXTENSIONS', ':', 'raise', 'Exception', '(', '"Invalid image path: {}"', '.', 'format', '(', 'X_img_path', ')', ')', 'if', 'knn_clf', 'is', 'None', 'and', 'model_path', 'is', 'None', ':', 'raise', 'Exception', '(', '"Must supply knn classifier either thourgh knn_clf or model_path"', ')', '# Load a trained KNN model (if one was passed in)', 'if', 'knn_clf', 'is', 'None', ':', 'with', 'open', '(', 'model_path', ',', "'rb'", ')', 'as', 'f', ':', 'knn_clf', '=', 'pickle', '.', 'load', '(', 'f', ')', '# Load image file and find face locations', 'X_img', '=', 'face_recognition', '.', 'load_image_file', '(', 'X_img_path', ')', 'X_face_locations', '=', 'face_recognition', '.', 'face_locations', '(', 'X_img', ')', '# If no faces are found in the image, return an empty result.', 'if', 'len', '(', 'X_face_locations', ')', '==', '0', ':', 'return', '[', ']', '# Find encodings for faces in the test iamge', 'faces_encodings', '=', 'face_recognition', '.', 'face_encodings', '(', 'X_img', ',', 'known_face_locations', '=', 'X_face_locations', ')', '# Use the KNN model to find the best matches for the test face', 'closest_distances', '=', 'knn_clf', '.', 'kneighbors', '(', 'faces_encodings', ',', 'n_neighbors', '=', '1', ')', 'are_matches', '=', '[', 'closest_distances', '[', '0', ']', '[', 'i', ']', '[', '0', ']', '<=', 'distance_threshold', 'for', 'i', 'in', 'range', '(', 'len', '(', 'X_face_locations', ')', ')', ']', "# Predict classes and remove classifications that aren't within the threshold", 'return', '[', '(', 'pred', ',', 'loc', ')', 'if', 'rec', 'else', '(', '"unknown"', ',', 'loc', ')', 'for', 'pred', ',', 'loc', ',', 'rec', 'in', 'zip', '(', 'knn_clf', '.', 'predict', '(', 'faces_encodings', ')', ',', 'X_face_locations', ',', 'are_matches', ')', ']']
Docstring: Recognizes faces in given image using a trained KNN classifier    :param X_img_path: path to image to be recognized    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance           of mis-classifying an unknown person as a known one.    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].        For faces of unrecognized persons, the name 'unknown' will be returned.
*******__*******
Code:def show_prediction_labels_on_image(img_path, predictions):\n    """\n    Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:\n    """\n    pil_image = Image.open(img_path).convert("RGB")\n    draw = ImageDraw.Draw(pil_image)\n\n    for name, (top, right, bottom, left) in predictions:\n        # Draw a box around the face using the Pillow module\n        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n        # There's a bug in Pillow where it blows up with non-UTF-8 text\n        # when using the default bitmap font\n        name = name.encode("UTF-8")\n\n        # Draw a label with a name below the face\n        text_width, text_height = draw.textsize(name)\n        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n    # Remove the drawing library from memory as per the Pillow docs\n    del draw\n\n    # Display the resulting image\n    pil_image.show()
Language: python
Code Tokens: ['def', 'show_prediction_labels_on_image', '(', 'img_path', ',', 'predictions', ')', ':', 'pil_image', '=', 'Image', '.', 'open', '(', 'img_path', ')', '.', 'convert', '(', '"RGB"', ')', 'draw', '=', 'ImageDraw', '.', 'Draw', '(', 'pil_image', ')', 'for', 'name', ',', '(', 'top', ',', 'right', ',', 'bottom', ',', 'left', ')', 'in', 'predictions', ':', '# Draw a box around the face using the Pillow module', 'draw', '.', 'rectangle', '(', '(', '(', 'left', ',', 'top', ')', ',', '(', 'right', ',', 'bottom', ')', ')', ',', 'outline', '=', '(', '0', ',', '0', ',', '255', ')', ')', "# There's a bug in Pillow where it blows up with non-UTF-8 text", '# when using the default bitmap font', 'name', '=', 'name', '.', 'encode', '(', '"UTF-8"', ')', '# Draw a label with a name below the face', 'text_width', ',', 'text_height', '=', 'draw', '.', 'textsize', '(', 'name', ')', 'draw', '.', 'rectangle', '(', '(', '(', 'left', ',', 'bottom', '-', 'text_height', '-', '10', ')', ',', '(', 'right', ',', 'bottom', ')', ')', ',', 'fill', '=', '(', '0', ',', '0', ',', '255', ')', ',', 'outline', '=', '(', '0', ',', '0', ',', '255', ')', ')', 'draw', '.', 'text', '(', '(', 'left', '+', '6', ',', 'bottom', '-', 'text_height', '-', '5', ')', ',', 'name', ',', 'fill', '=', '(', '255', ',', '255', ',', '255', ',', '255', ')', ')', '# Remove the drawing library from memory as per the Pillow docs', 'del', 'draw', '# Display the resulting image', 'pil_image', '.', 'show', '(', ')']
Docstring: Shows the face recognition results visually.    :param img_path: path to image to be recognized    :param predictions: results of the predict function    :return:
*******__*******
Code:def _rect_to_css(rect):\n    """\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n    """\n    return rect.top(), rect.right(), rect.bottom(), rect.left()
Language: python
Code Tokens: ['def', '_rect_to_css', '(', 'rect', ')', ':', 'return', 'rect', '.', 'top', '(', ')', ',', 'rect', '.', 'right', '(', ')', ',', 'rect', '.', 'bottom', '(', ')', ',', 'rect', '.', 'left', '(', ')']
Docstring: Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order    :param rect: a dlib 'rect' object    :return: a plain tuple representation of the rect in (top, right, bottom, left) order
*******__*******
Code:def _trim_css_to_bounds(css, image_shape):\n    """\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n    """\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)
Language: python
Code Tokens: ['def', '_trim_css_to_bounds', '(', 'css', ',', 'image_shape', ')', ':', 'return', 'max', '(', 'css', '[', '0', ']', ',', '0', ')', ',', 'min', '(', 'css', '[', '1', ']', ',', 'image_shape', '[', '1', ']', ')', ',', 'min', '(', 'css', '[', '2', ']', ',', 'image_shape', '[', '0', ']', ')', ',', 'max', '(', 'css', '[', '3', ']', ',', '0', ')']
Docstring: Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order    :param image_shape: numpy shape of the image array    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order
*******__*******
Code:def face_distance(face_encodings, face_to_compare):\n    """\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n    """\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)
Language: python
Code Tokens: ['def', 'face_distance', '(', 'face_encodings', ',', 'face_to_compare', ')', ':', 'if', 'len', '(', 'face_encodings', ')', '==', '0', ':', 'return', 'np', '.', 'empty', '(', '(', '0', ')', ')', 'return', 'np', '.', 'linalg', '.', 'norm', '(', 'face_encodings', '-', 'face_to_compare', ',', 'axis', '=', '1', ')']
Docstring: Given a list of face encodings, compare them to a known face encoding and get a euclidean distance    for each comparison face. The distance tells you how similar the faces are.    :param faces: List of face encodings to compare    :param face_to_compare: A face encoding to compare against    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array
*******__*******
Code:def load_image_file(file, mode='RGB'):\n    """\n    Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array\n    """\n    im = PIL.Image.open(file)\n    if mode:\n        im = im.convert(mode)\n    return np.array(im)
Language: python
Code Tokens: ['def', 'load_image_file', '(', 'file', ',', 'mode', '=', "'RGB'", ')', ':', 'im', '=', 'PIL', '.', 'Image', '.', 'open', '(', 'file', ')', 'if', 'mode', ':', 'im', '=', 'im', '.', 'convert', '(', 'mode', ')', 'return', 'np', '.', 'array', '(', 'im', ')']
Docstring: Loads an image file (.jpg, .png, etc) into a numpy array    :param file: image file name or file object to load    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.    :return: image contents as numpy array
*******__*******
Code:def _raw_face_locations(img, number_of_times_to_upsample=1, model="hog"):\n    """\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".\n    :return: A list of dlib 'rect' objects of found face locations\n    """\n    if model == "cnn":\n        return cnn_face_detector(img, number_of_times_to_upsample)\n    else:\n        return face_detector(img, number_of_times_to_upsample)
Language: python
Code Tokens: ['def', '_raw_face_locations', '(', 'img', ',', 'number_of_times_to_upsample', '=', '1', ',', 'model', '=', '"hog"', ')', ':', 'if', 'model', '==', '"cnn"', ':', 'return', 'cnn_face_detector', '(', 'img', ',', 'number_of_times_to_upsample', ')', 'else', ':', 'return', 'face_detector', '(', 'img', ',', 'number_of_times_to_upsample', ')']
Docstring: Returns an array of bounding boxes of human faces in a image    :param img: An image (as a numpy array)    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".    :return: A list of dlib 'rect' objects of found face locations
*******__*******
Code:def face_locations(img, number_of_times_to_upsample=1, model="hog"):\n    """\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    """\n    if model == "cnn":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, "cnn")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]
Language: python
Code Tokens: ['def', 'face_locations', '(', 'img', ',', 'number_of_times_to_upsample', '=', '1', ',', 'model', '=', '"hog"', ')', ':', 'if', 'model', '==', '"cnn"', ':', 'return', '[', '_trim_css_to_bounds', '(', '_rect_to_css', '(', 'face', '.', 'rect', ')', ',', 'img', '.', 'shape', ')', 'for', 'face', 'in', '_raw_face_locations', '(', 'img', ',', 'number_of_times_to_upsample', ',', '"cnn"', ')', ']', 'else', ':', 'return', '[', '_trim_css_to_bounds', '(', '_rect_to_css', '(', 'face', ')', ',', 'img', '.', 'shape', ')', 'for', 'face', 'in', '_raw_face_locations', '(', 'img', ',', 'number_of_times_to_upsample', ',', 'model', ')', ']']
Docstring: Returns an array of bounding boxes of human faces in a image    :param img: An image (as a numpy array)    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.    :param model: Which face detection model to use. "hog" is less accurate but faster on CPUs. "cnn" is a more accurate                  deep-learning model which is GPU/CUDA accelerated (if available). The default is "hog".    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
*******__*******
Code:def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n    """\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param img: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    """\n    def convert_cnn_detections_to_css(detections):\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))
Language: python
Code Tokens: ['def', 'batch_face_locations', '(', 'images', ',', 'number_of_times_to_upsample', '=', '1', ',', 'batch_size', '=', '128', ')', ':', 'def', 'convert_cnn_detections_to_css', '(', 'detections', ')', ':', 'return', '[', '_trim_css_to_bounds', '(', '_rect_to_css', '(', 'face', '.', 'rect', ')', ',', 'images', '[', '0', ']', '.', 'shape', ')', 'for', 'face', 'in', 'detections', ']', 'raw_detections_batched', '=', '_raw_face_locations_batched', '(', 'images', ',', 'number_of_times_to_upsample', ',', 'batch_size', ')', 'return', 'list', '(', 'map', '(', 'convert_cnn_detections_to_css', ',', 'raw_detections_batched', ')', ')']
Docstring: Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector    If you are using a GPU, this can give you much faster results since the GPU    can process batches of images at once. If you aren't using a GPU, you don't need this function.    :param img: A list of images (each as a numpy array)    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.    :param batch_size: How many images to include in each GPU processing batch.    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
*******__*******
Code:def face_landmarks(face_image, face_locations=None, model="large"):\n    """\n    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. "large" (default) or "small" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)\n    """\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n    if model == 'large':\n        return [{\n            "chin": points[0:17],\n            "left_eyebrow": points[17:22],\n            "right_eyebrow": points[22:27],\n            "nose_bridge": points[27:31],\n            "nose_tip": points[31:36],\n            "left_eye": points[36:42],\n            "right_eye": points[42:48],\n            "top_lip": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n            "bottom_lip": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n        } for points in landmarks_as_tuples]\n    elif model == 'small':\n        return [{\n            "nose_tip": [points[4]],\n            "left_eye": points[2:4],\n            "right_eye": points[0:2],\n        } for points in landmarks_as_tuples]\n    else:\n        raise ValueError("Invalid landmarks model type. Supported models are ['small', 'large'].")
Language: python
Code Tokens: ['def', 'face_landmarks', '(', 'face_image', ',', 'face_locations', '=', 'None', ',', 'model', '=', '"large"', ')', ':', 'landmarks', '=', '_raw_face_landmarks', '(', 'face_image', ',', 'face_locations', ',', 'model', ')', 'landmarks_as_tuples', '=', '[', '[', '(', 'p', '.', 'x', ',', 'p', '.', 'y', ')', 'for', 'p', 'in', 'landmark', '.', 'parts', '(', ')', ']', 'for', 'landmark', 'in', 'landmarks', ']', '# For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png', 'if', 'model', '==', "'large'", ':', 'return', '[', '{', '"chin"', ':', 'points', '[', '0', ':', '17', ']', ',', '"left_eyebrow"', ':', 'points', '[', '17', ':', '22', ']', ',', '"right_eyebrow"', ':', 'points', '[', '22', ':', '27', ']', ',', '"nose_bridge"', ':', 'points', '[', '27', ':', '31', ']', ',', '"nose_tip"', ':', 'points', '[', '31', ':', '36', ']', ',', '"left_eye"', ':', 'points', '[', '36', ':', '42', ']', ',', '"right_eye"', ':', 'points', '[', '42', ':', '48', ']', ',', '"top_lip"', ':', 'points', '[', '48', ':', '55', ']', '+', '[', 'points', '[', '64', ']', ']', '+', '[', 'points', '[', '63', ']', ']', '+', '[', 'points', '[', '62', ']', ']', '+', '[', 'points', '[', '61', ']', ']', '+', '[', 'points', '[', '60', ']', ']', ',', '"bottom_lip"', ':', 'points', '[', '54', ':', '60', ']', '+', '[', 'points', '[', '48', ']', ']', '+', '[', 'points', '[', '60', ']', ']', '+', '[', 'points', '[', '67', ']', ']', '+', '[', 'points', '[', '66', ']', ']', '+', '[', 'points', '[', '65', ']', ']', '+', '[', 'points', '[', '64', ']', ']', '}', 'for', 'points', 'in', 'landmarks_as_tuples', ']', 'elif', 'model', '==', "'small'", ':', 'return', '[', '{', '"nose_tip"', ':', '[', 'points', '[', '4', ']', ']', ',', '"left_eye"', ':', 'points', '[', '2', ':', '4', ']', ',', '"right_eye"', ':', 'points', '[', '0', ':', '2', ']', ',', '}', 'for', 'points', 'in', 'landmarks_as_tuples', ']', 'else', ':', 'raise', 'ValueError', '(', '"Invalid landmarks model type. Supported models are [\'small\', \'large\']."', ')']
Docstring: Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image    :param face_image: image to search    :param face_locations: Optionally provide a list of face locations to check.    :param model: Optional - which model to use. "large" (default) or "small" which only returns 5 points but is faster.    :return: A list of dicts of face feature locations (eyes, nose, etc)
*******__*******
Code:def face_encodings(face_image, known_face_locations=None, num_jitters=1):\n    """\n    Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :return: A list of 128-dimensional face encodings (one for each face in the image)\n    """\n    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model="small")\n    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]
Language: python
Code Tokens: ['def', 'face_encodings', '(', 'face_image', ',', 'known_face_locations', '=', 'None', ',', 'num_jitters', '=', '1', ')', ':', 'raw_landmarks', '=', '_raw_face_landmarks', '(', 'face_image', ',', 'known_face_locations', ',', 'model', '=', '"small"', ')', 'return', '[', 'np', '.', 'array', '(', 'face_encoder', '.', 'compute_face_descriptor', '(', 'face_image', ',', 'raw_landmark_set', ',', 'num_jitters', ')', ')', 'for', 'raw_landmark_set', 'in', 'raw_landmarks', ']']
Docstring: Given an image, return the 128-dimension face encoding for each face in the image.    :param face_image: The image that contains one or more faces    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)    :return: A list of 128-dimensional face encodings (one for each face in the image)
*******__*******
Code:def _parse_datatype_string(s):\n    """\n    Parses the given data type string to a :class:`DataType`. The data type string format equals\n    to :class:`DataType.simpleString`, except that top level struct type can omit\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\n    string and case-insensitive strings.\n\n    >>> _parse_datatype_string("int ")\n    IntegerType\n    >>> _parse_datatype_string("INT ")\n    IntegerType\n    >>> _parse_datatype_string("a: byte, b: decimal(  16 , 8   ) ")\n    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))\n    >>> _parse_datatype_string("a DOUBLE, b STRING")\n    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))\n    >>> _parse_datatype_string("a: array< short>")\n    StructType(List(StructField(a,ArrayType(ShortType,true),true)))\n    >>> _parse_datatype_string(" map<string , string > ")\n    MapType(StringType,StringType,true)\n\n    >>> # Error cases\n    >>> _parse_datatype_string("blabla") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string("a: int,") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string("array<int") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    >>> _parse_datatype_string("map<int, boolean>>") # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ParseException:...\n    """\n    sc = SparkContext._active_spark_context\n\n    def from_ddl_schema(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n\n    def from_ddl_datatype(type_str):\n        return _parse_datatype_json_string(\n            sc._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.parseDataType(type_str).json())\n\n    try:\n        # DDL format, "fieldname datatype, fieldname datatype".\n        return from_ddl_schema(s)\n    except Exception as e:\n        try:\n            # For backwards compatibility, "integer", "struct<fieldname: datatype>" and etc.\n            return from_ddl_datatype(s)\n        except:\n            try:\n                # For backwards compatibility, "fieldname: datatype, fieldname: datatype" case.\n                return from_ddl_datatype("struct<%s>" % s.strip())\n            except:\n                raise e
Language: python
Code Tokens: ['def', '_parse_datatype_string', '(', 's', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'def', 'from_ddl_schema', '(', 'type_str', ')', ':', 'return', '_parse_datatype_json_string', '(', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'sql', '.', 'types', '.', 'StructType', '.', 'fromDDL', '(', 'type_str', ')', '.', 'json', '(', ')', ')', 'def', 'from_ddl_datatype', '(', 'type_str', ')', ':', 'return', '_parse_datatype_json_string', '(', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'sql', '.', 'api', '.', 'python', '.', 'PythonSQLUtils', '.', 'parseDataType', '(', 'type_str', ')', '.', 'json', '(', ')', ')', 'try', ':', '# DDL format, "fieldname datatype, fieldname datatype".', 'return', 'from_ddl_schema', '(', 's', ')', 'except', 'Exception', 'as', 'e', ':', 'try', ':', '# For backwards compatibility, "integer", "struct<fieldname: datatype>" and etc.', 'return', 'from_ddl_datatype', '(', 's', ')', 'except', ':', 'try', ':', '# For backwards compatibility, "fieldname: datatype, fieldname: datatype" case.', 'return', 'from_ddl_datatype', '(', '"struct<%s>"', '%', 's', '.', 'strip', '(', ')', ')', 'except', ':', 'raise', 'e']
Docstring: Parses the given data type string to a :class:`DataType`. The data type string format equals    to :class:`DataType.simpleString`, except that top level struct type can omit    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted    string and case-insensitive strings.    >>> _parse_datatype_string("int ")    IntegerType    >>> _parse_datatype_string("INT ")    IntegerType    >>> _parse_datatype_string("a: byte, b: decimal(  16 , 8   ) ")    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))    >>> _parse_datatype_string("a DOUBLE, b STRING")    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))    >>> _parse_datatype_string("a: array< short>")    StructType(List(StructField(a,ArrayType(ShortType,true),true)))    >>> _parse_datatype_string(" map<string , string > ")    MapType(StringType,StringType,true)    >>> # Error cases    >>> _parse_datatype_string("blabla") # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ParseException:...    >>> _parse_datatype_string("a: int,") # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ParseException:...    >>> _parse_datatype_string("array<int") # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ParseException:...    >>> _parse_datatype_string("map<int, boolean>>") # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ParseException:...
*******__*******
Code:def _int_size_to_type(size):\n    """\n    Return the Catalyst datatype from the size of integers.\n    """\n    if size <= 8:\n        return ByteType\n    if size <= 16:\n        return ShortType\n    if size <= 32:\n        return IntegerType\n    if size <= 64:\n        return LongType
Language: python
Code Tokens: ['def', '_int_size_to_type', '(', 'size', ')', ':', 'if', 'size', '<=', '8', ':', 'return', 'ByteType', 'if', 'size', '<=', '16', ':', 'return', 'ShortType', 'if', 'size', '<=', '32', ':', 'return', 'IntegerType', 'if', 'size', '<=', '64', ':', 'return', 'LongType']
Docstring: Return the Catalyst datatype from the size of integers.
*******__*******
Code:def _infer_type(obj):\n    """Infer the DataType from obj\n    """\n    if obj is None:\n        return NullType()\n\n    if hasattr(obj, '__UDT__'):\n        return obj.__UDT__\n\n    dataType = _type_mappings.get(type(obj))\n    if dataType is DecimalType:\n        # the precision and scale of `obj` may be different from row to row.\n        return DecimalType(38, 18)\n    elif dataType is not None:\n        return dataType()\n\n    if isinstance(obj, dict):\n        for key, value in obj.items():\n            if key is not None and value is not None:\n                return MapType(_infer_type(key), _infer_type(value), True)\n        return MapType(NullType(), NullType(), True)\n    elif isinstance(obj, list):\n        for v in obj:\n            if v is not None:\n                return ArrayType(_infer_type(obj[0]), True)\n        return ArrayType(NullType(), True)\n    elif isinstance(obj, array):\n        if obj.typecode in _array_type_mappings:\n            return ArrayType(_array_type_mappings[obj.typecode](), False)\n        else:\n            raise TypeError("not supported type: array(%s)" % obj.typecode)\n    else:\n        try:\n            return _infer_schema(obj)\n        except TypeError:\n            raise TypeError("not supported type: %s" % type(obj))
Language: python
Code Tokens: ['def', '_infer_type', '(', 'obj', ')', ':', 'if', 'obj', 'is', 'None', ':', 'return', 'NullType', '(', ')', 'if', 'hasattr', '(', 'obj', ',', "'__UDT__'", ')', ':', 'return', 'obj', '.', '__UDT__', 'dataType', '=', '_type_mappings', '.', 'get', '(', 'type', '(', 'obj', ')', ')', 'if', 'dataType', 'is', 'DecimalType', ':', '# the precision and scale of `obj` may be different from row to row.', 'return', 'DecimalType', '(', '38', ',', '18', ')', 'elif', 'dataType', 'is', 'not', 'None', ':', 'return', 'dataType', '(', ')', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'for', 'key', ',', 'value', 'in', 'obj', '.', 'items', '(', ')', ':', 'if', 'key', 'is', 'not', 'None', 'and', 'value', 'is', 'not', 'None', ':', 'return', 'MapType', '(', '_infer_type', '(', 'key', ')', ',', '_infer_type', '(', 'value', ')', ',', 'True', ')', 'return', 'MapType', '(', 'NullType', '(', ')', ',', 'NullType', '(', ')', ',', 'True', ')', 'elif', 'isinstance', '(', 'obj', ',', 'list', ')', ':', 'for', 'v', 'in', 'obj', ':', 'if', 'v', 'is', 'not', 'None', ':', 'return', 'ArrayType', '(', '_infer_type', '(', 'obj', '[', '0', ']', ')', ',', 'True', ')', 'return', 'ArrayType', '(', 'NullType', '(', ')', ',', 'True', ')', 'elif', 'isinstance', '(', 'obj', ',', 'array', ')', ':', 'if', 'obj', '.', 'typecode', 'in', '_array_type_mappings', ':', 'return', 'ArrayType', '(', '_array_type_mappings', '[', 'obj', '.', 'typecode', ']', '(', ')', ',', 'False', ')', 'else', ':', 'raise', 'TypeError', '(', '"not supported type: array(%s)"', '%', 'obj', '.', 'typecode', ')', 'else', ':', 'try', ':', 'return', '_infer_schema', '(', 'obj', ')', 'except', 'TypeError', ':', 'raise', 'TypeError', '(', '"not supported type: %s"', '%', 'type', '(', 'obj', ')', ')']
Docstring: Infer the DataType from obj
*******__*******
Code:def _infer_schema(row, names=None):\n    """Infer the schema from dict/namedtuple/object"""\n    if isinstance(row, dict):\n        items = sorted(row.items())\n\n    elif isinstance(row, (tuple, list)):\n        if hasattr(row, "__fields__"):  # Row\n            items = zip(row.__fields__, tuple(row))\n        elif hasattr(row, "_fields"):  # namedtuple\n            items = zip(row._fields, tuple(row))\n        else:\n            if names is None:\n                names = ['_%d' % i for i in range(1, len(row) + 1)]\n            elif len(names) < len(row):\n                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))\n            items = zip(names, row)\n\n    elif hasattr(row, "__dict__"):  # object\n        items = sorted(row.__dict__.items())\n\n    else:\n        raise TypeError("Can not infer schema for type: %s" % type(row))\n\n    fields = [StructField(k, _infer_type(v), True) for k, v in items]\n    return StructType(fields)
Language: python
Code Tokens: ['def', '_infer_schema', '(', 'row', ',', 'names', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'row', ',', 'dict', ')', ':', 'items', '=', 'sorted', '(', 'row', '.', 'items', '(', ')', ')', 'elif', 'isinstance', '(', 'row', ',', '(', 'tuple', ',', 'list', ')', ')', ':', 'if', 'hasattr', '(', 'row', ',', '"__fields__"', ')', ':', '# Row', 'items', '=', 'zip', '(', 'row', '.', '__fields__', ',', 'tuple', '(', 'row', ')', ')', 'elif', 'hasattr', '(', 'row', ',', '"_fields"', ')', ':', '# namedtuple', 'items', '=', 'zip', '(', 'row', '.', '_fields', ',', 'tuple', '(', 'row', ')', ')', 'else', ':', 'if', 'names', 'is', 'None', ':', 'names', '=', '[', "'_%d'", '%', 'i', 'for', 'i', 'in', 'range', '(', '1', ',', 'len', '(', 'row', ')', '+', '1', ')', ']', 'elif', 'len', '(', 'names', ')', '<', 'len', '(', 'row', ')', ':', 'names', '.', 'extend', '(', "'_%d'", '%', 'i', 'for', 'i', 'in', 'range', '(', 'len', '(', 'names', ')', '+', '1', ',', 'len', '(', 'row', ')', '+', '1', ')', ')', 'items', '=', 'zip', '(', 'names', ',', 'row', ')', 'elif', 'hasattr', '(', 'row', ',', '"__dict__"', ')', ':', '# object', 'items', '=', 'sorted', '(', 'row', '.', '__dict__', '.', 'items', '(', ')', ')', 'else', ':', 'raise', 'TypeError', '(', '"Can not infer schema for type: %s"', '%', 'type', '(', 'row', ')', ')', 'fields', '=', '[', 'StructField', '(', 'k', ',', '_infer_type', '(', 'v', ')', ',', 'True', ')', 'for', 'k', ',', 'v', 'in', 'items', ']', 'return', 'StructType', '(', 'fields', ')']
Docstring: Infer the schema from dict/namedtuple/object
*******__*******
Code:def _has_nulltype(dt):\n    """ Return whether there is NullType in `dt` or not """\n    if isinstance(dt, StructType):\n        return any(_has_nulltype(f.dataType) for f in dt.fields)\n    elif isinstance(dt, ArrayType):\n        return _has_nulltype((dt.elementType))\n    elif isinstance(dt, MapType):\n        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)\n    else:\n        return isinstance(dt, NullType)
Language: python
Code Tokens: ['def', '_has_nulltype', '(', 'dt', ')', ':', 'if', 'isinstance', '(', 'dt', ',', 'StructType', ')', ':', 'return', 'any', '(', '_has_nulltype', '(', 'f', '.', 'dataType', ')', 'for', 'f', 'in', 'dt', '.', 'fields', ')', 'elif', 'isinstance', '(', 'dt', ',', 'ArrayType', ')', ':', 'return', '_has_nulltype', '(', '(', 'dt', '.', 'elementType', ')', ')', 'elif', 'isinstance', '(', 'dt', ',', 'MapType', ')', ':', 'return', '_has_nulltype', '(', 'dt', '.', 'keyType', ')', 'or', '_has_nulltype', '(', 'dt', '.', 'valueType', ')', 'else', ':', 'return', 'isinstance', '(', 'dt', ',', 'NullType', ')']
Docstring: Return whether there is NullType in `dt` or not
*******__*******
Code:def _create_converter(dataType):\n    """Create a converter to drop the names of fields in obj """\n    if not _need_converter(dataType):\n        return lambda x: x\n\n    if isinstance(dataType, ArrayType):\n        conv = _create_converter(dataType.elementType)\n        return lambda row: [conv(v) for v in row]\n\n    elif isinstance(dataType, MapType):\n        kconv = _create_converter(dataType.keyType)\n        vconv = _create_converter(dataType.valueType)\n        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())\n\n    elif isinstance(dataType, NullType):\n        return lambda x: None\n\n    elif not isinstance(dataType, StructType):\n        return lambda x: x\n\n    # dataType must be StructType\n    names = [f.name for f in dataType.fields]\n    converters = [_create_converter(f.dataType) for f in dataType.fields]\n    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)\n\n    def convert_struct(obj):\n        if obj is None:\n            return\n\n        if isinstance(obj, (tuple, list)):\n            if convert_fields:\n                return tuple(conv(v) for v, conv in zip(obj, converters))\n            else:\n                return tuple(obj)\n\n        if isinstance(obj, dict):\n            d = obj\n        elif hasattr(obj, "__dict__"):  # object\n            d = obj.__dict__\n        else:\n            raise TypeError("Unexpected obj type: %s" % type(obj))\n\n        if convert_fields:\n            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])\n        else:\n            return tuple([d.get(name) for name in names])\n\n    return convert_struct
Language: python
Code Tokens: ['def', '_create_converter', '(', 'dataType', ')', ':', 'if', 'not', '_need_converter', '(', 'dataType', ')', ':', 'return', 'lambda', 'x', ':', 'x', 'if', 'isinstance', '(', 'dataType', ',', 'ArrayType', ')', ':', 'conv', '=', '_create_converter', '(', 'dataType', '.', 'elementType', ')', 'return', 'lambda', 'row', ':', '[', 'conv', '(', 'v', ')', 'for', 'v', 'in', 'row', ']', 'elif', 'isinstance', '(', 'dataType', ',', 'MapType', ')', ':', 'kconv', '=', '_create_converter', '(', 'dataType', '.', 'keyType', ')', 'vconv', '=', '_create_converter', '(', 'dataType', '.', 'valueType', ')', 'return', 'lambda', 'row', ':', 'dict', '(', '(', 'kconv', '(', 'k', ')', ',', 'vconv', '(', 'v', ')', ')', 'for', 'k', ',', 'v', 'in', 'row', '.', 'items', '(', ')', ')', 'elif', 'isinstance', '(', 'dataType', ',', 'NullType', ')', ':', 'return', 'lambda', 'x', ':', 'None', 'elif', 'not', 'isinstance', '(', 'dataType', ',', 'StructType', ')', ':', 'return', 'lambda', 'x', ':', 'x', '# dataType must be StructType', 'names', '=', '[', 'f', '.', 'name', 'for', 'f', 'in', 'dataType', '.', 'fields', ']', 'converters', '=', '[', '_create_converter', '(', 'f', '.', 'dataType', ')', 'for', 'f', 'in', 'dataType', '.', 'fields', ']', 'convert_fields', '=', 'any', '(', '_need_converter', '(', 'f', '.', 'dataType', ')', 'for', 'f', 'in', 'dataType', '.', 'fields', ')', 'def', 'convert_struct', '(', 'obj', ')', ':', 'if', 'obj', 'is', 'None', ':', 'return', 'if', 'isinstance', '(', 'obj', ',', '(', 'tuple', ',', 'list', ')', ')', ':', 'if', 'convert_fields', ':', 'return', 'tuple', '(', 'conv', '(', 'v', ')', 'for', 'v', ',', 'conv', 'in', 'zip', '(', 'obj', ',', 'converters', ')', ')', 'else', ':', 'return', 'tuple', '(', 'obj', ')', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'd', '=', 'obj', 'elif', 'hasattr', '(', 'obj', ',', '"__dict__"', ')', ':', '# object', 'd', '=', 'obj', '.', '__dict__', 'else', ':', 'raise', 'TypeError', '(', '"Unexpected obj type: %s"', '%', 'type', '(', 'obj', ')', ')', 'if', 'convert_fields', ':', 'return', 'tuple', '(', '[', 'conv', '(', 'd', '.', 'get', '(', 'name', ')', ')', 'for', 'name', ',', 'conv', 'in', 'zip', '(', 'names', ',', 'converters', ')', ']', ')', 'else', ':', 'return', 'tuple', '(', '[', 'd', '.', 'get', '(', 'name', ')', 'for', 'name', 'in', 'names', ']', ')', 'return', 'convert_struct']
Docstring: Create a converter to drop the names of fields in obj
*******__*******
Code:def _make_type_verifier(dataType, nullable=True, name=None):\n    """\n    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\n    not match.\n\n    This verifier also checks the value of obj against datatype and raises a ValueError if it's not\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\n    not checked, so it will become infinity when cast to Java float if it overflows.\n\n    >>> _make_type_verifier(StructType([]))(None)\n    >>> _make_type_verifier(StringType())("")\n    >>> _make_type_verifier(LongType())(0)\n    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))\n    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})\n    >>> _make_type_verifier(StructType([]))(())\n    >>> _make_type_verifier(StructType([]))([])\n    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> # Check if numeric values are within the allowed range.\n    >>> _make_type_verifier(ByteType())(12)\n    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(\n    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    >>> schema = StructType().add("a", IntegerType()).add("b", StringType(), False)\n    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    ValueError:...\n    """\n\n    if name is None:\n        new_msg = lambda msg: msg\n        new_name = lambda n: "field %s" % n\n    else:\n        new_msg = lambda msg: "%s: %s" % (name, msg)\n        new_name = lambda n: "field %s in %s" % (n, name)\n\n    def verify_nullability(obj):\n        if obj is None:\n            if nullable:\n                return True\n            else:\n                raise ValueError(new_msg("This field is not nullable, but got None"))\n        else:\n            return False\n\n    _type = type(dataType)\n\n    def assert_acceptable_types(obj):\n        assert _type in _acceptable_types, \\n            new_msg("unknown datatype: %s for object %r" % (dataType, obj))\n\n    def verify_acceptable_types(obj):\n        # subclass of them can not be fromInternal in JVM\n        if type(obj) not in _acceptable_types[_type]:\n            raise TypeError(new_msg("%s can not accept object %r in type %s"\n                                    % (dataType, obj, type(obj))))\n\n    if isinstance(dataType, StringType):\n        # StringType can work with any types\n        verify_value = lambda _: _\n\n    elif isinstance(dataType, UserDefinedType):\n        verifier = _make_type_verifier(dataType.sqlType(), name=name)\n\n        def verify_udf(obj):\n            if not (hasattr(obj, '__UDT__') and obj.__UDT__ == dataType):\n                raise ValueError(new_msg("%r is not an instance of type %r" % (obj, dataType)))\n            verifier(dataType.toInternal(obj))\n\n        verify_value = verify_udf\n\n    elif isinstance(dataType, ByteType):\n        def verify_byte(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -128 or obj > 127:\n                raise ValueError(new_msg("object of ByteType out of range, got: %s" % obj))\n\n        verify_value = verify_byte\n\n    elif isinstance(dataType, ShortType):\n        def verify_short(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -32768 or obj > 32767:\n                raise ValueError(new_msg("object of ShortType out of range, got: %s" % obj))\n\n        verify_value = verify_short\n\n    elif isinstance(dataType, IntegerType):\n        def verify_integer(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            if obj < -2147483648 or obj > 2147483647:\n                raise ValueError(\n                    new_msg("object of IntegerType out of range, got: %s" % obj))\n\n        verify_value = verify_integer\n\n    elif isinstance(dataType, ArrayType):\n        element_verifier = _make_type_verifier(\n            dataType.elementType, dataType.containsNull, name="element in array %s" % name)\n\n        def verify_array(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            for i in obj:\n                element_verifier(i)\n\n        verify_value = verify_array\n\n    elif isinstance(dataType, MapType):\n        key_verifier = _make_type_verifier(dataType.keyType, False, name="key of map %s" % name)\n        value_verifier = _make_type_verifier(\n            dataType.valueType, dataType.valueContainsNull, name="value of map %s" % name)\n\n        def verify_map(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n            for k, v in obj.items():\n                key_verifier(k)\n                value_verifier(v)\n\n        verify_value = verify_map\n\n    elif isinstance(dataType, StructType):\n        verifiers = []\n        for f in dataType.fields:\n            verifier = _make_type_verifier(f.dataType, f.nullable, name=new_name(f.name))\n            verifiers.append((f.name, verifier))\n\n        def verify_struct(obj):\n            assert_acceptable_types(obj)\n\n            if isinstance(obj, dict):\n                for f, verifier in verifiers:\n                    verifier(obj.get(f))\n            elif isinstance(obj, Row) and getattr(obj, "__from_dict__", False):\n                # the order in obj could be different than dataType.fields\n                for f, verifier in verifiers:\n                    verifier(obj[f])\n            elif isinstance(obj, (tuple, list)):\n                if len(obj) != len(verifiers):\n                    raise ValueError(\n                        new_msg("Length of object (%d) does not match with "\n                                "length of fields (%d)" % (len(obj), len(verifiers))))\n                for v, (_, verifier) in zip(obj, verifiers):\n                    verifier(v)\n            elif hasattr(obj, "__dict__"):\n                d = obj.__dict__\n                for f, verifier in verifiers:\n                    verifier(d.get(f))\n            else:\n                raise TypeError(new_msg("StructType can not accept object %r in type %s"\n                                        % (obj, type(obj))))\n        verify_value = verify_struct\n\n    else:\n        def verify_default(obj):\n            assert_acceptable_types(obj)\n            verify_acceptable_types(obj)\n\n        verify_value = verify_default\n\n    def verify(obj):\n        if not verify_nullability(obj):\n            verify_value(obj)\n\n    return verify
Language: python
Code Tokens: ['def', '_make_type_verifier', '(', 'dataType', ',', 'nullable', '=', 'True', ',', 'name', '=', 'None', ')', ':', 'if', 'name', 'is', 'None', ':', 'new_msg', '=', 'lambda', 'msg', ':', 'msg', 'new_name', '=', 'lambda', 'n', ':', '"field %s"', '%', 'n', 'else', ':', 'new_msg', '=', 'lambda', 'msg', ':', '"%s: %s"', '%', '(', 'name', ',', 'msg', ')', 'new_name', '=', 'lambda', 'n', ':', '"field %s in %s"', '%', '(', 'n', ',', 'name', ')', 'def', 'verify_nullability', '(', 'obj', ')', ':', 'if', 'obj', 'is', 'None', ':', 'if', 'nullable', ':', 'return', 'True', 'else', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '"This field is not nullable, but got None"', ')', ')', 'else', ':', 'return', 'False', '_type', '=', 'type', '(', 'dataType', ')', 'def', 'assert_acceptable_types', '(', 'obj', ')', ':', 'assert', '_type', 'in', '_acceptable_types', ',', 'new_msg', '(', '"unknown datatype: %s for object %r"', '%', '(', 'dataType', ',', 'obj', ')', ')', 'def', 'verify_acceptable_types', '(', 'obj', ')', ':', '# subclass of them can not be fromInternal in JVM', 'if', 'type', '(', 'obj', ')', 'not', 'in', '_acceptable_types', '[', '_type', ']', ':', 'raise', 'TypeError', '(', 'new_msg', '(', '"%s can not accept object %r in type %s"', '%', '(', 'dataType', ',', 'obj', ',', 'type', '(', 'obj', ')', ')', ')', ')', 'if', 'isinstance', '(', 'dataType', ',', 'StringType', ')', ':', '# StringType can work with any types', 'verify_value', '=', 'lambda', '_', ':', '_', 'elif', 'isinstance', '(', 'dataType', ',', 'UserDefinedType', ')', ':', 'verifier', '=', '_make_type_verifier', '(', 'dataType', '.', 'sqlType', '(', ')', ',', 'name', '=', 'name', ')', 'def', 'verify_udf', '(', 'obj', ')', ':', 'if', 'not', '(', 'hasattr', '(', 'obj', ',', "'__UDT__'", ')', 'and', 'obj', '.', '__UDT__', '==', 'dataType', ')', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '"%r is not an instance of type %r"', '%', '(', 'obj', ',', 'dataType', ')', ')', ')', 'verifier', '(', 'dataType', '.', 'toInternal', '(', 'obj', ')', ')', 'verify_value', '=', 'verify_udf', 'elif', 'isinstance', '(', 'dataType', ',', 'ByteType', ')', ':', 'def', 'verify_byte', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'if', 'obj', '<', '-', '128', 'or', 'obj', '>', '127', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '"object of ByteType out of range, got: %s"', '%', 'obj', ')', ')', 'verify_value', '=', 'verify_byte', 'elif', 'isinstance', '(', 'dataType', ',', 'ShortType', ')', ':', 'def', 'verify_short', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'if', 'obj', '<', '-', '32768', 'or', 'obj', '>', '32767', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '"object of ShortType out of range, got: %s"', '%', 'obj', ')', ')', 'verify_value', '=', 'verify_short', 'elif', 'isinstance', '(', 'dataType', ',', 'IntegerType', ')', ':', 'def', 'verify_integer', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'if', 'obj', '<', '-', '2147483648', 'or', 'obj', '>', '2147483647', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '"object of IntegerType out of range, got: %s"', '%', 'obj', ')', ')', 'verify_value', '=', 'verify_integer', 'elif', 'isinstance', '(', 'dataType', ',', 'ArrayType', ')', ':', 'element_verifier', '=', '_make_type_verifier', '(', 'dataType', '.', 'elementType', ',', 'dataType', '.', 'containsNull', ',', 'name', '=', '"element in array %s"', '%', 'name', ')', 'def', 'verify_array', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'for', 'i', 'in', 'obj', ':', 'element_verifier', '(', 'i', ')', 'verify_value', '=', 'verify_array', 'elif', 'isinstance', '(', 'dataType', ',', 'MapType', ')', ':', 'key_verifier', '=', '_make_type_verifier', '(', 'dataType', '.', 'keyType', ',', 'False', ',', 'name', '=', '"key of map %s"', '%', 'name', ')', 'value_verifier', '=', '_make_type_verifier', '(', 'dataType', '.', 'valueType', ',', 'dataType', '.', 'valueContainsNull', ',', 'name', '=', '"value of map %s"', '%', 'name', ')', 'def', 'verify_map', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', ':', 'key_verifier', '(', 'k', ')', 'value_verifier', '(', 'v', ')', 'verify_value', '=', 'verify_map', 'elif', 'isinstance', '(', 'dataType', ',', 'StructType', ')', ':', 'verifiers', '=', '[', ']', 'for', 'f', 'in', 'dataType', '.', 'fields', ':', 'verifier', '=', '_make_type_verifier', '(', 'f', '.', 'dataType', ',', 'f', '.', 'nullable', ',', 'name', '=', 'new_name', '(', 'f', '.', 'name', ')', ')', 'verifiers', '.', 'append', '(', '(', 'f', '.', 'name', ',', 'verifier', ')', ')', 'def', 'verify_struct', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'for', 'f', ',', 'verifier', 'in', 'verifiers', ':', 'verifier', '(', 'obj', '.', 'get', '(', 'f', ')', ')', 'elif', 'isinstance', '(', 'obj', ',', 'Row', ')', 'and', 'getattr', '(', 'obj', ',', '"__from_dict__"', ',', 'False', ')', ':', '# the order in obj could be different than dataType.fields', 'for', 'f', ',', 'verifier', 'in', 'verifiers', ':', 'verifier', '(', 'obj', '[', 'f', ']', ')', 'elif', 'isinstance', '(', 'obj', ',', '(', 'tuple', ',', 'list', ')', ')', ':', 'if', 'len', '(', 'obj', ')', '!=', 'len', '(', 'verifiers', ')', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '"Length of object (%d) does not match with "', '"length of fields (%d)"', '%', '(', 'len', '(', 'obj', ')', ',', 'len', '(', 'verifiers', ')', ')', ')', ')', 'for', 'v', ',', '(', '_', ',', 'verifier', ')', 'in', 'zip', '(', 'obj', ',', 'verifiers', ')', ':', 'verifier', '(', 'v', ')', 'elif', 'hasattr', '(', 'obj', ',', '"__dict__"', ')', ':', 'd', '=', 'obj', '.', '__dict__', 'for', 'f', ',', 'verifier', 'in', 'verifiers', ':', 'verifier', '(', 'd', '.', 'get', '(', 'f', ')', ')', 'else', ':', 'raise', 'TypeError', '(', 'new_msg', '(', '"StructType can not accept object %r in type %s"', '%', '(', 'obj', ',', 'type', '(', 'obj', ')', ')', ')', ')', 'verify_value', '=', 'verify_struct', 'else', ':', 'def', 'verify_default', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'verify_value', '=', 'verify_default', 'def', 'verify', '(', 'obj', ')', ':', 'if', 'not', 'verify_nullability', '(', 'obj', ')', ':', 'verify_value', '(', 'obj', ')', 'return', 'verify']
Docstring: Make a verifier that checks the type of obj against dataType and raises a TypeError if they do    not match.    This verifier also checks the value of obj against datatype and raises a ValueError if it's not    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is    not checked, so it will become infinity when cast to Java float if it overflows.    >>> _make_type_verifier(StructType([]))(None)    >>> _make_type_verifier(StringType())("")    >>> _make_type_verifier(LongType())(0)    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    TypeError:...    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})    >>> _make_type_verifier(StructType([]))(())    >>> _make_type_verifier(StructType([]))([])    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ValueError:...    >>> # Check if numeric values are within the allowed range.    >>> _make_type_verifier(ByteType())(12)    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ValueError:...    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ValueError:...    >>> _make_type_verifier(    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ValueError:...    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})    Traceback (most recent call last):        ...    ValueError:...    >>> schema = StructType().add("a", IntegerType()).add("b", StringType(), False)    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL    Traceback (most recent call last):        ...    ValueError:...
*******__*******
Code:def to_arrow_type(dt):\n    """ Convert Spark data type to pyarrow type\n    """\n    import pyarrow as pa\n    if type(dt) == BooleanType:\n        arrow_type = pa.bool_()\n    elif type(dt) == ByteType:\n        arrow_type = pa.int8()\n    elif type(dt) == ShortType:\n        arrow_type = pa.int16()\n    elif type(dt) == IntegerType:\n        arrow_type = pa.int32()\n    elif type(dt) == LongType:\n        arrow_type = pa.int64()\n    elif type(dt) == FloatType:\n        arrow_type = pa.float32()\n    elif type(dt) == DoubleType:\n        arrow_type = pa.float64()\n    elif type(dt) == DecimalType:\n        arrow_type = pa.decimal128(dt.precision, dt.scale)\n    elif type(dt) == StringType:\n        arrow_type = pa.string()\n    elif type(dt) == BinaryType:\n        arrow_type = pa.binary()\n    elif type(dt) == DateType:\n        arrow_type = pa.date32()\n    elif type(dt) == TimestampType:\n        # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read\n        arrow_type = pa.timestamp('us', tz='UTC')\n    elif type(dt) == ArrayType:\n        if type(dt.elementType) in [StructType, TimestampType]:\n            raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))\n        arrow_type = pa.list_(to_arrow_type(dt.elementType))\n    elif type(dt) == StructType:\n        if any(type(field.dataType) == StructType for field in dt):\n            raise TypeError("Nested StructType not supported in conversion to Arrow")\n        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)\n                  for field in dt]\n        arrow_type = pa.struct(fields)\n    else:\n        raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))\n    return arrow_type
Language: python
Code Tokens: ['def', 'to_arrow_type', '(', 'dt', ')', ':', 'import', 'pyarrow', 'as', 'pa', 'if', 'type', '(', 'dt', ')', '==', 'BooleanType', ':', 'arrow_type', '=', 'pa', '.', 'bool_', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'ByteType', ':', 'arrow_type', '=', 'pa', '.', 'int8', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'ShortType', ':', 'arrow_type', '=', 'pa', '.', 'int16', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'IntegerType', ':', 'arrow_type', '=', 'pa', '.', 'int32', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'LongType', ':', 'arrow_type', '=', 'pa', '.', 'int64', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'FloatType', ':', 'arrow_type', '=', 'pa', '.', 'float32', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'DoubleType', ':', 'arrow_type', '=', 'pa', '.', 'float64', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'DecimalType', ':', 'arrow_type', '=', 'pa', '.', 'decimal128', '(', 'dt', '.', 'precision', ',', 'dt', '.', 'scale', ')', 'elif', 'type', '(', 'dt', ')', '==', 'StringType', ':', 'arrow_type', '=', 'pa', '.', 'string', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'BinaryType', ':', 'arrow_type', '=', 'pa', '.', 'binary', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'DateType', ':', 'arrow_type', '=', 'pa', '.', 'date32', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'TimestampType', ':', '# Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read', 'arrow_type', '=', 'pa', '.', 'timestamp', '(', "'us'", ',', 'tz', '=', "'UTC'", ')', 'elif', 'type', '(', 'dt', ')', '==', 'ArrayType', ':', 'if', 'type', '(', 'dt', '.', 'elementType', ')', 'in', '[', 'StructType', ',', 'TimestampType', ']', ':', 'raise', 'TypeError', '(', '"Unsupported type in conversion to Arrow: "', '+', 'str', '(', 'dt', ')', ')', 'arrow_type', '=', 'pa', '.', 'list_', '(', 'to_arrow_type', '(', 'dt', '.', 'elementType', ')', ')', 'elif', 'type', '(', 'dt', ')', '==', 'StructType', ':', 'if', 'any', '(', 'type', '(', 'field', '.', 'dataType', ')', '==', 'StructType', 'for', 'field', 'in', 'dt', ')', ':', 'raise', 'TypeError', '(', '"Nested StructType not supported in conversion to Arrow"', ')', 'fields', '=', '[', 'pa', '.', 'field', '(', 'field', '.', 'name', ',', 'to_arrow_type', '(', 'field', '.', 'dataType', ')', ',', 'nullable', '=', 'field', '.', 'nullable', ')', 'for', 'field', 'in', 'dt', ']', 'arrow_type', '=', 'pa', '.', 'struct', '(', 'fields', ')', 'else', ':', 'raise', 'TypeError', '(', '"Unsupported type in conversion to Arrow: "', '+', 'str', '(', 'dt', ')', ')', 'return', 'arrow_type']
Docstring: Convert Spark data type to pyarrow type
*******__*******
Code:def to_arrow_schema(schema):\n    """ Convert a schema from Spark to Arrow\n    """\n    import pyarrow as pa\n    fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)\n              for field in schema]\n    return pa.schema(fields)
Language: python
Code Tokens: ['def', 'to_arrow_schema', '(', 'schema', ')', ':', 'import', 'pyarrow', 'as', 'pa', 'fields', '=', '[', 'pa', '.', 'field', '(', 'field', '.', 'name', ',', 'to_arrow_type', '(', 'field', '.', 'dataType', ')', ',', 'nullable', '=', 'field', '.', 'nullable', ')', 'for', 'field', 'in', 'schema', ']', 'return', 'pa', '.', 'schema', '(', 'fields', ')']
Docstring: Convert a schema from Spark to Arrow
*******__*******
Code:def from_arrow_type(at):\n    """ Convert pyarrow type to Spark data type.\n    """\n    import pyarrow.types as types\n    if types.is_boolean(at):\n        spark_type = BooleanType()\n    elif types.is_int8(at):\n        spark_type = ByteType()\n    elif types.is_int16(at):\n        spark_type = ShortType()\n    elif types.is_int32(at):\n        spark_type = IntegerType()\n    elif types.is_int64(at):\n        spark_type = LongType()\n    elif types.is_float32(at):\n        spark_type = FloatType()\n    elif types.is_float64(at):\n        spark_type = DoubleType()\n    elif types.is_decimal(at):\n        spark_type = DecimalType(precision=at.precision, scale=at.scale)\n    elif types.is_string(at):\n        spark_type = StringType()\n    elif types.is_binary(at):\n        spark_type = BinaryType()\n    elif types.is_date32(at):\n        spark_type = DateType()\n    elif types.is_timestamp(at):\n        spark_type = TimestampType()\n    elif types.is_list(at):\n        if types.is_timestamp(at.value_type):\n            raise TypeError("Unsupported type in conversion from Arrow: " + str(at))\n        spark_type = ArrayType(from_arrow_type(at.value_type))\n    elif types.is_struct(at):\n        if any(types.is_struct(field.type) for field in at):\n            raise TypeError("Nested StructType not supported in conversion from Arrow: " + str(at))\n        return StructType(\n            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)\n             for field in at])\n    else:\n        raise TypeError("Unsupported type in conversion from Arrow: " + str(at))\n    return spark_type
Language: python
Code Tokens: ['def', 'from_arrow_type', '(', 'at', ')', ':', 'import', 'pyarrow', '.', 'types', 'as', 'types', 'if', 'types', '.', 'is_boolean', '(', 'at', ')', ':', 'spark_type', '=', 'BooleanType', '(', ')', 'elif', 'types', '.', 'is_int8', '(', 'at', ')', ':', 'spark_type', '=', 'ByteType', '(', ')', 'elif', 'types', '.', 'is_int16', '(', 'at', ')', ':', 'spark_type', '=', 'ShortType', '(', ')', 'elif', 'types', '.', 'is_int32', '(', 'at', ')', ':', 'spark_type', '=', 'IntegerType', '(', ')', 'elif', 'types', '.', 'is_int64', '(', 'at', ')', ':', 'spark_type', '=', 'LongType', '(', ')', 'elif', 'types', '.', 'is_float32', '(', 'at', ')', ':', 'spark_type', '=', 'FloatType', '(', ')', 'elif', 'types', '.', 'is_float64', '(', 'at', ')', ':', 'spark_type', '=', 'DoubleType', '(', ')', 'elif', 'types', '.', 'is_decimal', '(', 'at', ')', ':', 'spark_type', '=', 'DecimalType', '(', 'precision', '=', 'at', '.', 'precision', ',', 'scale', '=', 'at', '.', 'scale', ')', 'elif', 'types', '.', 'is_string', '(', 'at', ')', ':', 'spark_type', '=', 'StringType', '(', ')', 'elif', 'types', '.', 'is_binary', '(', 'at', ')', ':', 'spark_type', '=', 'BinaryType', '(', ')', 'elif', 'types', '.', 'is_date32', '(', 'at', ')', ':', 'spark_type', '=', 'DateType', '(', ')', 'elif', 'types', '.', 'is_timestamp', '(', 'at', ')', ':', 'spark_type', '=', 'TimestampType', '(', ')', 'elif', 'types', '.', 'is_list', '(', 'at', ')', ':', 'if', 'types', '.', 'is_timestamp', '(', 'at', '.', 'value_type', ')', ':', 'raise', 'TypeError', '(', '"Unsupported type in conversion from Arrow: "', '+', 'str', '(', 'at', ')', ')', 'spark_type', '=', 'ArrayType', '(', 'from_arrow_type', '(', 'at', '.', 'value_type', ')', ')', 'elif', 'types', '.', 'is_struct', '(', 'at', ')', ':', 'if', 'any', '(', 'types', '.', 'is_struct', '(', 'field', '.', 'type', ')', 'for', 'field', 'in', 'at', ')', ':', 'raise', 'TypeError', '(', '"Nested StructType not supported in conversion from Arrow: "', '+', 'str', '(', 'at', ')', ')', 'return', 'StructType', '(', '[', 'StructField', '(', 'field', '.', 'name', ',', 'from_arrow_type', '(', 'field', '.', 'type', ')', ',', 'nullable', '=', 'field', '.', 'nullable', ')', 'for', 'field', 'in', 'at', ']', ')', 'else', ':', 'raise', 'TypeError', '(', '"Unsupported type in conversion from Arrow: "', '+', 'str', '(', 'at', ')', ')', 'return', 'spark_type']
Docstring: Convert pyarrow type to Spark data type.
*******__*******
Code:def from_arrow_schema(arrow_schema):\n    """ Convert schema from Arrow to Spark.\n    """\n    return StructType(\n        [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)\n         for field in arrow_schema])
Language: python
Code Tokens: ['def', 'from_arrow_schema', '(', 'arrow_schema', ')', ':', 'return', 'StructType', '(', '[', 'StructField', '(', 'field', '.', 'name', ',', 'from_arrow_type', '(', 'field', '.', 'type', ')', ',', 'nullable', '=', 'field', '.', 'nullable', ')', 'for', 'field', 'in', 'arrow_schema', ']', ')']
Docstring: Convert schema from Arrow to Spark.
*******__*******
Code:def _check_series_localize_timestamps(s, timezone):\n    """\n    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.\n\n    If the input series is not a timestamp series, then the same series is returned. If the input\n    series is a timestamp series, then a converted series is returned.\n\n    :param s: pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series that have been converted to tz-naive\n    """\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    from pandas.api.types import is_datetime64tz_dtype\n    tz = timezone or _get_local_timezone()\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert(tz).dt.tz_localize(None)\n    else:\n        return s
Language: python
Code Tokens: ['def', '_check_series_localize_timestamps', '(', 's', ',', 'timezone', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'from', 'pandas', '.', 'api', '.', 'types', 'import', 'is_datetime64tz_dtype', 'tz', '=', 'timezone', 'or', '_get_local_timezone', '(', ')', '# TODO: handle nested timestamps, such as ArrayType(TimestampType())?', 'if', 'is_datetime64tz_dtype', '(', 's', '.', 'dtype', ')', ':', 'return', 's', '.', 'dt', '.', 'tz_convert', '(', 'tz', ')', '.', 'dt', '.', 'tz_localize', '(', 'None', ')', 'else', ':', 'return', 's']
Docstring: Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.    If the input series is not a timestamp series, then the same series is returned. If the input    series is a timestamp series, then a converted series is returned.    :param s: pandas.Series    :param timezone: the timezone to convert. if None then use local timezone    :return pandas.Series that have been converted to tz-naive
*******__*******
Code:def _check_dataframe_localize_timestamps(pdf, timezone):\n    """\n    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone\n\n    :param pdf: pandas.DataFrame\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive\n    """\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    for column, series in pdf.iteritems():\n        pdf[column] = _check_series_localize_timestamps(series, timezone)\n    return pdf
Language: python
Code Tokens: ['def', '_check_dataframe_localize_timestamps', '(', 'pdf', ',', 'timezone', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'for', 'column', ',', 'series', 'in', 'pdf', '.', 'iteritems', '(', ')', ':', 'pdf', '[', 'column', ']', '=', '_check_series_localize_timestamps', '(', 'series', ',', 'timezone', ')', 'return', 'pdf']
Docstring: Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone    :param pdf: pandas.DataFrame    :param timezone: the timezone to convert. if None then use local timezone    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive
*******__*******
Code:def _check_series_convert_timestamps_internal(s, timezone):\n    """\n    Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for\n    Spark internal storage\n\n    :param s: a pandas.Series\n    :param timezone: the timezone to convert. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone\n    """\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64_dtype(s.dtype):\n        # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive\n        # timestamp is during the hour when the clock is adjusted backward during due to\n        # daylight saving time (dst).\n        # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to\n        # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize\n        # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either\n        # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).\n        #\n        # Here we explicit choose to use standard time. This matches the default behavior of\n        # pytz.\n        #\n        # Here are some code to help understand this behavior:\n        # >>> import datetime\n        # >>> import pandas as pd\n        # >>> import pytz\n        # >>>\n        # >>> t = datetime.datetime(2015, 11, 1, 1, 30)\n        # >>> ts = pd.Series([t])\n        # >>> tz = pytz.timezone('America/New_York')\n        # >>>\n        # >>> ts.dt.tz_localize(tz, ambiguous=True)\n        # 0   2015-11-01 01:30:00-04:00\n        # dtype: datetime64[ns, America/New_York]\n        # >>>\n        # >>> ts.dt.tz_localize(tz, ambiguous=False)\n        # 0   2015-11-01 01:30:00-05:00\n        # dtype: datetime64[ns, America/New_York]\n        # >>>\n        # >>> str(tz.localize(t))\n        # '2015-11-01 01:30:00-05:00'\n        tz = timezone or _get_local_timezone()\n        return s.dt.tz_localize(tz, ambiguous=False).dt.tz_convert('UTC')\n    elif is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert('UTC')\n    else:\n        return s
Language: python
Code Tokens: ['def', '_check_series_convert_timestamps_internal', '(', 's', ',', 'timezone', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'from', 'pandas', '.', 'api', '.', 'types', 'import', 'is_datetime64_dtype', ',', 'is_datetime64tz_dtype', '# TODO: handle nested timestamps, such as ArrayType(TimestampType())?', 'if', 'is_datetime64_dtype', '(', 's', '.', 'dtype', ')', ':', '# When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive', '# timestamp is during the hour when the clock is adjusted backward during due to', '# daylight saving time (dst).', '# E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to', '# 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize', '# a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either', '# dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).', '#', '# Here we explicit choose to use standard time. This matches the default behavior of', '# pytz.', '#', '# Here are some code to help understand this behavior:', '# >>> import datetime', '# >>> import pandas as pd', '# >>> import pytz', '# >>>', '# >>> t = datetime.datetime(2015, 11, 1, 1, 30)', '# >>> ts = pd.Series([t])', "# >>> tz = pytz.timezone('America/New_York')", '# >>>', '# >>> ts.dt.tz_localize(tz, ambiguous=True)', '# 0   2015-11-01 01:30:00-04:00', '# dtype: datetime64[ns, America/New_York]', '# >>>', '# >>> ts.dt.tz_localize(tz, ambiguous=False)', '# 0   2015-11-01 01:30:00-05:00', '# dtype: datetime64[ns, America/New_York]', '# >>>', '# >>> str(tz.localize(t))', "# '2015-11-01 01:30:00-05:00'", 'tz', '=', 'timezone', 'or', '_get_local_timezone', '(', ')', 'return', 's', '.', 'dt', '.', 'tz_localize', '(', 'tz', ',', 'ambiguous', '=', 'False', ')', '.', 'dt', '.', 'tz_convert', '(', "'UTC'", ')', 'elif', 'is_datetime64tz_dtype', '(', 's', '.', 'dtype', ')', ':', 'return', 's', '.', 'dt', '.', 'tz_convert', '(', "'UTC'", ')', 'else', ':', 'return', 's']
Docstring: Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for    Spark internal storage    :param s: a pandas.Series    :param timezone: the timezone to convert. if None then use local timezone    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone
*******__*******
Code:def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):\n    """\n    Convert timestamp to timezone-naive in the specified timezone or local timezone\n\n    :param s: a pandas.Series\n    :param from_timezone: the timezone to convert from. if None then use local timezone\n    :param to_timezone: the timezone to convert to. if None then use local timezone\n    :return pandas.Series where if it is a timestamp, has been converted to tz-naive\n    """\n    from pyspark.sql.utils import require_minimum_pandas_version\n    require_minimum_pandas_version()\n\n    import pandas as pd\n    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype\n    from_tz = from_timezone or _get_local_timezone()\n    to_tz = to_timezone or _get_local_timezone()\n    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n    if is_datetime64tz_dtype(s.dtype):\n        return s.dt.tz_convert(to_tz).dt.tz_localize(None)\n    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:\n        # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.\n        return s.apply(\n            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)\n            if ts is not pd.NaT else pd.NaT)\n    else:\n        return s
Language: python
Code Tokens: ['def', '_check_series_convert_timestamps_localize', '(', 's', ',', 'from_timezone', ',', 'to_timezone', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'import', 'pandas', 'as', 'pd', 'from', 'pandas', '.', 'api', '.', 'types', 'import', 'is_datetime64tz_dtype', ',', 'is_datetime64_dtype', 'from_tz', '=', 'from_timezone', 'or', '_get_local_timezone', '(', ')', 'to_tz', '=', 'to_timezone', 'or', '_get_local_timezone', '(', ')', '# TODO: handle nested timestamps, such as ArrayType(TimestampType())?', 'if', 'is_datetime64tz_dtype', '(', 's', '.', 'dtype', ')', ':', 'return', 's', '.', 'dt', '.', 'tz_convert', '(', 'to_tz', ')', '.', 'dt', '.', 'tz_localize', '(', 'None', ')', 'elif', 'is_datetime64_dtype', '(', 's', '.', 'dtype', ')', 'and', 'from_tz', '!=', 'to_tz', ':', "# `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.", 'return', 's', '.', 'apply', '(', 'lambda', 'ts', ':', 'ts', '.', 'tz_localize', '(', 'from_tz', ',', 'ambiguous', '=', 'False', ')', '.', 'tz_convert', '(', 'to_tz', ')', '.', 'tz_localize', '(', 'None', ')', 'if', 'ts', 'is', 'not', 'pd', '.', 'NaT', 'else', 'pd', '.', 'NaT', ')', 'else', ':', 'return', 's']
Docstring: Convert timestamp to timezone-naive in the specified timezone or local timezone    :param s: a pandas.Series    :param from_timezone: the timezone to convert from. if None then use local timezone    :param to_timezone: the timezone to convert to. if None then use local timezone    :return pandas.Series where if it is a timestamp, has been converted to tz-naive
*******__*******
Code:def add(self, field, data_type=None, nullable=True, metadata=None):\n        """\n        Construct a StructType by adding new elements to it to define the schema. The method accepts\n        either:\n\n            a) A single parameter which is a StructField object.\n            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n               metadata(optional). The data_type parameter may be either a String or a\n               DataType object.\n\n        >>> struct1 = StructType().add("f1", StringType(), True).add("f2", StringType(), True, None)\n        >>> struct2 = StructType([StructField("f1", StringType(), True), \\\n        ...     StructField("f2", StringType(), True, None)])\n        >>> struct1 == struct2\n        True\n        >>> struct1 = StructType().add(StructField("f1", StringType(), True))\n        >>> struct2 = StructType([StructField("f1", StringType(), True)])\n        >>> struct1 == struct2\n        True\n        >>> struct1 = StructType().add("f1", "string", True)\n        >>> struct2 = StructType([StructField("f1", StringType(), True)])\n        >>> struct1 == struct2\n        True\n\n        :param field: Either the name of the field or a StructField object\n        :param data_type: If present, the DataType of the StructField to create\n        :param nullable: Whether the field to add should be nullable (default True)\n        :param metadata: Any additional metadata (default None)\n        :return: a new updated StructType\n        """\n        if isinstance(field, StructField):\n            self.fields.append(field)\n            self.names.append(field.name)\n        else:\n            if isinstance(field, str) and data_type is None:\n                raise ValueError("Must specify DataType if passing name of struct_field to create.")\n\n            if isinstance(data_type, str):\n                data_type_f = _parse_datatype_json_value(data_type)\n            else:\n                data_type_f = data_type\n            self.fields.append(StructField(field, data_type_f, nullable, metadata))\n            self.names.append(field)\n        # Precalculated list of fields that need conversion with fromInternal/toInternal functions\n        self._needConversion = [f.needConversion() for f in self]\n        self._needSerializeAnyField = any(self._needConversion)\n        return self
Language: python
Code Tokens: ['def', 'add', '(', 'self', ',', 'field', ',', 'data_type', '=', 'None', ',', 'nullable', '=', 'True', ',', 'metadata', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'field', ',', 'StructField', ')', ':', 'self', '.', 'fields', '.', 'append', '(', 'field', ')', 'self', '.', 'names', '.', 'append', '(', 'field', '.', 'name', ')', 'else', ':', 'if', 'isinstance', '(', 'field', ',', 'str', ')', 'and', 'data_type', 'is', 'None', ':', 'raise', 'ValueError', '(', '"Must specify DataType if passing name of struct_field to create."', ')', 'if', 'isinstance', '(', 'data_type', ',', 'str', ')', ':', 'data_type_f', '=', '_parse_datatype_json_value', '(', 'data_type', ')', 'else', ':', 'data_type_f', '=', 'data_type', 'self', '.', 'fields', '.', 'append', '(', 'StructField', '(', 'field', ',', 'data_type_f', ',', 'nullable', ',', 'metadata', ')', ')', 'self', '.', 'names', '.', 'append', '(', 'field', ')', '# Precalculated list of fields that need conversion with fromInternal/toInternal functions', 'self', '.', '_needConversion', '=', '[', 'f', '.', 'needConversion', '(', ')', 'for', 'f', 'in', 'self', ']', 'self', '.', '_needSerializeAnyField', '=', 'any', '(', 'self', '.', '_needConversion', ')', 'return', 'self']
Docstring: Construct a StructType by adding new elements to it to define the schema. The method accepts        either:            a) A single parameter which is a StructField object.            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),               metadata(optional). The data_type parameter may be either a String or a               DataType object.        >>> struct1 = StructType().add("f1", StringType(), True).add("f2", StringType(), True, None)        >>> struct2 = StructType([StructField("f1", StringType(), True), \\        ...     StructField("f2", StringType(), True, None)])        >>> struct1 == struct2        True        >>> struct1 = StructType().add(StructField("f1", StringType(), True))        >>> struct2 = StructType([StructField("f1", StringType(), True)])        >>> struct1 == struct2        True        >>> struct1 = StructType().add("f1", "string", True)        >>> struct2 = StructType([StructField("f1", StringType(), True)])        >>> struct1 == struct2        True        :param field: Either the name of the field or a StructField object        :param data_type: If present, the DataType of the StructField to create        :param nullable: Whether the field to add should be nullable (default True)        :param metadata: Any additional metadata (default None)        :return: a new updated StructType
*******__*******
Code:def _cachedSqlType(cls):\n        """\n        Cache the sqlType() into class, because it's heavy used in `toInternal`.\n        """\n        if not hasattr(cls, "_cached_sql_type"):\n            cls._cached_sql_type = cls.sqlType()\n        return cls._cached_sql_type
Language: python
Code Tokens: ['def', '_cachedSqlType', '(', 'cls', ')', ':', 'if', 'not', 'hasattr', '(', 'cls', ',', '"_cached_sql_type"', ')', ':', 'cls', '.', '_cached_sql_type', '=', 'cls', '.', 'sqlType', '(', ')', 'return', 'cls', '.', '_cached_sql_type']
Docstring: Cache the sqlType() into class, because it's heavy used in `toInternal`.
*******__*******
Code:def asDict(self, recursive=False):\n        """\n        Return as an dict\n\n        :param recursive: turns the nested Row as dict (default: False).\n\n        >>> Row(name="Alice", age=11).asDict() == {'name': 'Alice', 'age': 11}\n        True\n        >>> row = Row(key=1, value=Row(name='a', age=2))\n        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n        True\n        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n        True\n        """\n        if not hasattr(self, "__fields__"):\n            raise TypeError("Cannot convert a Row class into dict")\n\n        if recursive:\n            def conv(obj):\n                if isinstance(obj, Row):\n                    return obj.asDict(True)\n                elif isinstance(obj, list):\n                    return [conv(o) for o in obj]\n                elif isinstance(obj, dict):\n                    return dict((k, conv(v)) for k, v in obj.items())\n                else:\n                    return obj\n            return dict(zip(self.__fields__, (conv(o) for o in self)))\n        else:\n            return dict(zip(self.__fields__, self))
Language: python
Code Tokens: ['def', 'asDict', '(', 'self', ',', 'recursive', '=', 'False', ')', ':', 'if', 'not', 'hasattr', '(', 'self', ',', '"__fields__"', ')', ':', 'raise', 'TypeError', '(', '"Cannot convert a Row class into dict"', ')', 'if', 'recursive', ':', 'def', 'conv', '(', 'obj', ')', ':', 'if', 'isinstance', '(', 'obj', ',', 'Row', ')', ':', 'return', 'obj', '.', 'asDict', '(', 'True', ')', 'elif', 'isinstance', '(', 'obj', ',', 'list', ')', ':', 'return', '[', 'conv', '(', 'o', ')', 'for', 'o', 'in', 'obj', ']', 'elif', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'return', 'dict', '(', '(', 'k', ',', 'conv', '(', 'v', ')', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', ')', 'else', ':', 'return', 'obj', 'return', 'dict', '(', 'zip', '(', 'self', '.', '__fields__', ',', '(', 'conv', '(', 'o', ')', 'for', 'o', 'in', 'self', ')', ')', ')', 'else', ':', 'return', 'dict', '(', 'zip', '(', 'self', '.', '__fields__', ',', 'self', ')', ')']
Docstring: Return as an dict        :param recursive: turns the nested Row as dict (default: False).        >>> Row(name="Alice", age=11).asDict() == {'name': 'Alice', 'age': 11}        True        >>> row = Row(key=1, value=Row(name='a', age=2))        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}        True        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}        True
*******__*******
Code:def summary(self):\n        """\n        Gets summary (e.g. residuals, mse, r-squared ) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.\n        """\n        if self.hasSummary:\n            return LinearRegressionTrainingSummary(super(LinearRegressionModel, self).summary)\n        else:\n            raise RuntimeError("No training summary available for this %s" %\n                               self.__class__.__name__)
Language: python
Code Tokens: ['def', 'summary', '(', 'self', ')', ':', 'if', 'self', '.', 'hasSummary', ':', 'return', 'LinearRegressionTrainingSummary', '(', 'super', '(', 'LinearRegressionModel', ',', 'self', ')', '.', 'summary', ')', 'else', ':', 'raise', 'RuntimeError', '(', '"No training summary available for this %s"', '%', 'self', '.', '__class__', '.', '__name__', ')']
Docstring: Gets summary (e.g. residuals, mse, r-squared ) of model on        training set. An exception is thrown if        `trainingSummary is None`.
*******__*******
Code:def evaluate(self, dataset):\n        """\n        Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`\n        """\n        if not isinstance(dataset, DataFrame):\n            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))\n        java_lr_summary = self._call_java("evaluate", dataset)\n        return LinearRegressionSummary(java_lr_summary)
Language: python
Code Tokens: ['def', 'evaluate', '(', 'self', ',', 'dataset', ')', ':', 'if', 'not', 'isinstance', '(', 'dataset', ',', 'DataFrame', ')', ':', 'raise', 'ValueError', '(', '"dataset must be a DataFrame but got %s."', '%', 'type', '(', 'dataset', ')', ')', 'java_lr_summary', '=', 'self', '.', '_call_java', '(', '"evaluate"', ',', 'dataset', ')', 'return', 'LinearRegressionSummary', '(', 'java_lr_summary', ')']
Docstring: Evaluates the model on a test dataset.        :param dataset:          Test dataset to evaluate model on, where dataset is an          instance of :py:class:`pyspark.sql.DataFrame`
*******__*******
Code:def summary(self):\n        """\n        Gets summary (e.g. residuals, deviance, pValues) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.\n        """\n        if self.hasSummary:\n            return GeneralizedLinearRegressionTrainingSummary(\n                super(GeneralizedLinearRegressionModel, self).summary)\n        else:\n            raise RuntimeError("No training summary available for this %s" %\n                               self.__class__.__name__)
Language: python
Code Tokens: ['def', 'summary', '(', 'self', ')', ':', 'if', 'self', '.', 'hasSummary', ':', 'return', 'GeneralizedLinearRegressionTrainingSummary', '(', 'super', '(', 'GeneralizedLinearRegressionModel', ',', 'self', ')', '.', 'summary', ')', 'else', ':', 'raise', 'RuntimeError', '(', '"No training summary available for this %s"', '%', 'self', '.', '__class__', '.', '__name__', ')']
Docstring: Gets summary (e.g. residuals, deviance, pValues) of model on        training set. An exception is thrown if        `trainingSummary is None`.
*******__*******
Code:def evaluate(self, dataset):\n        """\n        Evaluates the model on a test dataset.\n\n        :param dataset:\n          Test dataset to evaluate model on, where dataset is an\n          instance of :py:class:`pyspark.sql.DataFrame`\n        """\n        if not isinstance(dataset, DataFrame):\n            raise ValueError("dataset must be a DataFrame but got %s." % type(dataset))\n        java_glr_summary = self._call_java("evaluate", dataset)\n        return GeneralizedLinearRegressionSummary(java_glr_summary)
Language: python
Code Tokens: ['def', 'evaluate', '(', 'self', ',', 'dataset', ')', ':', 'if', 'not', 'isinstance', '(', 'dataset', ',', 'DataFrame', ')', ':', 'raise', 'ValueError', '(', '"dataset must be a DataFrame but got %s."', '%', 'type', '(', 'dataset', ')', ')', 'java_glr_summary', '=', 'self', '.', '_call_java', '(', '"evaluate"', ',', 'dataset', ')', 'return', 'GeneralizedLinearRegressionSummary', '(', 'java_glr_summary', ')']
Docstring: Evaluates the model on a test dataset.        :param dataset:          Test dataset to evaluate model on, where dataset is an          instance of :py:class:`pyspark.sql.DataFrame`
*******__*******
Code:def _get_local_dirs(sub):\n    """ Get all the directories """\n    path = os.environ.get("SPARK_LOCAL_DIRS", "/tmp")\n    dirs = path.split(",")\n    if len(dirs) > 1:\n        # different order in different processes and instances\n        rnd = random.Random(os.getpid() + id(dirs))\n        random.shuffle(dirs, rnd.random)\n    return [os.path.join(d, "python", str(os.getpid()), sub) for d in dirs]
Language: python
Code Tokens: ['def', '_get_local_dirs', '(', 'sub', ')', ':', 'path', '=', 'os', '.', 'environ', '.', 'get', '(', '"SPARK_LOCAL_DIRS"', ',', '"/tmp"', ')', 'dirs', '=', 'path', '.', 'split', '(', '","', ')', 'if', 'len', '(', 'dirs', ')', '>', '1', ':', '# different order in different processes and instances', 'rnd', '=', 'random', '.', 'Random', '(', 'os', '.', 'getpid', '(', ')', '+', 'id', '(', 'dirs', ')', ')', 'random', '.', 'shuffle', '(', 'dirs', ',', 'rnd', '.', 'random', ')', 'return', '[', 'os', '.', 'path', '.', 'join', '(', 'd', ',', '"python"', ',', 'str', '(', 'os', '.', 'getpid', '(', ')', ')', ',', 'sub', ')', 'for', 'd', 'in', 'dirs', ']']
Docstring: Get all the directories
*******__*******
Code:def _get_spill_dir(self, n):\n        """ Choose one directory for spill by number n """\n        return os.path.join(self.localdirs[n % len(self.localdirs)], str(n))
Language: python
Code Tokens: ['def', '_get_spill_dir', '(', 'self', ',', 'n', ')', ':', 'return', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'localdirs', '[', 'n', '%', 'len', '(', 'self', '.', 'localdirs', ')', ']', ',', 'str', '(', 'n', ')', ')']
Docstring: Choose one directory for spill by number n
*******__*******
Code:def mergeValues(self, iterator):\n        """ Combine the items by creator and combiner """\n        # speedup attribute lookup\n        creator, comb = self.agg.createCombiner, self.agg.mergeValue\n        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch\n        limit = self.memory_limit\n\n        for k, v in iterator:\n            d = pdata[hfun(k)] if pdata else data\n            d[k] = comb(d[k], v) if k in d else creator(v)\n\n            c += 1\n            if c >= batch:\n                if get_used_memory() >= limit:\n                    self._spill()\n                    limit = self._next_limit()\n                    batch /= 2\n                    c = 0\n                else:\n                    batch *= 1.5\n\n        if get_used_memory() >= limit:\n            self._spill()
Language: python
Code Tokens: ['def', 'mergeValues', '(', 'self', ',', 'iterator', ')', ':', '# speedup attribute lookup', 'creator', ',', 'comb', '=', 'self', '.', 'agg', '.', 'createCombiner', ',', 'self', '.', 'agg', '.', 'mergeValue', 'c', ',', 'data', ',', 'pdata', ',', 'hfun', ',', 'batch', '=', '0', ',', 'self', '.', 'data', ',', 'self', '.', 'pdata', ',', 'self', '.', '_partition', ',', 'self', '.', 'batch', 'limit', '=', 'self', '.', 'memory_limit', 'for', 'k', ',', 'v', 'in', 'iterator', ':', 'd', '=', 'pdata', '[', 'hfun', '(', 'k', ')', ']', 'if', 'pdata', 'else', 'data', 'd', '[', 'k', ']', '=', 'comb', '(', 'd', '[', 'k', ']', ',', 'v', ')', 'if', 'k', 'in', 'd', 'else', 'creator', '(', 'v', ')', 'c', '+=', '1', 'if', 'c', '>=', 'batch', ':', 'if', 'get_used_memory', '(', ')', '>=', 'limit', ':', 'self', '.', '_spill', '(', ')', 'limit', '=', 'self', '.', '_next_limit', '(', ')', 'batch', '/=', '2', 'c', '=', '0', 'else', ':', 'batch', '*=', '1.5', 'if', 'get_used_memory', '(', ')', '>=', 'limit', ':', 'self', '.', '_spill', '(', ')']
Docstring: Combine the items by creator and combiner
*******__*******
Code:def mergeCombiners(self, iterator, limit=None):\n        """ Merge (K,V) pair by mergeCombiner """\n        if limit is None:\n            limit = self.memory_limit\n        # speedup attribute lookup\n        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size\n        c, data, pdata, batch = 0, self.data, self.pdata, self.batch\n        for k, v in iterator:\n            d = pdata[hfun(k)] if pdata else data\n            d[k] = comb(d[k], v) if k in d else v\n            if not limit:\n                continue\n\n            c += objsize(v)\n            if c > batch:\n                if get_used_memory() > limit:\n                    self._spill()\n                    limit = self._next_limit()\n                    batch /= 2\n                    c = 0\n                else:\n                    batch *= 1.5\n\n        if limit and get_used_memory() >= limit:\n            self._spill()
Language: python
Code Tokens: ['def', 'mergeCombiners', '(', 'self', ',', 'iterator', ',', 'limit', '=', 'None', ')', ':', 'if', 'limit', 'is', 'None', ':', 'limit', '=', 'self', '.', 'memory_limit', '# speedup attribute lookup', 'comb', ',', 'hfun', ',', 'objsize', '=', 'self', '.', 'agg', '.', 'mergeCombiners', ',', 'self', '.', '_partition', ',', 'self', '.', '_object_size', 'c', ',', 'data', ',', 'pdata', ',', 'batch', '=', '0', ',', 'self', '.', 'data', ',', 'self', '.', 'pdata', ',', 'self', '.', 'batch', 'for', 'k', ',', 'v', 'in', 'iterator', ':', 'd', '=', 'pdata', '[', 'hfun', '(', 'k', ')', ']', 'if', 'pdata', 'else', 'data', 'd', '[', 'k', ']', '=', 'comb', '(', 'd', '[', 'k', ']', ',', 'v', ')', 'if', 'k', 'in', 'd', 'else', 'v', 'if', 'not', 'limit', ':', 'continue', 'c', '+=', 'objsize', '(', 'v', ')', 'if', 'c', '>', 'batch', ':', 'if', 'get_used_memory', '(', ')', '>', 'limit', ':', 'self', '.', '_spill', '(', ')', 'limit', '=', 'self', '.', '_next_limit', '(', ')', 'batch', '/=', '2', 'c', '=', '0', 'else', ':', 'batch', '*=', '1.5', 'if', 'limit', 'and', 'get_used_memory', '(', ')', '>=', 'limit', ':', 'self', '.', '_spill', '(', ')']
Docstring: Merge (K,V) pair by mergeCombiner
*******__*******
Code:def _spill(self):\n        """\n        dump already partitioned data into disks.\n\n        It will dump the data in batch for better performance.\n        """\n        global MemoryBytesSpilled, DiskBytesSpilled\n        path = self._get_spill_dir(self.spills)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        used_memory = get_used_memory()\n        if not self.pdata:\n            # The data has not been partitioned, it will iterator the\n            # dataset once, write them into different files, has no\n            # additional memory. It only called when the memory goes\n            # above limit at the first time.\n\n            # open all the files for writing\n            streams = [open(os.path.join(path, str(i)), 'wb')\n                       for i in range(self.partitions)]\n\n            for k, v in self.data.items():\n                h = self._partition(k)\n                # put one item in batch, make it compatible with load_stream\n                # it will increase the memory if dump them in batch\n                self.serializer.dump_stream([(k, v)], streams[h])\n\n            for s in streams:\n                DiskBytesSpilled += s.tell()\n                s.close()\n\n            self.data.clear()\n            self.pdata.extend([{} for i in range(self.partitions)])\n\n        else:\n            for i in range(self.partitions):\n                p = os.path.join(path, str(i))\n                with open(p, "wb") as f:\n                    # dump items in batch\n                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)\n                self.pdata[i].clear()\n                DiskBytesSpilled += os.path.getsize(p)\n\n        self.spills += 1\n        gc.collect()  # release the memory as much as possible\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
Language: python
Code Tokens: ['def', '_spill', '(', 'self', ')', ':', 'global', 'MemoryBytesSpilled', ',', 'DiskBytesSpilled', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'self', '.', 'spills', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'path', ')', ':', 'os', '.', 'makedirs', '(', 'path', ')', 'used_memory', '=', 'get_used_memory', '(', ')', 'if', 'not', 'self', '.', 'pdata', ':', '# The data has not been partitioned, it will iterator the', '# dataset once, write them into different files, has no', '# additional memory. It only called when the memory goes', '# above limit at the first time.', '# open all the files for writing', 'streams', '=', '[', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', ',', "'wb'", ')', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', 'for', 'k', ',', 'v', 'in', 'self', '.', 'data', '.', 'items', '(', ')', ':', 'h', '=', 'self', '.', '_partition', '(', 'k', ')', '# put one item in batch, make it compatible with load_stream', '# it will increase the memory if dump them in batch', 'self', '.', 'serializer', '.', 'dump_stream', '(', '[', '(', 'k', ',', 'v', ')', ']', ',', 'streams', '[', 'h', ']', ')', 'for', 's', 'in', 'streams', ':', 'DiskBytesSpilled', '+=', 's', '.', 'tell', '(', ')', 's', '.', 'close', '(', ')', 'self', '.', 'data', '.', 'clear', '(', ')', 'self', '.', 'pdata', '.', 'extend', '(', '[', '{', '}', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', ')', 'else', ':', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ':', 'p', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', 'with', 'open', '(', 'p', ',', '"wb"', ')', 'as', 'f', ':', '# dump items in batch', 'self', '.', 'serializer', '.', 'dump_stream', '(', 'iter', '(', 'self', '.', 'pdata', '[', 'i', ']', '.', 'items', '(', ')', ')', ',', 'f', ')', 'self', '.', 'pdata', '[', 'i', ']', '.', 'clear', '(', ')', 'DiskBytesSpilled', '+=', 'os', '.', 'path', '.', 'getsize', '(', 'p', ')', 'self', '.', 'spills', '+=', '1', 'gc', '.', 'collect', '(', ')', '# release the memory as much as possible', 'MemoryBytesSpilled', '+=', 'max', '(', 'used_memory', '-', 'get_used_memory', '(', ')', ',', '0', ')', '<<', '20']
Docstring: dump already partitioned data into disks.        It will dump the data in batch for better performance.
*******__*******
Code:def items(self):\n        """ Return all merged items as iterator """\n        if not self.pdata and not self.spills:\n            return iter(self.data.items())\n        return self._external_items()
Language: python
Code Tokens: ['def', 'items', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'pdata', 'and', 'not', 'self', '.', 'spills', ':', 'return', 'iter', '(', 'self', '.', 'data', '.', 'items', '(', ')', ')', 'return', 'self', '.', '_external_items', '(', ')']
Docstring: Return all merged items as iterator
*******__*******
Code:def _external_items(self):\n        """ Return all partitioned items as iterator """\n        assert not self.data\n        if any(self.pdata):\n            self._spill()\n        # disable partitioning and spilling when merge combiners from disk\n        self.pdata = []\n\n        try:\n            for i in range(self.partitions):\n                for v in self._merged_items(i):\n                    yield v\n                self.data.clear()\n\n                # remove the merged partition\n                for j in range(self.spills):\n                    path = self._get_spill_dir(j)\n                    os.remove(os.path.join(path, str(i)))\n        finally:\n            self._cleanup()
Language: python
Code Tokens: ['def', '_external_items', '(', 'self', ')', ':', 'assert', 'not', 'self', '.', 'data', 'if', 'any', '(', 'self', '.', 'pdata', ')', ':', 'self', '.', '_spill', '(', ')', '# disable partitioning and spilling when merge combiners from disk', 'self', '.', 'pdata', '=', '[', ']', 'try', ':', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ':', 'for', 'v', 'in', 'self', '.', '_merged_items', '(', 'i', ')', ':', 'yield', 'v', 'self', '.', 'data', '.', 'clear', '(', ')', '# remove the merged partition', 'for', 'j', 'in', 'range', '(', 'self', '.', 'spills', ')', ':', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'j', ')', 'os', '.', 'remove', '(', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', ')', 'finally', ':', 'self', '.', '_cleanup', '(', ')']
Docstring: Return all partitioned items as iterator
*******__*******
Code:def _recursive_merged_items(self, index):\n        """\n        merge the partitioned items and return the as iterator\n\n        If one partition can not be fit in memory, then them will be\n        partitioned and merged recursively.\n        """\n        subdirs = [os.path.join(d, "parts", str(index)) for d in self.localdirs]\n        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,\n                           self.scale * self.partitions, self.partitions, self.batch)\n        m.pdata = [{} for _ in range(self.partitions)]\n        limit = self._next_limit()\n\n        for j in range(self.spills):\n            path = self._get_spill_dir(j)\n            p = os.path.join(path, str(index))\n            with open(p, 'rb') as f:\n                m.mergeCombiners(self.serializer.load_stream(f), 0)\n\n            if get_used_memory() > limit:\n                m._spill()\n                limit = self._next_limit()\n\n        return m._external_items()
Language: python
Code Tokens: ['def', '_recursive_merged_items', '(', 'self', ',', 'index', ')', ':', 'subdirs', '=', '[', 'os', '.', 'path', '.', 'join', '(', 'd', ',', '"parts"', ',', 'str', '(', 'index', ')', ')', 'for', 'd', 'in', 'self', '.', 'localdirs', ']', 'm', '=', 'ExternalMerger', '(', 'self', '.', 'agg', ',', 'self', '.', 'memory_limit', ',', 'self', '.', 'serializer', ',', 'subdirs', ',', 'self', '.', 'scale', '*', 'self', '.', 'partitions', ',', 'self', '.', 'partitions', ',', 'self', '.', 'batch', ')', 'm', '.', 'pdata', '=', '[', '{', '}', 'for', '_', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', 'limit', '=', 'self', '.', '_next_limit', '(', ')', 'for', 'j', 'in', 'range', '(', 'self', '.', 'spills', ')', ':', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'j', ')', 'p', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'index', ')', ')', 'with', 'open', '(', 'p', ',', "'rb'", ')', 'as', 'f', ':', 'm', '.', 'mergeCombiners', '(', 'self', '.', 'serializer', '.', 'load_stream', '(', 'f', ')', ',', '0', ')', 'if', 'get_used_memory', '(', ')', '>', 'limit', ':', 'm', '.', '_spill', '(', ')', 'limit', '=', 'self', '.', '_next_limit', '(', ')', 'return', 'm', '.', '_external_items', '(', ')']
Docstring: merge the partitioned items and return the as iterator        If one partition can not be fit in memory, then them will be        partitioned and merged recursively.
*******__*******
Code:def _get_path(self, n):\n        """ Choose one directory for spill by number n """\n        d = self.local_dirs[n % len(self.local_dirs)]\n        if not os.path.exists(d):\n            os.makedirs(d)\n        return os.path.join(d, str(n))
Language: python
Code Tokens: ['def', '_get_path', '(', 'self', ',', 'n', ')', ':', 'd', '=', 'self', '.', 'local_dirs', '[', 'n', '%', 'len', '(', 'self', '.', 'local_dirs', ')', ']', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'd', ')', ':', 'os', '.', 'makedirs', '(', 'd', ')', 'return', 'os', '.', 'path', '.', 'join', '(', 'd', ',', 'str', '(', 'n', ')', ')']
Docstring: Choose one directory for spill by number n
*******__*******
Code:def sorted(self, iterator, key=None, reverse=False):\n        """\n        Sort the elements in iterator, do external sort when the memory\n        goes above the limit.\n        """\n        global MemoryBytesSpilled, DiskBytesSpilled\n        batch, limit = 100, self._next_limit()\n        chunks, current_chunk = [], []\n        iterator = iter(iterator)\n        while True:\n            # pick elements in batch\n            chunk = list(itertools.islice(iterator, batch))\n            current_chunk.extend(chunk)\n            if len(chunk) < batch:\n                break\n\n            used_memory = get_used_memory()\n            if used_memory > limit:\n                # sort them inplace will save memory\n                current_chunk.sort(key=key, reverse=reverse)\n                path = self._get_path(len(chunks))\n                with open(path, 'wb') as f:\n                    self.serializer.dump_stream(current_chunk, f)\n\n                def load(f):\n                    for v in self.serializer.load_stream(f):\n                        yield v\n                    # close the file explicit once we consume all the items\n                    # to avoid ResourceWarning in Python3\n                    f.close()\n                chunks.append(load(open(path, 'rb')))\n                current_chunk = []\n                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20\n                DiskBytesSpilled += os.path.getsize(path)\n                os.unlink(path)  # data will be deleted after close\n\n            elif not chunks:\n                batch = min(int(batch * 1.5), 10000)\n\n        current_chunk.sort(key=key, reverse=reverse)\n        if not chunks:\n            return current_chunk\n\n        if current_chunk:\n            chunks.append(iter(current_chunk))\n\n        return heapq.merge(chunks, key=key, reverse=reverse)
Language: python
Code Tokens: ['def', 'sorted', '(', 'self', ',', 'iterator', ',', 'key', '=', 'None', ',', 'reverse', '=', 'False', ')', ':', 'global', 'MemoryBytesSpilled', ',', 'DiskBytesSpilled', 'batch', ',', 'limit', '=', '100', ',', 'self', '.', '_next_limit', '(', ')', 'chunks', ',', 'current_chunk', '=', '[', ']', ',', '[', ']', 'iterator', '=', 'iter', '(', 'iterator', ')', 'while', 'True', ':', '# pick elements in batch', 'chunk', '=', 'list', '(', 'itertools', '.', 'islice', '(', 'iterator', ',', 'batch', ')', ')', 'current_chunk', '.', 'extend', '(', 'chunk', ')', 'if', 'len', '(', 'chunk', ')', '<', 'batch', ':', 'break', 'used_memory', '=', 'get_used_memory', '(', ')', 'if', 'used_memory', '>', 'limit', ':', '# sort them inplace will save memory', 'current_chunk', '.', 'sort', '(', 'key', '=', 'key', ',', 'reverse', '=', 'reverse', ')', 'path', '=', 'self', '.', '_get_path', '(', 'len', '(', 'chunks', ')', ')', 'with', 'open', '(', 'path', ',', "'wb'", ')', 'as', 'f', ':', 'self', '.', 'serializer', '.', 'dump_stream', '(', 'current_chunk', ',', 'f', ')', 'def', 'load', '(', 'f', ')', ':', 'for', 'v', 'in', 'self', '.', 'serializer', '.', 'load_stream', '(', 'f', ')', ':', 'yield', 'v', '# close the file explicit once we consume all the items', '# to avoid ResourceWarning in Python3', 'f', '.', 'close', '(', ')', 'chunks', '.', 'append', '(', 'load', '(', 'open', '(', 'path', ',', "'rb'", ')', ')', ')', 'current_chunk', '=', '[', ']', 'MemoryBytesSpilled', '+=', 'max', '(', 'used_memory', '-', 'get_used_memory', '(', ')', ',', '0', ')', '<<', '20', 'DiskBytesSpilled', '+=', 'os', '.', 'path', '.', 'getsize', '(', 'path', ')', 'os', '.', 'unlink', '(', 'path', ')', '# data will be deleted after close', 'elif', 'not', 'chunks', ':', 'batch', '=', 'min', '(', 'int', '(', 'batch', '*', '1.5', ')', ',', '10000', ')', 'current_chunk', '.', 'sort', '(', 'key', '=', 'key', ',', 'reverse', '=', 'reverse', ')', 'if', 'not', 'chunks', ':', 'return', 'current_chunk', 'if', 'current_chunk', ':', 'chunks', '.', 'append', '(', 'iter', '(', 'current_chunk', ')', ')', 'return', 'heapq', '.', 'merge', '(', 'chunks', ',', 'key', '=', 'key', ',', 'reverse', '=', 'reverse', ')']
Docstring: Sort the elements in iterator, do external sort when the memory        goes above the limit.
*******__*******
Code:def _spill(self):\n        """ dump the values into disk """\n        global MemoryBytesSpilled, DiskBytesSpilled\n        if self._file is None:\n            self._open_file()\n\n        used_memory = get_used_memory()\n        pos = self._file.tell()\n        self._ser.dump_stream(self.values, self._file)\n        self.values = []\n        gc.collect()\n        DiskBytesSpilled += self._file.tell() - pos\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
Language: python
Code Tokens: ['def', '_spill', '(', 'self', ')', ':', 'global', 'MemoryBytesSpilled', ',', 'DiskBytesSpilled', 'if', 'self', '.', '_file', 'is', 'None', ':', 'self', '.', '_open_file', '(', ')', 'used_memory', '=', 'get_used_memory', '(', ')', 'pos', '=', 'self', '.', '_file', '.', 'tell', '(', ')', 'self', '.', '_ser', '.', 'dump_stream', '(', 'self', '.', 'values', ',', 'self', '.', '_file', ')', 'self', '.', 'values', '=', '[', ']', 'gc', '.', 'collect', '(', ')', 'DiskBytesSpilled', '+=', 'self', '.', '_file', '.', 'tell', '(', ')', '-', 'pos', 'MemoryBytesSpilled', '+=', 'max', '(', 'used_memory', '-', 'get_used_memory', '(', ')', ',', '0', ')', '<<', '20']
Docstring: dump the values into disk
*******__*******
Code:def _spill(self):\n        """\n        dump already partitioned data into disks.\n        """\n        global MemoryBytesSpilled, DiskBytesSpilled\n        path = self._get_spill_dir(self.spills)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        used_memory = get_used_memory()\n        if not self.pdata:\n            # The data has not been partitioned, it will iterator the\n            # data once, write them into different files, has no\n            # additional memory. It only called when the memory goes\n            # above limit at the first time.\n\n            # open all the files for writing\n            streams = [open(os.path.join(path, str(i)), 'wb')\n                       for i in range(self.partitions)]\n\n            # If the number of keys is small, then the overhead of sort is small\n            # sort them before dumping into disks\n            self._sorted = len(self.data) < self.SORT_KEY_LIMIT\n            if self._sorted:\n                self.serializer = self.flattened_serializer()\n                for k in sorted(self.data.keys()):\n                    h = self._partition(k)\n                    self.serializer.dump_stream([(k, self.data[k])], streams[h])\n            else:\n                for k, v in self.data.items():\n                    h = self._partition(k)\n                    self.serializer.dump_stream([(k, v)], streams[h])\n\n            for s in streams:\n                DiskBytesSpilled += s.tell()\n                s.close()\n\n            self.data.clear()\n            # self.pdata is cached in `mergeValues` and `mergeCombiners`\n            self.pdata.extend([{} for i in range(self.partitions)])\n\n        else:\n            for i in range(self.partitions):\n                p = os.path.join(path, str(i))\n                with open(p, "wb") as f:\n                    # dump items in batch\n                    if self._sorted:\n                        # sort by key only (stable)\n                        sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))\n                        self.serializer.dump_stream(sorted_items, f)\n                    else:\n                        self.serializer.dump_stream(self.pdata[i].items(), f)\n                self.pdata[i].clear()\n                DiskBytesSpilled += os.path.getsize(p)\n\n        self.spills += 1\n        gc.collect()  # release the memory as much as possible\n        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
Language: python
Code Tokens: ['def', '_spill', '(', 'self', ')', ':', 'global', 'MemoryBytesSpilled', ',', 'DiskBytesSpilled', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'self', '.', 'spills', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'path', ')', ':', 'os', '.', 'makedirs', '(', 'path', ')', 'used_memory', '=', 'get_used_memory', '(', ')', 'if', 'not', 'self', '.', 'pdata', ':', '# The data has not been partitioned, it will iterator the', '# data once, write them into different files, has no', '# additional memory. It only called when the memory goes', '# above limit at the first time.', '# open all the files for writing', 'streams', '=', '[', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', ',', "'wb'", ')', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', '# If the number of keys is small, then the overhead of sort is small', '# sort them before dumping into disks', 'self', '.', '_sorted', '=', 'len', '(', 'self', '.', 'data', ')', '<', 'self', '.', 'SORT_KEY_LIMIT', 'if', 'self', '.', '_sorted', ':', 'self', '.', 'serializer', '=', 'self', '.', 'flattened_serializer', '(', ')', 'for', 'k', 'in', 'sorted', '(', 'self', '.', 'data', '.', 'keys', '(', ')', ')', ':', 'h', '=', 'self', '.', '_partition', '(', 'k', ')', 'self', '.', 'serializer', '.', 'dump_stream', '(', '[', '(', 'k', ',', 'self', '.', 'data', '[', 'k', ']', ')', ']', ',', 'streams', '[', 'h', ']', ')', 'else', ':', 'for', 'k', ',', 'v', 'in', 'self', '.', 'data', '.', 'items', '(', ')', ':', 'h', '=', 'self', '.', '_partition', '(', 'k', ')', 'self', '.', 'serializer', '.', 'dump_stream', '(', '[', '(', 'k', ',', 'v', ')', ']', ',', 'streams', '[', 'h', ']', ')', 'for', 's', 'in', 'streams', ':', 'DiskBytesSpilled', '+=', 's', '.', 'tell', '(', ')', 's', '.', 'close', '(', ')', 'self', '.', 'data', '.', 'clear', '(', ')', '# self.pdata is cached in `mergeValues` and `mergeCombiners`', 'self', '.', 'pdata', '.', 'extend', '(', '[', '{', '}', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', ')', 'else', ':', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ':', 'p', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', 'with', 'open', '(', 'p', ',', '"wb"', ')', 'as', 'f', ':', '# dump items in batch', 'if', 'self', '.', '_sorted', ':', '# sort by key only (stable)', 'sorted_items', '=', 'sorted', '(', 'self', '.', 'pdata', '[', 'i', ']', '.', 'items', '(', ')', ',', 'key', '=', 'operator', '.', 'itemgetter', '(', '0', ')', ')', 'self', '.', 'serializer', '.', 'dump_stream', '(', 'sorted_items', ',', 'f', ')', 'else', ':', 'self', '.', 'serializer', '.', 'dump_stream', '(', 'self', '.', 'pdata', '[', 'i', ']', '.', 'items', '(', ')', ',', 'f', ')', 'self', '.', 'pdata', '[', 'i', ']', '.', 'clear', '(', ')', 'DiskBytesSpilled', '+=', 'os', '.', 'path', '.', 'getsize', '(', 'p', ')', 'self', '.', 'spills', '+=', '1', 'gc', '.', 'collect', '(', ')', '# release the memory as much as possible', 'MemoryBytesSpilled', '+=', 'max', '(', 'used_memory', '-', 'get_used_memory', '(', ')', ',', '0', ')', '<<', '20']
Docstring: dump already partitioned data into disks.
*******__*******
Code:def _merge_sorted_items(self, index):\n        """ load a partition from disk, then sort and group by key """\n        def load_partition(j):\n            path = self._get_spill_dir(j)\n            p = os.path.join(path, str(index))\n            with open(p, 'rb', 65536) as f:\n                for v in self.serializer.load_stream(f):\n                    yield v\n\n        disk_items = [load_partition(j) for j in range(self.spills)]\n\n        if self._sorted:\n            # all the partitions are already sorted\n            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))\n\n        else:\n            # Flatten the combined values, so it will not consume huge\n            # memory during merging sort.\n            ser = self.flattened_serializer()\n            sorter = ExternalSorter(self.memory_limit, ser)\n            sorted_items = sorter.sorted(itertools.chain(*disk_items),\n                                         key=operator.itemgetter(0))\n        return ((k, vs) for k, vs in GroupByKey(sorted_items))
Language: python
Code Tokens: ['def', '_merge_sorted_items', '(', 'self', ',', 'index', ')', ':', 'def', 'load_partition', '(', 'j', ')', ':', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'j', ')', 'p', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'index', ')', ')', 'with', 'open', '(', 'p', ',', "'rb'", ',', '65536', ')', 'as', 'f', ':', 'for', 'v', 'in', 'self', '.', 'serializer', '.', 'load_stream', '(', 'f', ')', ':', 'yield', 'v', 'disk_items', '=', '[', 'load_partition', '(', 'j', ')', 'for', 'j', 'in', 'range', '(', 'self', '.', 'spills', ')', ']', 'if', 'self', '.', '_sorted', ':', '# all the partitions are already sorted', 'sorted_items', '=', 'heapq', '.', 'merge', '(', 'disk_items', ',', 'key', '=', 'operator', '.', 'itemgetter', '(', '0', ')', ')', 'else', ':', '# Flatten the combined values, so it will not consume huge', '# memory during merging sort.', 'ser', '=', 'self', '.', 'flattened_serializer', '(', ')', 'sorter', '=', 'ExternalSorter', '(', 'self', '.', 'memory_limit', ',', 'ser', ')', 'sorted_items', '=', 'sorter', '.', 'sorted', '(', 'itertools', '.', 'chain', '(', '*', 'disk_items', ')', ',', 'key', '=', 'operator', '.', 'itemgetter', '(', '0', ')', ')', 'return', '(', '(', 'k', ',', 'vs', ')', 'for', 'k', ',', 'vs', 'in', 'GroupByKey', '(', 'sorted_items', ')', ')']
Docstring: load a partition from disk, then sort and group by key
*******__*******
Code:def worker(sock, authenticated):\n    """\n    Called by a worker process after the fork().\n    """\n    signal.signal(SIGHUP, SIG_DFL)\n    signal.signal(SIGCHLD, SIG_DFL)\n    signal.signal(SIGTERM, SIG_DFL)\n    # restore the handler for SIGINT,\n    # it's useful for debugging (show the stacktrace before exit)\n    signal.signal(SIGINT, signal.default_int_handler)\n\n    # Read the socket using fdopen instead of socket.makefile() because the latter\n    # seems to be very slow; note that we need to dup() the file descriptor because\n    # otherwise writes also cause a seek that makes us miss data on the read side.\n    infile = os.fdopen(os.dup(sock.fileno()), "rb", 65536)\n    outfile = os.fdopen(os.dup(sock.fileno()), "wb", 65536)\n\n    if not authenticated:\n        client_secret = UTF8Deserializer().loads(infile)\n        if os.environ["PYTHON_WORKER_FACTORY_SECRET"] == client_secret:\n            write_with_length("ok".encode("utf-8"), outfile)\n            outfile.flush()\n        else:\n            write_with_length("err".encode("utf-8"), outfile)\n            outfile.flush()\n            sock.close()\n            return 1\n\n    exit_code = 0\n    try:\n        worker_main(infile, outfile)\n    except SystemExit as exc:\n        exit_code = compute_real_exit_code(exc.code)\n    finally:\n        try:\n            outfile.flush()\n        except Exception:\n            pass\n    return exit_code
Language: python
Code Tokens: ['def', 'worker', '(', 'sock', ',', 'authenticated', ')', ':', 'signal', '.', 'signal', '(', 'SIGHUP', ',', 'SIG_DFL', ')', 'signal', '.', 'signal', '(', 'SIGCHLD', ',', 'SIG_DFL', ')', 'signal', '.', 'signal', '(', 'SIGTERM', ',', 'SIG_DFL', ')', '# restore the handler for SIGINT,', "# it's useful for debugging (show the stacktrace before exit)", 'signal', '.', 'signal', '(', 'SIGINT', ',', 'signal', '.', 'default_int_handler', ')', '# Read the socket using fdopen instead of socket.makefile() because the latter', '# seems to be very slow; note that we need to dup() the file descriptor because', '# otherwise writes also cause a seek that makes us miss data on the read side.', 'infile', '=', 'os', '.', 'fdopen', '(', 'os', '.', 'dup', '(', 'sock', '.', 'fileno', '(', ')', ')', ',', '"rb"', ',', '65536', ')', 'outfile', '=', 'os', '.', 'fdopen', '(', 'os', '.', 'dup', '(', 'sock', '.', 'fileno', '(', ')', ')', ',', '"wb"', ',', '65536', ')', 'if', 'not', 'authenticated', ':', 'client_secret', '=', 'UTF8Deserializer', '(', ')', '.', 'loads', '(', 'infile', ')', 'if', 'os', '.', 'environ', '[', '"PYTHON_WORKER_FACTORY_SECRET"', ']', '==', 'client_secret', ':', 'write_with_length', '(', '"ok"', '.', 'encode', '(', '"utf-8"', ')', ',', 'outfile', ')', 'outfile', '.', 'flush', '(', ')', 'else', ':', 'write_with_length', '(', '"err"', '.', 'encode', '(', '"utf-8"', ')', ',', 'outfile', ')', 'outfile', '.', 'flush', '(', ')', 'sock', '.', 'close', '(', ')', 'return', '1', 'exit_code', '=', '0', 'try', ':', 'worker_main', '(', 'infile', ',', 'outfile', ')', 'except', 'SystemExit', 'as', 'exc', ':', 'exit_code', '=', 'compute_real_exit_code', '(', 'exc', '.', 'code', ')', 'finally', ':', 'try', ':', 'outfile', '.', 'flush', '(', ')', 'except', 'Exception', ':', 'pass', 'return', 'exit_code']
Docstring: Called by a worker process after the fork().
*******__*******
Code:def portable_hash(x):\n    """\n    This function returns consistent hash code for builtin types, especially\n    for None and tuple with None.\n\n    The algorithm is similar to that one used by CPython 2.7\n\n    >>> portable_hash(None)\n    0\n    >>> portable_hash((None, 1)) & 0xffffffff\n    219750521\n    """\n\n    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:\n        raise Exception("Randomness of hash of string should be disabled via PYTHONHASHSEED")\n\n    if x is None:\n        return 0\n    if isinstance(x, tuple):\n        h = 0x345678\n        for i in x:\n            h ^= portable_hash(i)\n            h *= 1000003\n            h &= sys.maxsize\n        h ^= len(x)\n        if h == -1:\n            h = -2\n        return int(h)\n    return hash(x)
Language: python
Code Tokens: ['def', 'portable_hash', '(', 'x', ')', ':', 'if', 'sys', '.', 'version_info', '>=', '(', '3', ',', '2', ',', '3', ')', 'and', "'PYTHONHASHSEED'", 'not', 'in', 'os', '.', 'environ', ':', 'raise', 'Exception', '(', '"Randomness of hash of string should be disabled via PYTHONHASHSEED"', ')', 'if', 'x', 'is', 'None', ':', 'return', '0', 'if', 'isinstance', '(', 'x', ',', 'tuple', ')', ':', 'h', '=', '0x345678', 'for', 'i', 'in', 'x', ':', 'h', '^=', 'portable_hash', '(', 'i', ')', 'h', '*=', '1000003', 'h', '&=', 'sys', '.', 'maxsize', 'h', '^=', 'len', '(', 'x', ')', 'if', 'h', '==', '-', '1', ':', 'h', '=', '-', '2', 'return', 'int', '(', 'h', ')', 'return', 'hash', '(', 'x', ')']
Docstring: This function returns consistent hash code for builtin types, especially    for None and tuple with None.    The algorithm is similar to that one used by CPython 2.7    >>> portable_hash(None)    0    >>> portable_hash((None, 1)) & 0xffffffff    219750521
*******__*******
Code:def _parse_memory(s):\n    """\n    Parse a memory string in the format supported by Java (e.g. 1g, 200m) and\n    return the value in MiB\n\n    >>> _parse_memory("256m")\n    256\n    >>> _parse_memory("2g")\n    2048\n    """\n    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}\n    if s[-1].lower() not in units:\n        raise ValueError("invalid format: " + s)\n    return int(float(s[:-1]) * units[s[-1].lower()])
Language: python
Code Tokens: ['def', '_parse_memory', '(', 's', ')', ':', 'units', '=', '{', "'g'", ':', '1024', ',', "'m'", ':', '1', ',', "'t'", ':', '1', '<<', '20', ',', "'k'", ':', '1.0', '/', '1024', '}', 'if', 's', '[', '-', '1', ']', '.', 'lower', '(', ')', 'not', 'in', 'units', ':', 'raise', 'ValueError', '(', '"invalid format: "', '+', 's', ')', 'return', 'int', '(', 'float', '(', 's', '[', ':', '-', '1', ']', ')', '*', 'units', '[', 's', '[', '-', '1', ']', '.', 'lower', '(', ')', ']', ')']
Docstring: Parse a memory string in the format supported by Java (e.g. 1g, 200m) and    return the value in MiB    >>> _parse_memory("256m")    256    >>> _parse_memory("2g")    2048
*******__*******
Code:def ignore_unicode_prefix(f):\n    """\n    Ignore the 'u' prefix of string in doc tests, to make it works\n    in both python 2 and 3\n    """\n    if sys.version >= '3':\n        # the representation of unicode string in Python 3 does not have prefix 'u',\n        # so remove the prefix 'u' for doc tests\n        literal_re = re.compile(r"(\W|^)[uU](['])", re.UNICODE)\n        f.__doc__ = literal_re.sub(r'\1\2', f.__doc__)\n    return f
Language: python
Code Tokens: ['def', 'ignore_unicode_prefix', '(', 'f', ')', ':', 'if', 'sys', '.', 'version', '>=', "'3'", ':', "# the representation of unicode string in Python 3 does not have prefix 'u',", "# so remove the prefix 'u' for doc tests", 'literal_re', '=', 're', '.', 'compile', '(', 'r"(\\W|^)[uU]([\'])"', ',', 're', '.', 'UNICODE', ')', 'f', '.', '__doc__', '=', 'literal_re', '.', 'sub', '(', "r'\\1\\2'", ',', 'f', '.', '__doc__', ')', 'return', 'f']
Docstring: Ignore the 'u' prefix of string in doc tests, to make it works    in both python 2 and 3
*******__*******
Code:def cache(self):\n        """\n        Persist this RDD with the default storage level (C{MEMORY_ONLY}).\n        """\n        self.is_cached = True\n        self.persist(StorageLevel.MEMORY_ONLY)\n        return self
Language: python
Code Tokens: ['def', 'cache', '(', 'self', ')', ':', 'self', '.', 'is_cached', '=', 'True', 'self', '.', 'persist', '(', 'StorageLevel', '.', 'MEMORY_ONLY', ')', 'return', 'self']
Docstring: Persist this RDD with the default storage level (C{MEMORY_ONLY}).
*******__*******
Code:def persist(self, storageLevel=StorageLevel.MEMORY_ONLY):\n        """\n        Set this RDD's storage level to persist its values across operations\n        after the first time it is computed. This can only be used to assign\n        a new storage level if the RDD does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_ONLY}).\n\n        >>> rdd = sc.parallelize(["b", "a", "c"])\n        >>> rdd.persist().is_cached\n        True\n        """\n        self.is_cached = True\n        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)\n        self._jrdd.persist(javaStorageLevel)\n        return self
Language: python
Code Tokens: ['def', 'persist', '(', 'self', ',', 'storageLevel', '=', 'StorageLevel', '.', 'MEMORY_ONLY', ')', ':', 'self', '.', 'is_cached', '=', 'True', 'javaStorageLevel', '=', 'self', '.', 'ctx', '.', '_getJavaStorageLevel', '(', 'storageLevel', ')', 'self', '.', '_jrdd', '.', 'persist', '(', 'javaStorageLevel', ')', 'return', 'self']
Docstring: Set this RDD's storage level to persist its values across operations        after the first time it is computed. This can only be used to assign        a new storage level if the RDD does not have a storage level set yet.        If no storage level is specified defaults to (C{MEMORY_ONLY}).        >>> rdd = sc.parallelize(["b", "a", "c"])        >>> rdd.persist().is_cached        True
*******__*******
Code:def unpersist(self, blocking=False):\n        """\n        Mark the RDD as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. versionchanged:: 3.0.0\n           Added optional argument `blocking` to specify whether to block until all\n           blocks are deleted.\n        """\n        self.is_cached = False\n        self._jrdd.unpersist(blocking)\n        return self
Language: python
Code Tokens: ['def', 'unpersist', '(', 'self', ',', 'blocking', '=', 'False', ')', ':', 'self', '.', 'is_cached', '=', 'False', 'self', '.', '_jrdd', '.', 'unpersist', '(', 'blocking', ')', 'return', 'self']
Docstring: Mark the RDD as non-persistent, and remove all blocks for it from        memory and disk.        .. versionchanged:: 3.0.0           Added optional argument `blocking` to specify whether to block until all           blocks are deleted.
*******__*******
Code:def getCheckpointFile(self):\n        """\n        Gets the name of the file to which this RDD was checkpointed\n\n        Not defined if RDD is checkpointed locally.\n        """\n        checkpointFile = self._jrdd.rdd().getCheckpointFile()\n        if checkpointFile.isDefined():\n            return checkpointFile.get()
Language: python
Code Tokens: ['def', 'getCheckpointFile', '(', 'self', ')', ':', 'checkpointFile', '=', 'self', '.', '_jrdd', '.', 'rdd', '(', ')', '.', 'getCheckpointFile', '(', ')', 'if', 'checkpointFile', '.', 'isDefined', '(', ')', ':', 'return', 'checkpointFile', '.', 'get', '(', ')']
Docstring: Gets the name of the file to which this RDD was checkpointed        Not defined if RDD is checkpointed locally.
*******__*******
Code:def map(self, f, preservesPartitioning=False):\n        """\n        Return a new RDD by applying a function to each element of this RDD.\n\n        >>> rdd = sc.parallelize(["b", "a", "c"])\n        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n        [('a', 1), ('b', 1), ('c', 1)]\n        """\n        def func(_, iterator):\n            return map(fail_on_stopiteration(f), iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)
Language: python
Code Tokens: ['def', 'map', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', '_', ',', 'iterator', ')', ':', 'return', 'map', '(', 'fail_on_stopiteration', '(', 'f', ')', ',', 'iterator', ')', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ',', 'preservesPartitioning', ')']
Docstring: Return a new RDD by applying a function to each element of this RDD.        >>> rdd = sc.parallelize(["b", "a", "c"])        >>> sorted(rdd.map(lambda x: (x, 1)).collect())        [('a', 1), ('b', 1), ('c', 1)]
*******__*******
Code:def flatMap(self, f, preservesPartitioning=False):\n        """\n        Return a new RDD by first applying a function to all elements of this\n        RDD, and then flattening the results.\n\n        >>> rdd = sc.parallelize([2, 3, 4])\n        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n        [1, 1, 1, 2, 2, 3]\n        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n        """\n        def func(s, iterator):\n            return chain.from_iterable(map(fail_on_stopiteration(f), iterator))\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)
Language: python
Code Tokens: ['def', 'flatMap', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', 's', ',', 'iterator', ')', ':', 'return', 'chain', '.', 'from_iterable', '(', 'map', '(', 'fail_on_stopiteration', '(', 'f', ')', ',', 'iterator', ')', ')', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ',', 'preservesPartitioning', ')']
Docstring: Return a new RDD by first applying a function to all elements of this        RDD, and then flattening the results.        >>> rdd = sc.parallelize([2, 3, 4])        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())        [1, 1, 1, 2, 2, 3]        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]
*******__*******
Code:def mapPartitions(self, f, preservesPartitioning=False):\n        """\n        Return a new RDD by applying a function to each partition of this RDD.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]\n        """\n        def func(s, iterator):\n            return f(iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)
Language: python
Code Tokens: ['def', 'mapPartitions', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', 's', ',', 'iterator', ')', ':', 'return', 'f', '(', 'iterator', ')', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ',', 'preservesPartitioning', ')']
Docstring: Return a new RDD by applying a function to each partition of this RDD.        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)        >>> def f(iterator): yield sum(iterator)        >>> rdd.mapPartitions(f).collect()        [3, 7]
*******__*******
Code:def mapPartitionsWithSplit(self, f, preservesPartitioning=False):\n        """\n        Deprecated: use mapPartitionsWithIndex instead.\n\n        Return a new RDD by applying a function to each partition of this RDD,\n        while tracking the index of the original partition.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n        >>> def f(splitIndex, iterator): yield splitIndex\n        >>> rdd.mapPartitionsWithSplit(f).sum()\n        6\n        """\n        warnings.warn("mapPartitionsWithSplit is deprecated; "\n                      "use mapPartitionsWithIndex instead", DeprecationWarning, stacklevel=2)\n        return self.mapPartitionsWithIndex(f, preservesPartitioning)
Language: python
Code Tokens: ['def', 'mapPartitionsWithSplit', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'warnings', '.', 'warn', '(', '"mapPartitionsWithSplit is deprecated; "', '"use mapPartitionsWithIndex instead"', ',', 'DeprecationWarning', ',', 'stacklevel', '=', '2', ')', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'f', ',', 'preservesPartitioning', ')']
Docstring: Deprecated: use mapPartitionsWithIndex instead.        Return a new RDD by applying a function to each partition of this RDD,        while tracking the index of the original partition.        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)        >>> def f(splitIndex, iterator): yield splitIndex        >>> rdd.mapPartitionsWithSplit(f).sum()        6
*******__*******
Code:def distinct(self, numPartitions=None):\n        """\n        Return a new RDD containing the distinct elements in this RDD.\n\n        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n        [1, 2, 3]\n        """\n        return self.map(lambda x: (x, None)) \\n                   .reduceByKey(lambda x, _: x, numPartitions) \\n                   .map(lambda x: x[0])
Language: python
Code Tokens: ['def', 'distinct', '(', 'self', ',', 'numPartitions', '=', 'None', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'None', ')', ')', '.', 'reduceByKey', '(', 'lambda', 'x', ',', '_', ':', 'x', ',', 'numPartitions', ')', '.', 'map', '(', 'lambda', 'x', ':', 'x', '[', '0', ']', ')']
Docstring: Return a new RDD containing the distinct elements in this RDD.        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())        [1, 2, 3]
*******__*******
Code:def sample(self, withReplacement, fraction, seed=None):\n        """\n        Return a sampled subset of this RDD.\n\n        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n        :param fraction: expected size of the sample as a fraction of this RDD's size\n            without replacement: probability that each element is chosen; fraction must be [0, 1]\n            with replacement: expected number of times each element is chosen; fraction must be >= 0\n        :param seed: seed for the random number generator\n\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n            count of the given :class:`DataFrame`.\n\n        >>> rdd = sc.parallelize(range(100), 4)\n        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n        True\n        """\n        assert fraction >= 0.0, "Negative fraction value: %s" % fraction\n        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)
Language: python
Code Tokens: ['def', 'sample', '(', 'self', ',', 'withReplacement', ',', 'fraction', ',', 'seed', '=', 'None', ')', ':', 'assert', 'fraction', '>=', '0.0', ',', '"Negative fraction value: %s"', '%', 'fraction', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'RDDSampler', '(', 'withReplacement', ',', 'fraction', ',', 'seed', ')', '.', 'func', ',', 'True', ')']
Docstring: Return a sampled subset of this RDD.        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)        :param fraction: expected size of the sample as a fraction of this RDD's size            without replacement: probability that each element is chosen; fraction must be [0, 1]            with replacement: expected number of times each element is chosen; fraction must be >= 0        :param seed: seed for the random number generator        .. note:: This is not guaranteed to provide exactly the fraction specified of the total            count of the given :class:`DataFrame`.        >>> rdd = sc.parallelize(range(100), 4)        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14        True
*******__*******
Code:def randomSplit(self, weights, seed=None):\n        """\n        Randomly splits this RDD with the provided weights.\n\n        :param weights: weights for splits, will be normalized if they don't sum to 1\n        :param seed: random seed\n        :return: split RDDs in a list\n\n        >>> rdd = sc.parallelize(range(500), 1)\n        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n        >>> len(rdd1.collect() + rdd2.collect())\n        500\n        >>> 150 < rdd1.count() < 250\n        True\n        >>> 250 < rdd2.count() < 350\n        True\n        """\n        s = float(sum(weights))\n        cweights = [0.0]\n        for w in weights:\n            cweights.append(cweights[-1] + w / s)\n        if seed is None:\n            seed = random.randint(0, 2 ** 32 - 1)\n        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)\n                for lb, ub in zip(cweights, cweights[1:])]
Language: python
Code Tokens: ['def', 'randomSplit', '(', 'self', ',', 'weights', ',', 'seed', '=', 'None', ')', ':', 's', '=', 'float', '(', 'sum', '(', 'weights', ')', ')', 'cweights', '=', '[', '0.0', ']', 'for', 'w', 'in', 'weights', ':', 'cweights', '.', 'append', '(', 'cweights', '[', '-', '1', ']', '+', 'w', '/', 's', ')', 'if', 'seed', 'is', 'None', ':', 'seed', '=', 'random', '.', 'randint', '(', '0', ',', '2', '**', '32', '-', '1', ')', 'return', '[', 'self', '.', 'mapPartitionsWithIndex', '(', 'RDDRangeSampler', '(', 'lb', ',', 'ub', ',', 'seed', ')', '.', 'func', ',', 'True', ')', 'for', 'lb', ',', 'ub', 'in', 'zip', '(', 'cweights', ',', 'cweights', '[', '1', ':', ']', ')', ']']
Docstring: Randomly splits this RDD with the provided weights.        :param weights: weights for splits, will be normalized if they don't sum to 1        :param seed: random seed        :return: split RDDs in a list        >>> rdd = sc.parallelize(range(500), 1)        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)        >>> len(rdd1.collect() + rdd2.collect())        500        >>> 150 < rdd1.count() < 250        True        >>> 250 < rdd2.count() < 350        True
*******__*******
Code:def takeSample(self, withReplacement, num, seed=None):\n        """\n        Return a fixed-size sampled subset of this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> rdd = sc.parallelize(range(0, 10))\n        >>> len(rdd.takeSample(True, 20, 1))\n        20\n        >>> len(rdd.takeSample(False, 5, 2))\n        5\n        >>> len(rdd.takeSample(False, 15, 3))\n        10\n        """\n        numStDev = 10.0\n\n        if num < 0:\n            raise ValueError("Sample size cannot be negative.")\n        elif num == 0:\n            return []\n\n        initialCount = self.count()\n        if initialCount == 0:\n            return []\n\n        rand = random.Random(seed)\n\n        if (not withReplacement) and num >= initialCount:\n            # shuffle current RDD and return\n            samples = self.collect()\n            rand.shuffle(samples)\n            return samples\n\n        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))\n        if num > maxSampleSize:\n            raise ValueError(\n                "Sample size cannot be greater than %d." % maxSampleSize)\n\n        fraction = RDD._computeFractionForSampleSize(\n            num, initialCount, withReplacement)\n        samples = self.sample(withReplacement, fraction, seed).collect()\n\n        # If the first sample didn't turn out large enough, keep trying to take samples;\n        # this shouldn't happen often because we use a big multiplier for their initial size.\n        # See: scala/spark/RDD.scala\n        while len(samples) < num:\n            # TODO: add log warning for when more than one iteration was run\n            seed = rand.randint(0, sys.maxsize)\n            samples = self.sample(withReplacement, fraction, seed).collect()\n\n        rand.shuffle(samples)\n\n        return samples[0:num]
Language: python
Code Tokens: ['def', 'takeSample', '(', 'self', ',', 'withReplacement', ',', 'num', ',', 'seed', '=', 'None', ')', ':', 'numStDev', '=', '10.0', 'if', 'num', '<', '0', ':', 'raise', 'ValueError', '(', '"Sample size cannot be negative."', ')', 'elif', 'num', '==', '0', ':', 'return', '[', ']', 'initialCount', '=', 'self', '.', 'count', '(', ')', 'if', 'initialCount', '==', '0', ':', 'return', '[', ']', 'rand', '=', 'random', '.', 'Random', '(', 'seed', ')', 'if', '(', 'not', 'withReplacement', ')', 'and', 'num', '>=', 'initialCount', ':', '# shuffle current RDD and return', 'samples', '=', 'self', '.', 'collect', '(', ')', 'rand', '.', 'shuffle', '(', 'samples', ')', 'return', 'samples', 'maxSampleSize', '=', 'sys', '.', 'maxsize', '-', 'int', '(', 'numStDev', '*', 'sqrt', '(', 'sys', '.', 'maxsize', ')', ')', 'if', 'num', '>', 'maxSampleSize', ':', 'raise', 'ValueError', '(', '"Sample size cannot be greater than %d."', '%', 'maxSampleSize', ')', 'fraction', '=', 'RDD', '.', '_computeFractionForSampleSize', '(', 'num', ',', 'initialCount', ',', 'withReplacement', ')', 'samples', '=', 'self', '.', 'sample', '(', 'withReplacement', ',', 'fraction', ',', 'seed', ')', '.', 'collect', '(', ')', "# If the first sample didn't turn out large enough, keep trying to take samples;", "# this shouldn't happen often because we use a big multiplier for their initial size.", '# See: scala/spark/RDD.scala', 'while', 'len', '(', 'samples', ')', '<', 'num', ':', '# TODO: add log warning for when more than one iteration was run', 'seed', '=', 'rand', '.', 'randint', '(', '0', ',', 'sys', '.', 'maxsize', ')', 'samples', '=', 'self', '.', 'sample', '(', 'withReplacement', ',', 'fraction', ',', 'seed', ')', '.', 'collect', '(', ')', 'rand', '.', 'shuffle', '(', 'samples', ')', 'return', 'samples', '[', '0', ':', 'num', ']']
Docstring: Return a fixed-size sampled subset of this RDD.        .. note:: This method should only be used if the resulting array is expected            to be small, as all the data is loaded into the driver's memory.        >>> rdd = sc.parallelize(range(0, 10))        >>> len(rdd.takeSample(True, 20, 1))        20        >>> len(rdd.takeSample(False, 5, 2))        5        >>> len(rdd.takeSample(False, 15, 3))        10
*******__*******
Code:def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):\n        """\n        Returns a sampling rate that guarantees a sample of\n        size >= sampleSizeLowerBound 99.99% of the time.\n\n        How the sampling rate is determined:\n        Let p = num / total, where num is the sample size and total is the\n        total number of data points in the RDD. We're trying to compute\n        q > p such that\n          - when sampling with replacement, we're drawing each data point\n            with prob_i ~ Pois(q), where we want to guarantee\n            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to\n            total), i.e. the failure rate of not having a sufficiently large\n            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient\n            to guarantee 0.9999 success rate for num > 12, but we need a\n            slightly larger q (9 empirically determined).\n          - when sampling without replacement, we're drawing each data point\n            with prob_i ~ Binomial(total, fraction) and our choice of q\n            guarantees 1-delta, or 0.9999 success rate, where success rate is\n            defined the same as in sampling with replacement.\n        """\n        fraction = float(sampleSizeLowerBound) / total\n        if withReplacement:\n            numStDev = 5\n            if (sampleSizeLowerBound < 12):\n                numStDev = 9\n            return fraction + numStDev * sqrt(fraction / total)\n        else:\n            delta = 0.00005\n            gamma = - log(delta) / total\n            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))
Language: python
Code Tokens: ['def', '_computeFractionForSampleSize', '(', 'sampleSizeLowerBound', ',', 'total', ',', 'withReplacement', ')', ':', 'fraction', '=', 'float', '(', 'sampleSizeLowerBound', ')', '/', 'total', 'if', 'withReplacement', ':', 'numStDev', '=', '5', 'if', '(', 'sampleSizeLowerBound', '<', '12', ')', ':', 'numStDev', '=', '9', 'return', 'fraction', '+', 'numStDev', '*', 'sqrt', '(', 'fraction', '/', 'total', ')', 'else', ':', 'delta', '=', '0.00005', 'gamma', '=', '-', 'log', '(', 'delta', ')', '/', 'total', 'return', 'min', '(', '1', ',', 'fraction', '+', 'gamma', '+', 'sqrt', '(', 'gamma', '*', 'gamma', '+', '2', '*', 'gamma', '*', 'fraction', ')', ')']
Docstring: Returns a sampling rate that guarantees a sample of        size >= sampleSizeLowerBound 99.99% of the time.        How the sampling rate is determined:        Let p = num / total, where num is the sample size and total is the        total number of data points in the RDD. We're trying to compute        q > p such that          - when sampling with replacement, we're drawing each data point            with prob_i ~ Pois(q), where we want to guarantee            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to            total), i.e. the failure rate of not having a sufficiently large            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient            to guarantee 0.9999 success rate for num > 12, but we need a            slightly larger q (9 empirically determined).          - when sampling without replacement, we're drawing each data point            with prob_i ~ Binomial(total, fraction) and our choice of q            guarantees 1-delta, or 0.9999 success rate, where success rate is            defined the same as in sampling with replacement.
*******__*******
Code:def union(self, other):\n        """\n        Return the union of this RDD and another one.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\n        >>> rdd.union(rdd).collect()\n        [1, 1, 2, 3, 1, 1, 2, 3]\n        """\n        if self._jrdd_deserializer == other._jrdd_deserializer:\n            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,\n                      self._jrdd_deserializer)\n        else:\n            # These RDDs contain data in different serialized formats, so we\n            # must normalize them to the default serializer.\n            self_copy = self._reserialize()\n            other_copy = other._reserialize()\n            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,\n                      self.ctx.serializer)\n        if (self.partitioner == other.partitioner and\n                self.getNumPartitions() == rdd.getNumPartitions()):\n            rdd.partitioner = self.partitioner\n        return rdd
Language: python
Code Tokens: ['def', 'union', '(', 'self', ',', 'other', ')', ':', 'if', 'self', '.', '_jrdd_deserializer', '==', 'other', '.', '_jrdd_deserializer', ':', 'rdd', '=', 'RDD', '(', 'self', '.', '_jrdd', '.', 'union', '(', 'other', '.', '_jrdd', ')', ',', 'self', '.', 'ctx', ',', 'self', '.', '_jrdd_deserializer', ')', 'else', ':', '# These RDDs contain data in different serialized formats, so we', '# must normalize them to the default serializer.', 'self_copy', '=', 'self', '.', '_reserialize', '(', ')', 'other_copy', '=', 'other', '.', '_reserialize', '(', ')', 'rdd', '=', 'RDD', '(', 'self_copy', '.', '_jrdd', '.', 'union', '(', 'other_copy', '.', '_jrdd', ')', ',', 'self', '.', 'ctx', ',', 'self', '.', 'ctx', '.', 'serializer', ')', 'if', '(', 'self', '.', 'partitioner', '==', 'other', '.', 'partitioner', 'and', 'self', '.', 'getNumPartitions', '(', ')', '==', 'rdd', '.', 'getNumPartitions', '(', ')', ')', ':', 'rdd', '.', 'partitioner', '=', 'self', '.', 'partitioner', 'return', 'rdd']
Docstring: Return the union of this RDD and another one.        >>> rdd = sc.parallelize([1, 1, 2, 3])        >>> rdd.union(rdd).collect()        [1, 1, 2, 3, 1, 1, 2, 3]
*******__*******
Code:def intersection(self, other):\n        """\n        Return the intersection of this RDD and another one. The output will\n        not contain any duplicate elements, even if the input RDDs did.\n\n        .. note:: This method performs a shuffle internally.\n\n        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n        >>> rdd1.intersection(rdd2).collect()\n        [1, 2, 3]\n        """\n        return self.map(lambda v: (v, None)) \\n            .cogroup(other.map(lambda v: (v, None))) \\n            .filter(lambda k_vs: all(k_vs[1])) \\n            .keys()
Language: python
Code Tokens: ['def', 'intersection', '(', 'self', ',', 'other', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'v', ':', '(', 'v', ',', 'None', ')', ')', '.', 'cogroup', '(', 'other', '.', 'map', '(', 'lambda', 'v', ':', '(', 'v', ',', 'None', ')', ')', ')', '.', 'filter', '(', 'lambda', 'k_vs', ':', 'all', '(', 'k_vs', '[', '1', ']', ')', ')', '.', 'keys', '(', ')']
Docstring: Return the intersection of this RDD and another one. The output will        not contain any duplicate elements, even if the input RDDs did.        .. note:: This method performs a shuffle internally.        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])        >>> rdd1.intersection(rdd2).collect()        [1, 2, 3]
*******__*******
Code:def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,\n                                           ascending=True, keyfunc=lambda x: x):\n        """\n        Repartition the RDD according to the given partitioner and, within each resulting partition,\n        sort records by their keys.\n\n        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n        >>> rdd2.glom().collect()\n        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n        """\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        memory = _parse_memory(self.ctx._conf.get("spark.python.worker.memory", "512m"))\n        serializer = self._jrdd_deserializer\n\n        def sortPartition(iterator):\n            sort = ExternalSorter(memory * 0.9, serializer).sorted\n            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))\n\n        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)
Language: python
Code Tokens: ['def', 'repartitionAndSortWithinPartitions', '(', 'self', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ',', 'ascending', '=', 'True', ',', 'keyfunc', '=', 'lambda', 'x', ':', 'x', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_defaultReducePartitions', '(', ')', 'memory', '=', '_parse_memory', '(', 'self', '.', 'ctx', '.', '_conf', '.', 'get', '(', '"spark.python.worker.memory"', ',', '"512m"', ')', ')', 'serializer', '=', 'self', '.', '_jrdd_deserializer', 'def', 'sortPartition', '(', 'iterator', ')', ':', 'sort', '=', 'ExternalSorter', '(', 'memory', '*', '0.9', ',', 'serializer', ')', '.', 'sorted', 'return', 'iter', '(', 'sort', '(', 'iterator', ',', 'key', '=', 'lambda', 'k_v', ':', 'keyfunc', '(', 'k_v', '[', '0', ']', ')', ',', 'reverse', '=', '(', 'not', 'ascending', ')', ')', ')', 'return', 'self', '.', 'partitionBy', '(', 'numPartitions', ',', 'partitionFunc', ')', '.', 'mapPartitions', '(', 'sortPartition', ',', 'True', ')']
Docstring: Repartition the RDD according to the given partitioner and, within each resulting partition,        sort records by their keys.        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)        >>> rdd2.glom().collect()        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]
*******__*******
Code:def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n        """\n        Sorts this RDD, which is assumed to consist of (key, value) pairs.\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortByKey().first()\n        ('1', 3)\n        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n        """\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        memory = self._memory_limit()\n        serializer = self._jrdd_deserializer\n\n        def sortPartition(iterator):\n            sort = ExternalSorter(memory * 0.9, serializer).sorted\n            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n\n        if numPartitions == 1:\n            if self.getNumPartitions() > 1:\n                self = self.coalesce(1)\n            return self.mapPartitions(sortPartition, True)\n\n        # first compute the boundary of each part via sampling: we want to partition\n        # the key-space into bins such that the bins have roughly the same\n        # number of (key, value) pairs falling into them\n        rddSize = self.count()\n        if not rddSize:\n            return self  # empty RDD\n        maxSampleSize = numPartitions * 20.0  # constant from Spark's RangePartitioner\n        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)\n        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()\n        samples = sorted(samples, key=keyfunc)\n\n        # we have numPartitions many parts but one of the them has\n        # an implicit boundary\n        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]\n                  for i in range(0, numPartitions - 1)]\n\n        def rangePartitioner(k):\n            p = bisect.bisect_left(bounds, keyfunc(k))\n            if ascending:\n                return p\n            else:\n                return numPartitions - 1 - p\n\n        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)
Language: python
Code Tokens: ['def', 'sortByKey', '(', 'self', ',', 'ascending', '=', 'True', ',', 'numPartitions', '=', 'None', ',', 'keyfunc', '=', 'lambda', 'x', ':', 'x', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_defaultReducePartitions', '(', ')', 'memory', '=', 'self', '.', '_memory_limit', '(', ')', 'serializer', '=', 'self', '.', '_jrdd_deserializer', 'def', 'sortPartition', '(', 'iterator', ')', ':', 'sort', '=', 'ExternalSorter', '(', 'memory', '*', '0.9', ',', 'serializer', ')', '.', 'sorted', 'return', 'iter', '(', 'sort', '(', 'iterator', ',', 'key', '=', 'lambda', 'kv', ':', 'keyfunc', '(', 'kv', '[', '0', ']', ')', ',', 'reverse', '=', '(', 'not', 'ascending', ')', ')', ')', 'if', 'numPartitions', '==', '1', ':', 'if', 'self', '.', 'getNumPartitions', '(', ')', '>', '1', ':', 'self', '=', 'self', '.', 'coalesce', '(', '1', ')', 'return', 'self', '.', 'mapPartitions', '(', 'sortPartition', ',', 'True', ')', '# first compute the boundary of each part via sampling: we want to partition', '# the key-space into bins such that the bins have roughly the same', '# number of (key, value) pairs falling into them', 'rddSize', '=', 'self', '.', 'count', '(', ')', 'if', 'not', 'rddSize', ':', 'return', 'self', '# empty RDD', 'maxSampleSize', '=', 'numPartitions', '*', '20.0', "# constant from Spark's RangePartitioner", 'fraction', '=', 'min', '(', 'maxSampleSize', '/', 'max', '(', 'rddSize', ',', '1', ')', ',', '1.0', ')', 'samples', '=', 'self', '.', 'sample', '(', 'False', ',', 'fraction', ',', '1', ')', '.', 'map', '(', 'lambda', 'kv', ':', 'kv', '[', '0', ']', ')', '.', 'collect', '(', ')', 'samples', '=', 'sorted', '(', 'samples', ',', 'key', '=', 'keyfunc', ')', '# we have numPartitions many parts but one of the them has', '# an implicit boundary', 'bounds', '=', '[', 'samples', '[', 'int', '(', 'len', '(', 'samples', ')', '*', '(', 'i', '+', '1', ')', '/', 'numPartitions', ')', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'numPartitions', '-', '1', ')', ']', 'def', 'rangePartitioner', '(', 'k', ')', ':', 'p', '=', 'bisect', '.', 'bisect_left', '(', 'bounds', ',', 'keyfunc', '(', 'k', ')', ')', 'if', 'ascending', ':', 'return', 'p', 'else', ':', 'return', 'numPartitions', '-', '1', '-', 'p', 'return', 'self', '.', 'partitionBy', '(', 'numPartitions', ',', 'rangePartitioner', ')', '.', 'mapPartitions', '(', 'sortPartition', ',', 'True', ')']
Docstring: Sorts this RDD, which is assumed to consist of (key, value) pairs.        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]        >>> sc.parallelize(tmp).sortByKey().first()        ('1', 3)        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]
*******__*******
Code:def sortBy(self, keyfunc, ascending=True, numPartitions=None):\n        """\n        Sorts this RDD by the given keyfunc\n\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n        """\n        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()
Language: python
Code Tokens: ['def', 'sortBy', '(', 'self', ',', 'keyfunc', ',', 'ascending', '=', 'True', ',', 'numPartitions', '=', 'None', ')', ':', 'return', 'self', '.', 'keyBy', '(', 'keyfunc', ')', '.', 'sortByKey', '(', 'ascending', ',', 'numPartitions', ')', '.', 'values', '(', ')']
Docstring: Sorts this RDD by the given keyfunc        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
*******__*******
Code:def cartesian(self, other):\n        """\n        Return the Cartesian product of this RDD and another one, that is, the\n        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n        C{b} is in C{other}.\n\n        >>> rdd = sc.parallelize([1, 2])\n        >>> sorted(rdd.cartesian(rdd).collect())\n        [(1, 1), (1, 2), (2, 1), (2, 2)]\n        """\n        # Due to batching, we can't use the Java cartesian method.\n        deserializer = CartesianDeserializer(self._jrdd_deserializer,\n                                             other._jrdd_deserializer)\n        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)
Language: python
Code Tokens: ['def', 'cartesian', '(', 'self', ',', 'other', ')', ':', "# Due to batching, we can't use the Java cartesian method.", 'deserializer', '=', 'CartesianDeserializer', '(', 'self', '.', '_jrdd_deserializer', ',', 'other', '.', '_jrdd_deserializer', ')', 'return', 'RDD', '(', 'self', '.', '_jrdd', '.', 'cartesian', '(', 'other', '.', '_jrdd', ')', ',', 'self', '.', 'ctx', ',', 'deserializer', ')']
Docstring: Return the Cartesian product of this RDD and another one, that is, the        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and        C{b} is in C{other}.        >>> rdd = sc.parallelize([1, 2])        >>> sorted(rdd.cartesian(rdd).collect())        [(1, 1), (1, 2), (2, 1), (2, 2)]
*******__*******
Code:def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash):\n        """\n        Return an RDD of grouped items.\n\n        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n        >>> result = rdd.groupBy(lambda x: x % 2).collect()\n        >>> sorted([(x, sorted(y)) for (x, y) in result])\n        [(0, [2, 8]), (1, [1, 1, 3, 5])]\n        """\n        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)
Language: python
Code Tokens: ['def', 'groupBy', '(', 'self', ',', 'f', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'f', '(', 'x', ')', ',', 'x', ')', ')', '.', 'groupByKey', '(', 'numPartitions', ',', 'partitionFunc', ')']
Docstring: Return an RDD of grouped items.        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])        >>> result = rdd.groupBy(lambda x: x % 2).collect()        >>> sorted([(x, sorted(y)) for (x, y) in result])        [(0, [2, 8]), (1, [1, 1, 3, 5])]
*******__*******
Code:def pipe(self, command, env=None, checkCode=False):\n        """\n        Return an RDD created by piping elements to a forked external process.\n\n        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n        [u'1', u'2', u'', u'3']\n\n        :param checkCode: whether or not to check the return value of the shell command.\n        """\n        if env is None:\n            env = dict()\n\n        def func(iterator):\n            pipe = Popen(\n                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)\n\n            def pipe_objs(out):\n                for obj in iterator:\n                    s = unicode(obj).rstrip('\n') + '\n'\n                    out.write(s.encode('utf-8'))\n                out.close()\n            Thread(target=pipe_objs, args=[pipe.stdin]).start()\n\n            def check_return_code():\n                pipe.wait()\n                if checkCode and pipe.returncode:\n                    raise Exception("Pipe function `%s' exited "\n                                    "with error code %d" % (command, pipe.returncode))\n                else:\n                    for i in range(0):\n                        yield i\n            return (x.rstrip(b'\n').decode('utf-8') for x in\n                    chain(iter(pipe.stdout.readline, b''), check_return_code()))\n        return self.mapPartitions(func)
Language: python
Code Tokens: ['def', 'pipe', '(', 'self', ',', 'command', ',', 'env', '=', 'None', ',', 'checkCode', '=', 'False', ')', ':', 'if', 'env', 'is', 'None', ':', 'env', '=', 'dict', '(', ')', 'def', 'func', '(', 'iterator', ')', ':', 'pipe', '=', 'Popen', '(', 'shlex', '.', 'split', '(', 'command', ')', ',', 'env', '=', 'env', ',', 'stdin', '=', 'PIPE', ',', 'stdout', '=', 'PIPE', ')', 'def', 'pipe_objs', '(', 'out', ')', ':', 'for', 'obj', 'in', 'iterator', ':', 's', '=', 'unicode', '(', 'obj', ')', '.', 'rstrip', '(', "'\\n'", ')', '+', "'\\n'", 'out', '.', 'write', '(', 's', '.', 'encode', '(', "'utf-8'", ')', ')', 'out', '.', 'close', '(', ')', 'Thread', '(', 'target', '=', 'pipe_objs', ',', 'args', '=', '[', 'pipe', '.', 'stdin', ']', ')', '.', 'start', '(', ')', 'def', 'check_return_code', '(', ')', ':', 'pipe', '.', 'wait', '(', ')', 'if', 'checkCode', 'and', 'pipe', '.', 'returncode', ':', 'raise', 'Exception', '(', '"Pipe function `%s\' exited "', '"with error code %d"', '%', '(', 'command', ',', 'pipe', '.', 'returncode', ')', ')', 'else', ':', 'for', 'i', 'in', 'range', '(', '0', ')', ':', 'yield', 'i', 'return', '(', 'x', '.', 'rstrip', '(', "b'\\n'", ')', '.', 'decode', '(', "'utf-8'", ')', 'for', 'x', 'in', 'chain', '(', 'iter', '(', 'pipe', '.', 'stdout', '.', 'readline', ',', "b''", ')', ',', 'check_return_code', '(', ')', ')', ')', 'return', 'self', '.', 'mapPartitions', '(', 'func', ')']
Docstring: Return an RDD created by piping elements to a forked external process.        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()        [u'1', u'2', u'', u'3']        :param checkCode: whether or not to check the return value of the shell command.
*******__*******
Code:def foreach(self, f):\n        """\n        Applies a function to all elements of this RDD.\n\n        >>> def f(x): print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n        """\n        f = fail_on_stopiteration(f)\n\n        def processPartition(iterator):\n            for x in iterator:\n                f(x)\n            return iter([])\n        self.mapPartitions(processPartition).count()
Language: python
Code Tokens: ['def', 'foreach', '(', 'self', ',', 'f', ')', ':', 'f', '=', 'fail_on_stopiteration', '(', 'f', ')', 'def', 'processPartition', '(', 'iterator', ')', ':', 'for', 'x', 'in', 'iterator', ':', 'f', '(', 'x', ')', 'return', 'iter', '(', '[', ']', ')', 'self', '.', 'mapPartitions', '(', 'processPartition', ')', '.', 'count', '(', ')']
Docstring: Applies a function to all elements of this RDD.        >>> def f(x): print(x)        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)
*******__*******
Code:def foreachPartition(self, f):\n        """\n        Applies a function to each partition of this RDD.\n\n        >>> def f(iterator):\n        ...     for x in iterator:\n        ...          print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n        """\n        def func(it):\n            r = f(it)\n            try:\n                return iter(r)\n            except TypeError:\n                return iter([])\n        self.mapPartitions(func).count()
Language: python
Code Tokens: ['def', 'foreachPartition', '(', 'self', ',', 'f', ')', ':', 'def', 'func', '(', 'it', ')', ':', 'r', '=', 'f', '(', 'it', ')', 'try', ':', 'return', 'iter', '(', 'r', ')', 'except', 'TypeError', ':', 'return', 'iter', '(', '[', ']', ')', 'self', '.', 'mapPartitions', '(', 'func', ')', '.', 'count', '(', ')']
Docstring: Applies a function to each partition of this RDD.        >>> def f(iterator):        ...     for x in iterator:        ...          print(x)        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)
*******__*******
Code:def collect(self):\n        """\n        Return a list that contains all of the elements in this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n        """\n        with SCCallSiteSync(self.context) as css:\n            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n        return list(_load_from_socket(sock_info, self._jrdd_deserializer))
Language: python
Code Tokens: ['def', 'collect', '(', 'self', ')', ':', 'with', 'SCCallSiteSync', '(', 'self', '.', 'context', ')', 'as', 'css', ':', 'sock_info', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'collectAndServe', '(', 'self', '.', '_jrdd', '.', 'rdd', '(', ')', ')', 'return', 'list', '(', '_load_from_socket', '(', 'sock_info', ',', 'self', '.', '_jrdd_deserializer', ')', ')']
Docstring: Return a list that contains all of the elements in this RDD.        .. note:: This method should only be used if the resulting array is expected            to be small, as all the data is loaded into the driver's memory.
*******__*******
Code:def reduce(self, f):\n        """\n        Reduces the elements of this RDD using the specified commutative and\n        associative binary operator. Currently reduces partitions locally.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n        15\n        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n        10\n        >>> sc.parallelize([]).reduce(add)\n        Traceback (most recent call last):\n            ...\n        ValueError: Can not reduce() empty RDD\n        """\n        f = fail_on_stopiteration(f)\n\n        def func(iterator):\n            iterator = iter(iterator)\n            try:\n                initial = next(iterator)\n            except StopIteration:\n                return\n            yield reduce(f, iterator, initial)\n\n        vals = self.mapPartitions(func).collect()\n        if vals:\n            return reduce(f, vals)\n        raise ValueError("Can not reduce() empty RDD")
Language: python
Code Tokens: ['def', 'reduce', '(', 'self', ',', 'f', ')', ':', 'f', '=', 'fail_on_stopiteration', '(', 'f', ')', 'def', 'func', '(', 'iterator', ')', ':', 'iterator', '=', 'iter', '(', 'iterator', ')', 'try', ':', 'initial', '=', 'next', '(', 'iterator', ')', 'except', 'StopIteration', ':', 'return', 'yield', 'reduce', '(', 'f', ',', 'iterator', ',', 'initial', ')', 'vals', '=', 'self', '.', 'mapPartitions', '(', 'func', ')', '.', 'collect', '(', ')', 'if', 'vals', ':', 'return', 'reduce', '(', 'f', ',', 'vals', ')', 'raise', 'ValueError', '(', '"Can not reduce() empty RDD"', ')']
Docstring: Reduces the elements of this RDD using the specified commutative and        associative binary operator. Currently reduces partitions locally.        >>> from operator import add        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)        15        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)        10        >>> sc.parallelize([]).reduce(add)        Traceback (most recent call last):            ...        ValueError: Can not reduce() empty RDD
*******__*******
Code:def treeReduce(self, f, depth=2):\n        """\n        Reduces the elements of this RDD in a multi-level tree pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeReduce(add)\n        -5\n        >>> rdd.treeReduce(add, 1)\n        -5\n        >>> rdd.treeReduce(add, 2)\n        -5\n        >>> rdd.treeReduce(add, 5)\n        -5\n        >>> rdd.treeReduce(add, 10)\n        -5\n        """\n        if depth < 1:\n            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)\n\n        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.\n\n        def op(x, y):\n            if x[1]:\n                return y\n            elif y[1]:\n                return x\n            else:\n                return f(x[0], y[0]), False\n\n        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)\n        if reduced[1]:\n            raise ValueError("Cannot reduce empty RDD.")\n        return reduced[0]
Language: python
Code Tokens: ['def', 'treeReduce', '(', 'self', ',', 'f', ',', 'depth', '=', '2', ')', ':', 'if', 'depth', '<', '1', ':', 'raise', 'ValueError', '(', '"Depth cannot be smaller than 1 but got %d."', '%', 'depth', ')', 'zeroValue', '=', 'None', ',', 'True', '# Use the second entry to indicate whether this is a dummy value.', 'def', 'op', '(', 'x', ',', 'y', ')', ':', 'if', 'x', '[', '1', ']', ':', 'return', 'y', 'elif', 'y', '[', '1', ']', ':', 'return', 'x', 'else', ':', 'return', 'f', '(', 'x', '[', '0', ']', ',', 'y', '[', '0', ']', ')', ',', 'False', 'reduced', '=', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'False', ')', ')', '.', 'treeAggregate', '(', 'zeroValue', ',', 'op', ',', 'op', ',', 'depth', ')', 'if', 'reduced', '[', '1', ']', ':', 'raise', 'ValueError', '(', '"Cannot reduce empty RDD."', ')', 'return', 'reduced', '[', '0', ']']
Docstring: Reduces the elements of this RDD in a multi-level tree pattern.        :param depth: suggested depth of the tree (default: 2)        >>> add = lambda x, y: x + y        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)        >>> rdd.treeReduce(add)        -5        >>> rdd.treeReduce(add, 1)        -5        >>> rdd.treeReduce(add, 2)        -5        >>> rdd.treeReduce(add, 5)        -5        >>> rdd.treeReduce(add, 10)        -5
*******__*******
Code:def fold(self, zeroValue, op):\n        """\n        Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given associative function and a neutral "zero value."\n\n        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        This behaves somewhat differently from fold operations implemented\n        for non-distributed collections in functional languages like Scala.\n        This fold operation may be applied to partitions individually, and then\n        fold those results into the final result, rather than apply the fold\n        to each element sequentially in some defined ordering. For functions\n        that are not commutative, the result may differ from that of a fold\n        applied to a non-distributed collection.\n\n        >>> from operator import add\n        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n        15\n        """\n        op = fail_on_stopiteration(op)\n\n        def func(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = op(acc, obj)\n            yield acc\n        # collecting result of mapPartitions here ensures that the copy of\n        # zeroValue provided to each partition is unique from the one provided\n        # to the final reduce call\n        vals = self.mapPartitions(func).collect()\n        return reduce(op, vals, zeroValue)
Language: python
Code Tokens: ['def', 'fold', '(', 'self', ',', 'zeroValue', ',', 'op', ')', ':', 'op', '=', 'fail_on_stopiteration', '(', 'op', ')', 'def', 'func', '(', 'iterator', ')', ':', 'acc', '=', 'zeroValue', 'for', 'obj', 'in', 'iterator', ':', 'acc', '=', 'op', '(', 'acc', ',', 'obj', ')', 'yield', 'acc', '# collecting result of mapPartitions here ensures that the copy of', '# zeroValue provided to each partition is unique from the one provided', '# to the final reduce call', 'vals', '=', 'self', '.', 'mapPartitions', '(', 'func', ')', '.', 'collect', '(', ')', 'return', 'reduce', '(', 'op', ',', 'vals', ',', 'zeroValue', ')']
Docstring: Aggregate the elements of each partition, and then the results for all        the partitions, using a given associative function and a neutral "zero value."        The function C{op(t1, t2)} is allowed to modify C{t1} and return it        as its result value to avoid object allocation; however, it should not        modify C{t2}.        This behaves somewhat differently from fold operations implemented        for non-distributed collections in functional languages like Scala.        This fold operation may be applied to partitions individually, and then        fold those results into the final result, rather than apply the fold        to each element sequentially in some defined ordering. For functions        that are not commutative, the result may differ from that of a fold        applied to a non-distributed collection.        >>> from operator import add        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)        15
*******__*******
Code:def aggregate(self, zeroValue, seqOp, combOp):\n        """\n        Aggregate the elements of each partition, and then the results for all\n        the partitions, using a given combine functions and a neutral "zero\n        value."\n\n        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n        as its result value to avoid object allocation; however, it should not\n        modify C{t2}.\n\n        The first function (seqOp) can return a different result type, U, than\n        the type of this RDD. Thus, we need one operation for merging a T into\n        an U and one operation for merging two U\n\n        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n        (10, 4)\n        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n        (0, 0)\n        """\n        seqOp = fail_on_stopiteration(seqOp)\n        combOp = fail_on_stopiteration(combOp)\n\n        def func(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = seqOp(acc, obj)\n            yield acc\n        # collecting result of mapPartitions here ensures that the copy of\n        # zeroValue provided to each partition is unique from the one provided\n        # to the final reduce call\n        vals = self.mapPartitions(func).collect()\n        return reduce(combOp, vals, zeroValue)
Language: python
Code Tokens: ['def', 'aggregate', '(', 'self', ',', 'zeroValue', ',', 'seqOp', ',', 'combOp', ')', ':', 'seqOp', '=', 'fail_on_stopiteration', '(', 'seqOp', ')', 'combOp', '=', 'fail_on_stopiteration', '(', 'combOp', ')', 'def', 'func', '(', 'iterator', ')', ':', 'acc', '=', 'zeroValue', 'for', 'obj', 'in', 'iterator', ':', 'acc', '=', 'seqOp', '(', 'acc', ',', 'obj', ')', 'yield', 'acc', '# collecting result of mapPartitions here ensures that the copy of', '# zeroValue provided to each partition is unique from the one provided', '# to the final reduce call', 'vals', '=', 'self', '.', 'mapPartitions', '(', 'func', ')', '.', 'collect', '(', ')', 'return', 'reduce', '(', 'combOp', ',', 'vals', ',', 'zeroValue', ')']
Docstring: Aggregate the elements of each partition, and then the results for all        the partitions, using a given combine functions and a neutral "zero        value."        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it        as its result value to avoid object allocation; however, it should not        modify C{t2}.        The first function (seqOp) can return a different result type, U, than        the type of this RDD. Thus, we need one operation for merging a T into        an U and one operation for merging two U        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)        (10, 4)        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)        (0, 0)
*******__*******
Code:def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):\n        """\n        Aggregates the elements of this RDD in a multi-level tree\n        pattern.\n\n        :param depth: suggested depth of the tree (default: 2)\n\n        >>> add = lambda x, y: x + y\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n        >>> rdd.treeAggregate(0, add, add)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 1)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 2)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 5)\n        -5\n        >>> rdd.treeAggregate(0, add, add, 10)\n        -5\n        """\n        if depth < 1:\n            raise ValueError("Depth cannot be smaller than 1 but got %d." % depth)\n\n        if self.getNumPartitions() == 0:\n            return zeroValue\n\n        def aggregatePartition(iterator):\n            acc = zeroValue\n            for obj in iterator:\n                acc = seqOp(acc, obj)\n            yield acc\n\n        partiallyAggregated = self.mapPartitions(aggregatePartition)\n        numPartitions = partiallyAggregated.getNumPartitions()\n        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)\n        # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree\n        # aggregation.\n        while numPartitions > scale + numPartitions / scale:\n            numPartitions /= scale\n            curNumPartitions = int(numPartitions)\n\n            def mapPartition(i, iterator):\n                for obj in iterator:\n                    yield (i % curNumPartitions, obj)\n\n            partiallyAggregated = partiallyAggregated \\n                .mapPartitionsWithIndex(mapPartition) \\n                .reduceByKey(combOp, curNumPartitions) \\n                .values()\n\n        return partiallyAggregated.reduce(combOp)
Language: python
Code Tokens: ['def', 'treeAggregate', '(', 'self', ',', 'zeroValue', ',', 'seqOp', ',', 'combOp', ',', 'depth', '=', '2', ')', ':', 'if', 'depth', '<', '1', ':', 'raise', 'ValueError', '(', '"Depth cannot be smaller than 1 but got %d."', '%', 'depth', ')', 'if', 'self', '.', 'getNumPartitions', '(', ')', '==', '0', ':', 'return', 'zeroValue', 'def', 'aggregatePartition', '(', 'iterator', ')', ':', 'acc', '=', 'zeroValue', 'for', 'obj', 'in', 'iterator', ':', 'acc', '=', 'seqOp', '(', 'acc', ',', 'obj', ')', 'yield', 'acc', 'partiallyAggregated', '=', 'self', '.', 'mapPartitions', '(', 'aggregatePartition', ')', 'numPartitions', '=', 'partiallyAggregated', '.', 'getNumPartitions', '(', ')', 'scale', '=', 'max', '(', 'int', '(', 'ceil', '(', 'pow', '(', 'numPartitions', ',', '1.0', '/', 'depth', ')', ')', ')', ',', '2', ')', "# If creating an extra level doesn't help reduce the wall-clock time, we stop the tree", '# aggregation.', 'while', 'numPartitions', '>', 'scale', '+', 'numPartitions', '/', 'scale', ':', 'numPartitions', '/=', 'scale', 'curNumPartitions', '=', 'int', '(', 'numPartitions', ')', 'def', 'mapPartition', '(', 'i', ',', 'iterator', ')', ':', 'for', 'obj', 'in', 'iterator', ':', 'yield', '(', 'i', '%', 'curNumPartitions', ',', 'obj', ')', 'partiallyAggregated', '=', 'partiallyAggregated', '.', 'mapPartitionsWithIndex', '(', 'mapPartition', ')', '.', 'reduceByKey', '(', 'combOp', ',', 'curNumPartitions', ')', '.', 'values', '(', ')', 'return', 'partiallyAggregated', '.', 'reduce', '(', 'combOp', ')']
Docstring: Aggregates the elements of this RDD in a multi-level tree        pattern.        :param depth: suggested depth of the tree (default: 2)        >>> add = lambda x, y: x + y        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)        >>> rdd.treeAggregate(0, add, add)        -5        >>> rdd.treeAggregate(0, add, add, 1)        -5        >>> rdd.treeAggregate(0, add, add, 2)        -5        >>> rdd.treeAggregate(0, add, add, 5)        -5        >>> rdd.treeAggregate(0, add, add, 10)        -5
*******__*******
Code:def max(self, key=None):\n        """\n        Find the maximum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n        >>> rdd.max()\n        43.0\n        >>> rdd.max(key=str)\n        5.0\n        """\n        if key is None:\n            return self.reduce(max)\n        return self.reduce(lambda a, b: max(a, b, key=key))
Language: python
Code Tokens: ['def', 'max', '(', 'self', ',', 'key', '=', 'None', ')', ':', 'if', 'key', 'is', 'None', ':', 'return', 'self', '.', 'reduce', '(', 'max', ')', 'return', 'self', '.', 'reduce', '(', 'lambda', 'a', ',', 'b', ':', 'max', '(', 'a', ',', 'b', ',', 'key', '=', 'key', ')', ')']
Docstring: Find the maximum item in this RDD.        :param key: A function used to generate key for comparing        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])        >>> rdd.max()        43.0        >>> rdd.max(key=str)        5.0
*******__*******
Code:def min(self, key=None):\n        """\n        Find the minimum item in this RDD.\n\n        :param key: A function used to generate key for comparing\n\n        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n        >>> rdd.min()\n        2.0\n        >>> rdd.min(key=str)\n        10.0\n        """\n        if key is None:\n            return self.reduce(min)\n        return self.reduce(lambda a, b: min(a, b, key=key))
Language: python
Code Tokens: ['def', 'min', '(', 'self', ',', 'key', '=', 'None', ')', ':', 'if', 'key', 'is', 'None', ':', 'return', 'self', '.', 'reduce', '(', 'min', ')', 'return', 'self', '.', 'reduce', '(', 'lambda', 'a', ',', 'b', ':', 'min', '(', 'a', ',', 'b', ',', 'key', '=', 'key', ')', ')']
Docstring: Find the minimum item in this RDD.        :param key: A function used to generate key for comparing        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])        >>> rdd.min()        2.0        >>> rdd.min(key=str)        10.0
*******__*******
Code:def sum(self):\n        """\n        Add up the elements in this RDD.\n\n        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n        6.0\n        """\n        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)
Language: python
Code Tokens: ['def', 'sum', '(', 'self', ')', ':', 'return', 'self', '.', 'mapPartitions', '(', 'lambda', 'x', ':', '[', 'sum', '(', 'x', ')', ']', ')', '.', 'fold', '(', '0', ',', 'operator', '.', 'add', ')']
Docstring: Add up the elements in this RDD.        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()        6.0
*******__*******
Code:def stats(self):\n        """\n        Return a L{StatCounter} object that captures the mean, variance\n        and count of the RDD's elements in one operation.\n        """\n        def redFunc(left_counter, right_counter):\n            return left_counter.mergeStats(right_counter)\n\n        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)
Language: python
Code Tokens: ['def', 'stats', '(', 'self', ')', ':', 'def', 'redFunc', '(', 'left_counter', ',', 'right_counter', ')', ':', 'return', 'left_counter', '.', 'mergeStats', '(', 'right_counter', ')', 'return', 'self', '.', 'mapPartitions', '(', 'lambda', 'i', ':', '[', 'StatCounter', '(', 'i', ')', ']', ')', '.', 'reduce', '(', 'redFunc', ')']
Docstring: Return a L{StatCounter} object that captures the mean, variance        and count of the RDD's elements in one operation.
*******__*******
Code:def histogram(self, buckets):\n        """\n        Compute a histogram using the provided buckets. The buckets\n        are all open to the right except for the last which is closed.\n        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n        and 50 we would have a histogram of 1,0,1.\n\n        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n        this can be switched from an O(log n) inseration to O(1) per\n        element (where n is the number of buckets).\n\n        Buckets must be sorted, not contain any duplicates, and have\n        at least two elements.\n\n        If `buckets` is a number, it will generate buckets which are\n        evenly spaced between the minimum and maximum of the RDD. For\n        example, if the min value is 0 and the max is 100, given `buckets`\n        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n        be at least 1. An exception is raised if the RDD contains infinity.\n        If the elements in the RDD do not vary (max == min), a single bucket\n        will be used.\n\n        The return value is a tuple of buckets and histogram.\n\n        >>> rdd = sc.parallelize(range(51))\n        >>> rdd.histogram(2)\n        ([0, 25, 50], [25, 26])\n        >>> rdd.histogram([0, 5, 25, 50])\n        ([0, 5, 25, 50], [5, 20, 26])\n        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n        ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n        >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])\n        >>> rdd.histogram(("a", "b", "c"))\n        (('a', 'b', 'c'), [2, 2])\n        """\n\n        if isinstance(buckets, int):\n            if buckets < 1:\n                raise ValueError("number of buckets must be >= 1")\n\n            # filter out non-comparable elements\n            def comparable(x):\n                if x is None:\n                    return False\n                if type(x) is float and isnan(x):\n                    return False\n                return True\n\n            filtered = self.filter(comparable)\n\n            # faster than stats()\n            def minmax(a, b):\n                return min(a[0], b[0]), max(a[1], b[1])\n            try:\n                minv, maxv = filtered.map(lambda x: (x, x)).reduce(minmax)\n            except TypeError as e:\n                if " empty " in str(e):\n                    raise ValueError("can not generate buckets from empty RDD")\n                raise\n\n            if minv == maxv or buckets == 1:\n                return [minv, maxv], [filtered.count()]\n\n            try:\n                inc = (maxv - minv) / buckets\n            except TypeError:\n                raise TypeError("Can not generate buckets with non-number in RDD")\n\n            if isinf(inc):\n                raise ValueError("Can not generate buckets with infinite value")\n\n            # keep them as integer if possible\n            inc = int(inc)\n            if inc * buckets != maxv - minv:\n                inc = (maxv - minv) * 1.0 / buckets\n\n            buckets = [i * inc + minv for i in range(buckets)]\n            buckets.append(maxv)  # fix accumulated error\n            even = True\n\n        elif isinstance(buckets, (list, tuple)):\n            if len(buckets) < 2:\n                raise ValueError("buckets should have more than one value")\n\n            if any(i is None or isinstance(i, float) and isnan(i) for i in buckets):\n                raise ValueError("can not have None or NaN in buckets")\n\n            if sorted(buckets) != list(buckets):\n                raise ValueError("buckets should be sorted")\n\n            if len(set(buckets)) != len(buckets):\n                raise ValueError("buckets should not contain duplicated values")\n\n            minv = buckets[0]\n            maxv = buckets[-1]\n            even = False\n            inc = None\n            try:\n                steps = [buckets[i + 1] - buckets[i] for i in range(len(buckets) - 1)]\n            except TypeError:\n                pass  # objects in buckets do not support '-'\n            else:\n                if max(steps) - min(steps) < 1e-10:  # handle precision errors\n                    even = True\n                    inc = (maxv - minv) / (len(buckets) - 1)\n\n        else:\n            raise TypeError("buckets should be a list or tuple or number(int or long)")\n\n        def histogram(iterator):\n            counters = [0] * len(buckets)\n            for i in iterator:\n                if i is None or (type(i) is float and isnan(i)) or i > maxv or i < minv:\n                    continue\n                t = (int((i - minv) / inc) if even\n                     else bisect.bisect_right(buckets, i) - 1)\n                counters[t] += 1\n            # add last two together\n            last = counters.pop()\n            counters[-1] += last\n            return [counters]\n\n        def mergeCounters(a, b):\n            return [i + j for i, j in zip(a, b)]\n\n        return buckets, self.mapPartitions(histogram).reduce(mergeCounters)
Language: python
Code Tokens: ['def', 'histogram', '(', 'self', ',', 'buckets', ')', ':', 'if', 'isinstance', '(', 'buckets', ',', 'int', ')', ':', 'if', 'buckets', '<', '1', ':', 'raise', 'ValueError', '(', '"number of buckets must be >= 1"', ')', '# filter out non-comparable elements', 'def', 'comparable', '(', 'x', ')', ':', 'if', 'x', 'is', 'None', ':', 'return', 'False', 'if', 'type', '(', 'x', ')', 'is', 'float', 'and', 'isnan', '(', 'x', ')', ':', 'return', 'False', 'return', 'True', 'filtered', '=', 'self', '.', 'filter', '(', 'comparable', ')', '# faster than stats()', 'def', 'minmax', '(', 'a', ',', 'b', ')', ':', 'return', 'min', '(', 'a', '[', '0', ']', ',', 'b', '[', '0', ']', ')', ',', 'max', '(', 'a', '[', '1', ']', ',', 'b', '[', '1', ']', ')', 'try', ':', 'minv', ',', 'maxv', '=', 'filtered', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'x', ')', ')', '.', 'reduce', '(', 'minmax', ')', 'except', 'TypeError', 'as', 'e', ':', 'if', '" empty "', 'in', 'str', '(', 'e', ')', ':', 'raise', 'ValueError', '(', '"can not generate buckets from empty RDD"', ')', 'raise', 'if', 'minv', '==', 'maxv', 'or', 'buckets', '==', '1', ':', 'return', '[', 'minv', ',', 'maxv', ']', ',', '[', 'filtered', '.', 'count', '(', ')', ']', 'try', ':', 'inc', '=', '(', 'maxv', '-', 'minv', ')', '/', 'buckets', 'except', 'TypeError', ':', 'raise', 'TypeError', '(', '"Can not generate buckets with non-number in RDD"', ')', 'if', 'isinf', '(', 'inc', ')', ':', 'raise', 'ValueError', '(', '"Can not generate buckets with infinite value"', ')', '# keep them as integer if possible', 'inc', '=', 'int', '(', 'inc', ')', 'if', 'inc', '*', 'buckets', '!=', 'maxv', '-', 'minv', ':', 'inc', '=', '(', 'maxv', '-', 'minv', ')', '*', '1.0', '/', 'buckets', 'buckets', '=', '[', 'i', '*', 'inc', '+', 'minv', 'for', 'i', 'in', 'range', '(', 'buckets', ')', ']', 'buckets', '.', 'append', '(', 'maxv', ')', '# fix accumulated error', 'even', '=', 'True', 'elif', 'isinstance', '(', 'buckets', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'if', 'len', '(', 'buckets', ')', '<', '2', ':', 'raise', 'ValueError', '(', '"buckets should have more than one value"', ')', 'if', 'any', '(', 'i', 'is', 'None', 'or', 'isinstance', '(', 'i', ',', 'float', ')', 'and', 'isnan', '(', 'i', ')', 'for', 'i', 'in', 'buckets', ')', ':', 'raise', 'ValueError', '(', '"can not have None or NaN in buckets"', ')', 'if', 'sorted', '(', 'buckets', ')', '!=', 'list', '(', 'buckets', ')', ':', 'raise', 'ValueError', '(', '"buckets should be sorted"', ')', 'if', 'len', '(', 'set', '(', 'buckets', ')', ')', '!=', 'len', '(', 'buckets', ')', ':', 'raise', 'ValueError', '(', '"buckets should not contain duplicated values"', ')', 'minv', '=', 'buckets', '[', '0', ']', 'maxv', '=', 'buckets', '[', '-', '1', ']', 'even', '=', 'False', 'inc', '=', 'None', 'try', ':', 'steps', '=', '[', 'buckets', '[', 'i', '+', '1', ']', '-', 'buckets', '[', 'i', ']', 'for', 'i', 'in', 'range', '(', 'len', '(', 'buckets', ')', '-', '1', ')', ']', 'except', 'TypeError', ':', 'pass', "# objects in buckets do not support '-'", 'else', ':', 'if', 'max', '(', 'steps', ')', '-', 'min', '(', 'steps', ')', '<', '1e-10', ':', '# handle precision errors', 'even', '=', 'True', 'inc', '=', '(', 'maxv', '-', 'minv', ')', '/', '(', 'len', '(', 'buckets', ')', '-', '1', ')', 'else', ':', 'raise', 'TypeError', '(', '"buckets should be a list or tuple or number(int or long)"', ')', 'def', 'histogram', '(', 'iterator', ')', ':', 'counters', '=', '[', '0', ']', '*', 'len', '(', 'buckets', ')', 'for', 'i', 'in', 'iterator', ':', 'if', 'i', 'is', 'None', 'or', '(', 'type', '(', 'i', ')', 'is', 'float', 'and', 'isnan', '(', 'i', ')', ')', 'or', 'i', '>', 'maxv', 'or', 'i', '<', 'minv', ':', 'continue', 't', '=', '(', 'int', '(', '(', 'i', '-', 'minv', ')', '/', 'inc', ')', 'if', 'even', 'else', 'bisect', '.', 'bisect_right', '(', 'buckets', ',', 'i', ')', '-', '1', ')', 'counters', '[', 't', ']', '+=', '1', '# add last two together', 'last', '=', 'counters', '.', 'pop', '(', ')', 'counters', '[', '-', '1', ']', '+=', 'last', 'return', '[', 'counters', ']', 'def', 'mergeCounters', '(', 'a', ',', 'b', ')', ':', 'return', '[', 'i', '+', 'j', 'for', 'i', ',', 'j', 'in', 'zip', '(', 'a', ',', 'b', ')', ']', 'return', 'buckets', ',', 'self', '.', 'mapPartitions', '(', 'histogram', ')', '.', 'reduce', '(', 'mergeCounters', ')']
Docstring: Compute a histogram using the provided buckets. The buckets        are all open to the right except for the last which is closed.        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1        and 50 we would have a histogram of 1,0,1.        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),        this can be switched from an O(log n) inseration to O(1) per        element (where n is the number of buckets).        Buckets must be sorted, not contain any duplicates, and have        at least two elements.        If `buckets` is a number, it will generate buckets which are        evenly spaced between the minimum and maximum of the RDD. For        example, if the min value is 0 and the max is 100, given `buckets`        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must        be at least 1. An exception is raised if the RDD contains infinity.        If the elements in the RDD do not vary (max == min), a single bucket        will be used.        The return value is a tuple of buckets and histogram.        >>> rdd = sc.parallelize(range(51))        >>> rdd.histogram(2)        ([0, 25, 50], [25, 26])        >>> rdd.histogram([0, 5, 25, 50])        ([0, 5, 25, 50], [5, 20, 26])        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets        ([0, 15, 30, 45, 60], [15, 15, 15, 6])        >>> rdd = sc.parallelize(["ab", "ac", "b", "bd", "ef"])        >>> rdd.histogram(("a", "b", "c"))        (('a', 'b', 'c'), [2, 2])
*******__*******
Code:def countByValue(self):\n        """\n        Return the count of each unique value in this RDD as a dictionary of\n        (value, count) pairs.\n\n        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n        [(1, 2), (2, 3)]\n        """\n        def countPartition(iterator):\n            counts = defaultdict(int)\n            for obj in iterator:\n                counts[obj] += 1\n            yield counts\n\n        def mergeMaps(m1, m2):\n            for k, v in m2.items():\n                m1[k] += v\n            return m1\n        return self.mapPartitions(countPartition).reduce(mergeMaps)
Language: python
Code Tokens: ['def', 'countByValue', '(', 'self', ')', ':', 'def', 'countPartition', '(', 'iterator', ')', ':', 'counts', '=', 'defaultdict', '(', 'int', ')', 'for', 'obj', 'in', 'iterator', ':', 'counts', '[', 'obj', ']', '+=', '1', 'yield', 'counts', 'def', 'mergeMaps', '(', 'm1', ',', 'm2', ')', ':', 'for', 'k', ',', 'v', 'in', 'm2', '.', 'items', '(', ')', ':', 'm1', '[', 'k', ']', '+=', 'v', 'return', 'm1', 'return', 'self', '.', 'mapPartitions', '(', 'countPartition', ')', '.', 'reduce', '(', 'mergeMaps', ')']
Docstring: Return the count of each unique value in this RDD as a dictionary of        (value, count) pairs.        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())        [(1, 2), (2, 3)]
*******__*******
Code:def top(self, num, key=None):\n        """\n        Get the top N elements from an RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        .. note:: It returns the list sorted in descending order.\n\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n        [12]\n        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n        [6, 5]\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n        [4, 3, 2]\n        """\n        def topIterator(iterator):\n            yield heapq.nlargest(num, iterator, key=key)\n\n        def merge(a, b):\n            return heapq.nlargest(num, a + b, key=key)\n\n        return self.mapPartitions(topIterator).reduce(merge)
Language: python
Code Tokens: ['def', 'top', '(', 'self', ',', 'num', ',', 'key', '=', 'None', ')', ':', 'def', 'topIterator', '(', 'iterator', ')', ':', 'yield', 'heapq', '.', 'nlargest', '(', 'num', ',', 'iterator', ',', 'key', '=', 'key', ')', 'def', 'merge', '(', 'a', ',', 'b', ')', ':', 'return', 'heapq', '.', 'nlargest', '(', 'num', ',', 'a', '+', 'b', ',', 'key', '=', 'key', ')', 'return', 'self', '.', 'mapPartitions', '(', 'topIterator', ')', '.', 'reduce', '(', 'merge', ')']
Docstring: Get the top N elements from an RDD.        .. note:: This method should only be used if the resulting array is expected            to be small, as all the data is loaded into the driver's memory.        .. note:: It returns the list sorted in descending order.        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)        [12]        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)        [6, 5]        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)        [4, 3, 2]
*******__*******
Code:def takeOrdered(self, num, key=None):\n        """\n        Get the N elements from an RDD ordered in ascending order or as\n        specified by the optional key function.\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n        [1, 2, 3, 4, 5, 6]\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n        [10, 9, 7, 6, 5, 4]\n        """\n\n        def merge(a, b):\n            return heapq.nsmallest(num, a + b, key)\n\n        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)
Language: python
Code Tokens: ['def', 'takeOrdered', '(', 'self', ',', 'num', ',', 'key', '=', 'None', ')', ':', 'def', 'merge', '(', 'a', ',', 'b', ')', ':', 'return', 'heapq', '.', 'nsmallest', '(', 'num', ',', 'a', '+', 'b', ',', 'key', ')', 'return', 'self', '.', 'mapPartitions', '(', 'lambda', 'it', ':', '[', 'heapq', '.', 'nsmallest', '(', 'num', ',', 'it', ',', 'key', ')', ']', ')', '.', 'reduce', '(', 'merge', ')']
Docstring: Get the N elements from an RDD ordered in ascending order or as        specified by the optional key function.        .. note:: this method should only be used if the resulting array is expected            to be small, as all the data is loaded into the driver's memory.        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)        [1, 2, 3, 4, 5, 6]        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)        [10, 9, 7, 6, 5, 4]
*******__*******
Code:def take(self, num):\n        """\n        Take the first num elements of the RDD.\n\n        It works by first scanning one partition, and use the results from\n        that partition to estimate the number of additional partitions needed\n        to satisfy the limit.\n\n        Translated from the Scala implementation in RDD#take().\n\n        .. note:: this method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n        [2, 3]\n        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n        [2, 3, 4, 5, 6]\n        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n        [91, 92, 93]\n        """\n        items = []\n        totalParts = self.getNumPartitions()\n        partsScanned = 0\n\n        while len(items) < num and partsScanned < totalParts:\n            # The number of partitions to try in this iteration.\n            # It is ok for this number to be greater than totalParts because\n            # we actually cap it at totalParts in runJob.\n            numPartsToTry = 1\n            if partsScanned > 0:\n                # If we didn't find any rows after the previous iteration,\n                # quadruple and retry.  Otherwise, interpolate the number of\n                # partitions we need to try, but overestimate it by 50%.\n                # We also cap the estimation in the end.\n                if len(items) == 0:\n                    numPartsToTry = partsScanned * 4\n                else:\n                    # the first parameter of max is >=1 whenever partsScanned >= 2\n                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned\n                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)\n\n            left = num - len(items)\n\n            def takeUpToNumLeft(iterator):\n                iterator = iter(iterator)\n                taken = 0\n                while taken < left:\n                    try:\n                        yield next(iterator)\n                    except StopIteration:\n                        return\n                    taken += 1\n\n            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))\n            res = self.context.runJob(self, takeUpToNumLeft, p)\n\n            items += res\n            partsScanned += numPartsToTry\n\n        return items[:num]
Language: python
Code Tokens: ['def', 'take', '(', 'self', ',', 'num', ')', ':', 'items', '=', '[', ']', 'totalParts', '=', 'self', '.', 'getNumPartitions', '(', ')', 'partsScanned', '=', '0', 'while', 'len', '(', 'items', ')', '<', 'num', 'and', 'partsScanned', '<', 'totalParts', ':', '# The number of partitions to try in this iteration.', '# It is ok for this number to be greater than totalParts because', '# we actually cap it at totalParts in runJob.', 'numPartsToTry', '=', '1', 'if', 'partsScanned', '>', '0', ':', "# If we didn't find any rows after the previous iteration,", '# quadruple and retry.  Otherwise, interpolate the number of', '# partitions we need to try, but overestimate it by 50%.', '# We also cap the estimation in the end.', 'if', 'len', '(', 'items', ')', '==', '0', ':', 'numPartsToTry', '=', 'partsScanned', '*', '4', 'else', ':', '# the first parameter of max is >=1 whenever partsScanned >= 2', 'numPartsToTry', '=', 'int', '(', '1.5', '*', 'num', '*', 'partsScanned', '/', 'len', '(', 'items', ')', ')', '-', 'partsScanned', 'numPartsToTry', '=', 'min', '(', 'max', '(', 'numPartsToTry', ',', '1', ')', ',', 'partsScanned', '*', '4', ')', 'left', '=', 'num', '-', 'len', '(', 'items', ')', 'def', 'takeUpToNumLeft', '(', 'iterator', ')', ':', 'iterator', '=', 'iter', '(', 'iterator', ')', 'taken', '=', '0', 'while', 'taken', '<', 'left', ':', 'try', ':', 'yield', 'next', '(', 'iterator', ')', 'except', 'StopIteration', ':', 'return', 'taken', '+=', '1', 'p', '=', 'range', '(', 'partsScanned', ',', 'min', '(', 'partsScanned', '+', 'numPartsToTry', ',', 'totalParts', ')', ')', 'res', '=', 'self', '.', 'context', '.', 'runJob', '(', 'self', ',', 'takeUpToNumLeft', ',', 'p', ')', 'items', '+=', 'res', 'partsScanned', '+=', 'numPartsToTry', 'return', 'items', '[', ':', 'num', ']']
Docstring: Take the first num elements of the RDD.        It works by first scanning one partition, and use the results from        that partition to estimate the number of additional partitions needed        to satisfy the limit.        Translated from the Scala implementation in RDD#take().        .. note:: this method should only be used if the resulting array is expected            to be small, as all the data is loaded into the driver's memory.        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)        [2, 3]        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)        [2, 3, 4, 5, 6]        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)        [91, 92, 93]
*******__*******
Code:def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):\n        """\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n        converted for output using either user specified converters or, by default,\n        L{org.apache.spark.api.python.JavaToWritableConverter}.\n\n        :param conf: Hadoop job configuration, passed in as a dict\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)\n        """\n        jconf = self.ctx._dictToJavaMap(conf)\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,\n                                                    keyConverter, valueConverter, True)
Language: python
Code Tokens: ['def', 'saveAsNewAPIHadoopDataset', '(', 'self', ',', 'conf', ',', 'keyConverter', '=', 'None', ',', 'valueConverter', '=', 'None', ')', ':', 'jconf', '=', 'self', '.', 'ctx', '.', '_dictToJavaMap', '(', 'conf', ')', 'pickledRDD', '=', 'self', '.', '_pickled', '(', ')', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'saveAsHadoopDataset', '(', 'pickledRDD', '.', '_jrdd', ',', 'True', ',', 'jconf', ',', 'keyConverter', ',', 'valueConverter', ',', 'True', ')']
Docstring: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are        converted for output using either user specified converters or, by default,        L{org.apache.spark.api.python.JavaToWritableConverter}.        :param conf: Hadoop job configuration, passed in as a dict        :param keyConverter: (None by default)        :param valueConverter: (None by default)
*******__*******
Code:def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,\n                               keyConverter=None, valueConverter=None, conf=None):\n        """\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n        will be inferred if not specified. Keys and values are converted for output using either\n        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n\n        :param path: path to Hadoop file\n        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n               (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")\n        :param keyClass: fully qualified classname of key Writable class\n               (e.g. "org.apache.hadoop.io.IntWritable", None by default)\n        :param valueClass: fully qualified classname of value Writable class\n               (e.g. "org.apache.hadoop.io.Text", None by default)\n        :param keyConverter: (None by default)\n        :param valueConverter: (None by default)\n        :param conf: Hadoop job configuration, passed in as a dict (None by default)\n        """\n        jconf = self.ctx._dictToJavaMap(conf)\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,\n                                                       outputFormatClass,\n                                                       keyClass, valueClass,\n                                                       keyConverter, valueConverter, jconf)
Language: python
Code Tokens: ['def', 'saveAsNewAPIHadoopFile', '(', 'self', ',', 'path', ',', 'outputFormatClass', ',', 'keyClass', '=', 'None', ',', 'valueClass', '=', 'None', ',', 'keyConverter', '=', 'None', ',', 'valueConverter', '=', 'None', ',', 'conf', '=', 'None', ')', ':', 'jconf', '=', 'self', '.', 'ctx', '.', '_dictToJavaMap', '(', 'conf', ')', 'pickledRDD', '=', 'self', '.', '_pickled', '(', ')', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'saveAsNewAPIHadoopFile', '(', 'pickledRDD', '.', '_jrdd', ',', 'True', ',', 'path', ',', 'outputFormatClass', ',', 'keyClass', ',', 'valueClass', ',', 'keyConverter', ',', 'valueConverter', ',', 'jconf', ')']
Docstring: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types        will be inferred if not specified. Keys and values are converted for output using either        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.        :param path: path to Hadoop file        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat               (e.g. "org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat")        :param keyClass: fully qualified classname of key Writable class               (e.g. "org.apache.hadoop.io.IntWritable", None by default)        :param valueClass: fully qualified classname of value Writable class               (e.g. "org.apache.hadoop.io.Text", None by default)        :param keyConverter: (None by default)        :param valueConverter: (None by default)        :param conf: Hadoop job configuration, passed in as a dict (None by default)
*******__*******
Code:def saveAsSequenceFile(self, path, compressionCodecClass=None):\n        """\n        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n        RDD's key and value types. The mechanism is as follows:\n\n            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n            2. Keys and values of this Java RDD are converted to Writables and written out.\n\n        :param path: path to sequence file\n        :param compressionCodecClass: (None by default)\n        """\n        pickledRDD = self._pickled()\n        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,\n                                                   path, compressionCodecClass)
Language: python
Code Tokens: ['def', 'saveAsSequenceFile', '(', 'self', ',', 'path', ',', 'compressionCodecClass', '=', 'None', ')', ':', 'pickledRDD', '=', 'self', '.', '_pickled', '(', ')', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'saveAsSequenceFile', '(', 'pickledRDD', '.', '_jrdd', ',', 'True', ',', 'path', ',', 'compressionCodecClass', ')']
Docstring: Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the        RDD's key and value types. The mechanism is as follows:            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.            2. Keys and values of this Java RDD are converted to Writables and written out.        :param path: path to sequence file        :param compressionCodecClass: (None by default)
*******__*******
Code:def saveAsPickleFile(self, path, batchSize=10):\n        """\n        Save this RDD as a SequenceFile of serialized objects. The serializer\n        used is L{pyspark.serializers.PickleSerializer}, default batch size\n        is 10.\n\n        >>> tmpFile = NamedTemporaryFile(delete=True)\n        >>> tmpFile.close()\n        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n        ['1', '2', 'rdd', 'spark']\n        """\n        if batchSize == 0:\n            ser = AutoBatchedSerializer(PickleSerializer())\n        else:\n            ser = BatchedSerializer(PickleSerializer(), batchSize)\n        self._reserialize(ser)._jrdd.saveAsObjectFile(path)
Language: python
Code Tokens: ['def', 'saveAsPickleFile', '(', 'self', ',', 'path', ',', 'batchSize', '=', '10', ')', ':', 'if', 'batchSize', '==', '0', ':', 'ser', '=', 'AutoBatchedSerializer', '(', 'PickleSerializer', '(', ')', ')', 'else', ':', 'ser', '=', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ',', 'batchSize', ')', 'self', '.', '_reserialize', '(', 'ser', ')', '.', '_jrdd', '.', 'saveAsObjectFile', '(', 'path', ')']
Docstring: Save this RDD as a SequenceFile of serialized objects. The serializer        used is L{pyspark.serializers.PickleSerializer}, default batch size        is 10.        >>> tmpFile = NamedTemporaryFile(delete=True)        >>> tmpFile.close()        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())        ['1', '2', 'rdd', 'spark']
*******__*******
Code:def saveAsTextFile(self, path, compressionCodecClass=None):\n        """\n        Save this RDD as a text file, using string representations of elements.\n\n        @param path: path to text file\n        @param compressionCodecClass: (None by default) string i.e.\n            "org.apache.hadoop.io.compress.GzipCodec"\n\n        >>> tempFile = NamedTemporaryFile(delete=True)\n        >>> tempFile.close()\n        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n        >>> from fileinput import input\n        >>> from glob import glob\n        >>> ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))\n        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n\n        Empty lines are tolerated when saving to text files.\n\n        >>> tempFile2 = NamedTemporaryFile(delete=True)\n        >>> tempFile2.close()\n        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n        >>> ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))\n        '\\n\\n\\nbar\\nfoo\\n'\n\n        Using compressionCodecClass\n\n        >>> tempFile3 = NamedTemporaryFile(delete=True)\n        >>> tempFile3.close()\n        >>> codec = "org.apache.hadoop.io.compress.GzipCodec"\n        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n        >>> from fileinput import input, hook_compressed\n        >>> result = sorted(input(glob(tempFile3.name + "/part*.gz"), openhook=hook_compressed))\n        >>> b''.join(result).decode('utf-8')\n        u'bar\\nfoo\\n'\n        """\n        def func(split, iterator):\n            for x in iterator:\n                if not isinstance(x, (unicode, bytes)):\n                    x = unicode(x)\n                if isinstance(x, unicode):\n                    x = x.encode("utf-8")\n                yield x\n        keyed = self.mapPartitionsWithIndex(func)\n        keyed._bypass_serializer = True\n        if compressionCodecClass:\n            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)\n            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)\n        else:\n            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)
Language: python
Code Tokens: ['def', 'saveAsTextFile', '(', 'self', ',', 'path', ',', 'compressionCodecClass', '=', 'None', ')', ':', 'def', 'func', '(', 'split', ',', 'iterator', ')', ':', 'for', 'x', 'in', 'iterator', ':', 'if', 'not', 'isinstance', '(', 'x', ',', '(', 'unicode', ',', 'bytes', ')', ')', ':', 'x', '=', 'unicode', '(', 'x', ')', 'if', 'isinstance', '(', 'x', ',', 'unicode', ')', ':', 'x', '=', 'x', '.', 'encode', '(', '"utf-8"', ')', 'yield', 'x', 'keyed', '=', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ')', 'keyed', '.', '_bypass_serializer', '=', 'True', 'if', 'compressionCodecClass', ':', 'compressionCodec', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'java', '.', 'lang', '.', 'Class', '.', 'forName', '(', 'compressionCodecClass', ')', 'keyed', '.', '_jrdd', '.', 'map', '(', 'self', '.', 'ctx', '.', '_jvm', '.', 'BytesToString', '(', ')', ')', '.', 'saveAsTextFile', '(', 'path', ',', 'compressionCodec', ')', 'else', ':', 'keyed', '.', '_jrdd', '.', 'map', '(', 'self', '.', 'ctx', '.', '_jvm', '.', 'BytesToString', '(', ')', ')', '.', 'saveAsTextFile', '(', 'path', ')']
Docstring: Save this RDD as a text file, using string representations of elements.        @param path: path to text file        @param compressionCodecClass: (None by default) string i.e.            "org.apache.hadoop.io.compress.GzipCodec"        >>> tempFile = NamedTemporaryFile(delete=True)        >>> tempFile.close()        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)        >>> from fileinput import input        >>> from glob import glob        >>> ''.join(sorted(input(glob(tempFile.name + "/part-0000*"))))        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'        Empty lines are tolerated when saving to text files.        >>> tempFile2 = NamedTemporaryFile(delete=True)        >>> tempFile2.close()        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)        >>> ''.join(sorted(input(glob(tempFile2.name + "/part-0000*"))))        '\\n\\n\\nbar\\nfoo\\n'        Using compressionCodecClass        >>> tempFile3 = NamedTemporaryFile(delete=True)        >>> tempFile3.close()        >>> codec = "org.apache.hadoop.io.compress.GzipCodec"        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)        >>> from fileinput import input, hook_compressed        >>> result = sorted(input(glob(tempFile3.name + "/part*.gz"), openhook=hook_compressed))        >>> b''.join(result).decode('utf-8')        u'bar\\nfoo\\n'
*******__*******
Code:def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash):\n        """\n        Merge the values for each key using an associative and commutative reduce function.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a "combiner" in MapReduce.\n\n        Output will be partitioned with C{numPartitions} partitions, or\n        the default parallelism level if C{numPartitions} is not specified.\n        Default partitioner is hash-partition.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> sorted(rdd.reduceByKey(add).collect())\n        [('a', 2), ('b', 1)]\n        """\n        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)
Language: python
Code Tokens: ['def', 'reduceByKey', '(', 'self', ',', 'func', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'return', 'self', '.', 'combineByKey', '(', 'lambda', 'x', ':', 'x', ',', 'func', ',', 'func', ',', 'numPartitions', ',', 'partitionFunc', ')']
Docstring: Merge the values for each key using an associative and commutative reduce function.        This will also perform the merging locally on each mapper before        sending results to a reducer, similarly to a "combiner" in MapReduce.        Output will be partitioned with C{numPartitions} partitions, or        the default parallelism level if C{numPartitions} is not specified.        Default partitioner is hash-partition.        >>> from operator import add        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])        >>> sorted(rdd.reduceByKey(add).collect())        [('a', 2), ('b', 1)]
*******__*******
Code:def reduceByKeyLocally(self, func):\n        """\n        Merge the values for each key using an associative and commutative reduce function, but\n        return the results immediately to the master as a dictionary.\n\n        This will also perform the merging locally on each mapper before\n        sending results to a reducer, similarly to a "combiner" in MapReduce.\n\n        >>> from operator import add\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> sorted(rdd.reduceByKeyLocally(add).items())\n        [('a', 2), ('b', 1)]\n        """\n        func = fail_on_stopiteration(func)\n\n        def reducePartition(iterator):\n            m = {}\n            for k, v in iterator:\n                m[k] = func(m[k], v) if k in m else v\n            yield m\n\n        def mergeMaps(m1, m2):\n            for k, v in m2.items():\n                m1[k] = func(m1[k], v) if k in m1 else v\n            return m1\n        return self.mapPartitions(reducePartition).reduce(mergeMaps)
Language: python
Code Tokens: ['def', 'reduceByKeyLocally', '(', 'self', ',', 'func', ')', ':', 'func', '=', 'fail_on_stopiteration', '(', 'func', ')', 'def', 'reducePartition', '(', 'iterator', ')', ':', 'm', '=', '{', '}', 'for', 'k', ',', 'v', 'in', 'iterator', ':', 'm', '[', 'k', ']', '=', 'func', '(', 'm', '[', 'k', ']', ',', 'v', ')', 'if', 'k', 'in', 'm', 'else', 'v', 'yield', 'm', 'def', 'mergeMaps', '(', 'm1', ',', 'm2', ')', ':', 'for', 'k', ',', 'v', 'in', 'm2', '.', 'items', '(', ')', ':', 'm1', '[', 'k', ']', '=', 'func', '(', 'm1', '[', 'k', ']', ',', 'v', ')', 'if', 'k', 'in', 'm1', 'else', 'v', 'return', 'm1', 'return', 'self', '.', 'mapPartitions', '(', 'reducePartition', ')', '.', 'reduce', '(', 'mergeMaps', ')']
Docstring: Merge the values for each key using an associative and commutative reduce function, but        return the results immediately to the master as a dictionary.        This will also perform the merging locally on each mapper before        sending results to a reducer, similarly to a "combiner" in MapReduce.        >>> from operator import add        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])        >>> sorted(rdd.reduceByKeyLocally(add).items())        [('a', 2), ('b', 1)]
*******__*******
Code:def partitionBy(self, numPartitions, partitionFunc=portable_hash):\n        """\n        Return a copy of the RDD partitioned using the specified partitioner.\n\n        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n        >>> sets = pairs.partitionBy(2).glom().collect()\n        >>> len(set(sets[0]).intersection(set(sets[1])))\n        0\n        """\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n        partitioner = Partitioner(numPartitions, partitionFunc)\n        if self.partitioner == partitioner:\n            return self\n\n        # Transferring O(n) objects to Java is too expensive.\n        # Instead, we'll form the hash buckets in Python,\n        # transferring O(numPartitions) objects to Java.\n        # Each object is a (splitNumber, [objects]) pair.\n        # In order to avoid too huge objects, the objects are\n        # grouped into chunks.\n        outputSerializer = self.ctx._unbatched_serializer\n\n        limit = (_parse_memory(self.ctx._conf.get(\n            "spark.python.worker.memory", "512m")) / 2)\n\n        def add_shuffle_key(split, iterator):\n\n            buckets = defaultdict(list)\n            c, batch = 0, min(10 * numPartitions, 1000)\n\n            for k, v in iterator:\n                buckets[partitionFunc(k) % numPartitions].append((k, v))\n                c += 1\n\n                # check used memory and avg size of chunk of objects\n                if (c % 1000 == 0 and get_used_memory() > limit\n                        or c > batch):\n                    n, size = len(buckets), 0\n                    for split in list(buckets.keys()):\n                        yield pack_long(split)\n                        d = outputSerializer.dumps(buckets[split])\n                        del buckets[split]\n                        yield d\n                        size += len(d)\n\n                    avg = int(size / n) >> 20\n                    # let 1M < avg < 10M\n                    if avg < 1:\n                        batch *= 1.5\n                    elif avg > 10:\n                        batch = max(int(batch / 1.5), 1)\n                    c = 0\n\n            for split, items in buckets.items():\n                yield pack_long(split)\n                yield outputSerializer.dumps(items)\n\n        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)\n        keyed._bypass_serializer = True\n        with SCCallSiteSync(self.context) as css:\n            pairRDD = self.ctx._jvm.PairwiseRDD(\n                keyed._jrdd.rdd()).asJavaPairRDD()\n            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,\n                                                           id(partitionFunc))\n        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))\n        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))\n        rdd.partitioner = partitioner\n        return rdd
Language: python
Code Tokens: ['def', 'partitionBy', '(', 'self', ',', 'numPartitions', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_defaultReducePartitions', '(', ')', 'partitioner', '=', 'Partitioner', '(', 'numPartitions', ',', 'partitionFunc', ')', 'if', 'self', '.', 'partitioner', '==', 'partitioner', ':', 'return', 'self', '# Transferring O(n) objects to Java is too expensive.', "# Instead, we'll form the hash buckets in Python,", '# transferring O(numPartitions) objects to Java.', '# Each object is a (splitNumber, [objects]) pair.', '# In order to avoid too huge objects, the objects are', '# grouped into chunks.', 'outputSerializer', '=', 'self', '.', 'ctx', '.', '_unbatched_serializer', 'limit', '=', '(', '_parse_memory', '(', 'self', '.', 'ctx', '.', '_conf', '.', 'get', '(', '"spark.python.worker.memory"', ',', '"512m"', ')', ')', '/', '2', ')', 'def', 'add_shuffle_key', '(', 'split', ',', 'iterator', ')', ':', 'buckets', '=', 'defaultdict', '(', 'list', ')', 'c', ',', 'batch', '=', '0', ',', 'min', '(', '10', '*', 'numPartitions', ',', '1000', ')', 'for', 'k', ',', 'v', 'in', 'iterator', ':', 'buckets', '[', 'partitionFunc', '(', 'k', ')', '%', 'numPartitions', ']', '.', 'append', '(', '(', 'k', ',', 'v', ')', ')', 'c', '+=', '1', '# check used memory and avg size of chunk of objects', 'if', '(', 'c', '%', '1000', '==', '0', 'and', 'get_used_memory', '(', ')', '>', 'limit', 'or', 'c', '>', 'batch', ')', ':', 'n', ',', 'size', '=', 'len', '(', 'buckets', ')', ',', '0', 'for', 'split', 'in', 'list', '(', 'buckets', '.', 'keys', '(', ')', ')', ':', 'yield', 'pack_long', '(', 'split', ')', 'd', '=', 'outputSerializer', '.', 'dumps', '(', 'buckets', '[', 'split', ']', ')', 'del', 'buckets', '[', 'split', ']', 'yield', 'd', 'size', '+=', 'len', '(', 'd', ')', 'avg', '=', 'int', '(', 'size', '/', 'n', ')', '>>', '20', '# let 1M < avg < 10M', 'if', 'avg', '<', '1', ':', 'batch', '*=', '1.5', 'elif', 'avg', '>', '10', ':', 'batch', '=', 'max', '(', 'int', '(', 'batch', '/', '1.5', ')', ',', '1', ')', 'c', '=', '0', 'for', 'split', ',', 'items', 'in', 'buckets', '.', 'items', '(', ')', ':', 'yield', 'pack_long', '(', 'split', ')', 'yield', 'outputSerializer', '.', 'dumps', '(', 'items', ')', 'keyed', '=', 'self', '.', 'mapPartitionsWithIndex', '(', 'add_shuffle_key', ',', 'preservesPartitioning', '=', 'True', ')', 'keyed', '.', '_bypass_serializer', '=', 'True', 'with', 'SCCallSiteSync', '(', 'self', '.', 'context', ')', 'as', 'css', ':', 'pairRDD', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PairwiseRDD', '(', 'keyed', '.', '_jrdd', '.', 'rdd', '(', ')', ')', '.', 'asJavaPairRDD', '(', ')', 'jpartitioner', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonPartitioner', '(', 'numPartitions', ',', 'id', '(', 'partitionFunc', ')', ')', 'jrdd', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'valueOfPair', '(', 'pairRDD', '.', 'partitionBy', '(', 'jpartitioner', ')', ')', 'rdd', '=', 'RDD', '(', 'jrdd', ',', 'self', '.', 'ctx', ',', 'BatchedSerializer', '(', 'outputSerializer', ')', ')', 'rdd', '.', 'partitioner', '=', 'partitioner', 'return', 'rdd']
Docstring: Return a copy of the RDD partitioned using the specified partitioner.        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))        >>> sets = pairs.partitionBy(2).glom().collect()        >>> len(set(sets[0]).intersection(set(sets[1])))        0
*******__*******
Code:def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\n                     numPartitions=None, partitionFunc=portable_hash):\n        """\n        Generic function to combine the elements for each key using a custom\n        set of aggregation functions.\n\n        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined\n        type" C.\n\n        Users provide three functions:\n\n            - C{createCombiner}, which turns a V into a C (e.g., creates\n              a one-element list)\n            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n              a list)\n            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges\n              the lists)\n\n        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n        modify and return their first argument instead of creating a new C.\n\n        In addition, users can control the partitioning of the output RDD.\n\n        .. note:: V and C can be different -- for example, one might group an RDD of type\n            (Int, Int) into an RDD of type (Int, List[Int]).\n\n        >>> x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)])\n        >>> def to_list(a):\n        ...     return [a]\n        ...\n        >>> def append(a, b):\n        ...     a.append(b)\n        ...     return a\n        ...\n        >>> def extend(a, b):\n        ...     a.extend(b)\n        ...     return a\n        ...\n        >>> sorted(x.combineByKey(to_list, append, extend).collect())\n        [('a', [1, 2]), ('b', [1])]\n        """\n        if numPartitions is None:\n            numPartitions = self._defaultReducePartitions()\n\n        serializer = self.ctx.serializer\n        memory = self._memory_limit()\n        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)\n\n        def combineLocally(iterator):\n            merger = ExternalMerger(agg, memory * 0.9, serializer)\n            merger.mergeValues(iterator)\n            return merger.items()\n\n        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)\n        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n\n        def _mergeCombiners(iterator):\n            merger = ExternalMerger(agg, memory, serializer)\n            merger.mergeCombiners(iterator)\n            return merger.items()\n\n        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)
Language: python
Code Tokens: ['def', 'combineByKey', '(', 'self', ',', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_defaultReducePartitions', '(', ')', 'serializer', '=', 'self', '.', 'ctx', '.', 'serializer', 'memory', '=', 'self', '.', '_memory_limit', '(', ')', 'agg', '=', 'Aggregator', '(', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ')', 'def', 'combineLocally', '(', 'iterator', ')', ':', 'merger', '=', 'ExternalMerger', '(', 'agg', ',', 'memory', '*', '0.9', ',', 'serializer', ')', 'merger', '.', 'mergeValues', '(', 'iterator', ')', 'return', 'merger', '.', 'items', '(', ')', 'locally_combined', '=', 'self', '.', 'mapPartitions', '(', 'combineLocally', ',', 'preservesPartitioning', '=', 'True', ')', 'shuffled', '=', 'locally_combined', '.', 'partitionBy', '(', 'numPartitions', ',', 'partitionFunc', ')', 'def', '_mergeCombiners', '(', 'iterator', ')', ':', 'merger', '=', 'ExternalMerger', '(', 'agg', ',', 'memory', ',', 'serializer', ')', 'merger', '.', 'mergeCombiners', '(', 'iterator', ')', 'return', 'merger', '.', 'items', '(', ')', 'return', 'shuffled', '.', 'mapPartitions', '(', '_mergeCombiners', ',', 'preservesPartitioning', '=', 'True', ')']
Docstring: Generic function to combine the elements for each key using a custom        set of aggregation functions.        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a "combined        type" C.        Users provide three functions:            - C{createCombiner}, which turns a V into a C (e.g., creates              a one-element list)            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of              a list)            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges              the lists)        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to        modify and return their first argument instead of creating a new C.        In addition, users can control the partitioning of the output RDD.        .. note:: V and C can be different -- for example, one might group an RDD of type            (Int, Int) into an RDD of type (Int, List[Int]).        >>> x = sc.parallelize([("a", 1), ("b", 1), ("a", 2)])        >>> def to_list(a):        ...     return [a]        ...        >>> def append(a, b):        ...     a.append(b)        ...     return a        ...        >>> def extend(a, b):        ...     a.extend(b)        ...     return a        ...        >>> sorted(x.combineByKey(to_list, append, extend).collect())        [('a', [1, 2]), ('b', [1])]
*******__*******
Code:def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,\n                       partitionFunc=portable_hash):\n        """\n        Aggregate the values of each key, using given combine functions and a neutral\n        "zero value". This function can return a different result type, U, than the type\n        of the values in this RDD, V. Thus, we need one operation for merging a V into\n        a U and one operation for merging two U's, The former operation is used for merging\n        values within a partition, and the latter is used for merging values between\n        partitions. To avoid memory allocation, both of these functions are\n        allowed to modify and return their first argument instead of creating a new U.\n        """\n        def createZero():\n            return copy.deepcopy(zeroValue)\n\n        return self.combineByKey(\n            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)
Language: python
Code Tokens: ['def', 'aggregateByKey', '(', 'self', ',', 'zeroValue', ',', 'seqFunc', ',', 'combFunc', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'def', 'createZero', '(', ')', ':', 'return', 'copy', '.', 'deepcopy', '(', 'zeroValue', ')', 'return', 'self', '.', 'combineByKey', '(', 'lambda', 'v', ':', 'seqFunc', '(', 'createZero', '(', ')', ',', 'v', ')', ',', 'seqFunc', ',', 'combFunc', ',', 'numPartitions', ',', 'partitionFunc', ')']
Docstring: Aggregate the values of each key, using given combine functions and a neutral        "zero value". This function can return a different result type, U, than the type        of the values in this RDD, V. Thus, we need one operation for merging a V into        a U and one operation for merging two U's, The former operation is used for merging        values within a partition, and the latter is used for merging values between        partitions. To avoid memory allocation, both of these functions are        allowed to modify and return their first argument instead of creating a new U.
*******__*******
Code:def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):\n        """\n        Merge the values for each key using an associative function "func"\n        and a neutral "zeroValue" which may be added to the result an\n        arbitrary number of times, and must not change the result\n        (e.g., 0 for addition, or 1 for multiplication.).\n\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> from operator import add\n        >>> sorted(rdd.foldByKey(0, add).collect())\n        [('a', 2), ('b', 1)]\n        """\n        def createZero():\n            return copy.deepcopy(zeroValue)\n\n        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,\n                                 partitionFunc)
Language: python
Code Tokens: ['def', 'foldByKey', '(', 'self', ',', 'zeroValue', ',', 'func', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'def', 'createZero', '(', ')', ':', 'return', 'copy', '.', 'deepcopy', '(', 'zeroValue', ')', 'return', 'self', '.', 'combineByKey', '(', 'lambda', 'v', ':', 'func', '(', 'createZero', '(', ')', ',', 'v', ')', ',', 'func', ',', 'func', ',', 'numPartitions', ',', 'partitionFunc', ')']
Docstring: Merge the values for each key using an associative function "func"        and a neutral "zeroValue" which may be added to the result an        arbitrary number of times, and must not change the result        (e.g., 0 for addition, or 1 for multiplication.).        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])        >>> from operator import add        >>> sorted(rdd.foldByKey(0, add).collect())        [('a', 2), ('b', 1)]
*******__*******
Code:def groupByKey(self, numPartitions=None, partitionFunc=portable_hash):\n        """\n        Group the values for each key in the RDD into a single sequence.\n        Hash-partitions the resulting RDD with numPartitions partitions.\n\n        .. note:: If you are grouping in order to perform an aggregation (such as a\n            sum or average) over each key, using reduceByKey or aggregateByKey will\n            provide much better performance.\n\n        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])\n        >>> sorted(rdd.groupByKey().mapValues(len).collect())\n        [('a', 2), ('b', 1)]\n        >>> sorted(rdd.groupByKey().mapValues(list).collect())\n        [('a', [1, 1]), ('b', [1])]\n        """\n        def createCombiner(x):\n            return [x]\n\n        def mergeValue(xs, x):\n            xs.append(x)\n            return xs\n\n        def mergeCombiners(a, b):\n            a.extend(b)\n            return a\n\n        memory = self._memory_limit()\n        serializer = self._jrdd_deserializer\n        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)\n\n        def combine(iterator):\n            merger = ExternalMerger(agg, memory * 0.9, serializer)\n            merger.mergeValues(iterator)\n            return merger.items()\n\n        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)\n        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n\n        def groupByKey(it):\n            merger = ExternalGroupBy(agg, memory, serializer)\n            merger.mergeCombiners(it)\n            return merger.items()\n\n        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)
Language: python
Code Tokens: ['def', 'groupByKey', '(', 'self', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'def', 'createCombiner', '(', 'x', ')', ':', 'return', '[', 'x', ']', 'def', 'mergeValue', '(', 'xs', ',', 'x', ')', ':', 'xs', '.', 'append', '(', 'x', ')', 'return', 'xs', 'def', 'mergeCombiners', '(', 'a', ',', 'b', ')', ':', 'a', '.', 'extend', '(', 'b', ')', 'return', 'a', 'memory', '=', 'self', '.', '_memory_limit', '(', ')', 'serializer', '=', 'self', '.', '_jrdd_deserializer', 'agg', '=', 'Aggregator', '(', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ')', 'def', 'combine', '(', 'iterator', ')', ':', 'merger', '=', 'ExternalMerger', '(', 'agg', ',', 'memory', '*', '0.9', ',', 'serializer', ')', 'merger', '.', 'mergeValues', '(', 'iterator', ')', 'return', 'merger', '.', 'items', '(', ')', 'locally_combined', '=', 'self', '.', 'mapPartitions', '(', 'combine', ',', 'preservesPartitioning', '=', 'True', ')', 'shuffled', '=', 'locally_combined', '.', 'partitionBy', '(', 'numPartitions', ',', 'partitionFunc', ')', 'def', 'groupByKey', '(', 'it', ')', ':', 'merger', '=', 'ExternalGroupBy', '(', 'agg', ',', 'memory', ',', 'serializer', ')', 'merger', '.', 'mergeCombiners', '(', 'it', ')', 'return', 'merger', '.', 'items', '(', ')', 'return', 'shuffled', '.', 'mapPartitions', '(', 'groupByKey', ',', 'True', ')', '.', 'mapValues', '(', 'ResultIterable', ')']
Docstring: Group the values for each key in the RDD into a single sequence.        Hash-partitions the resulting RDD with numPartitions partitions.        .. note:: If you are grouping in order to perform an aggregation (such as a            sum or average) over each key, using reduceByKey or aggregateByKey will            provide much better performance.        >>> rdd = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])        >>> sorted(rdd.groupByKey().mapValues(len).collect())        [('a', 2), ('b', 1)]        >>> sorted(rdd.groupByKey().mapValues(list).collect())        [('a', [1, 1]), ('b', [1])]
*******__*******
Code:def flatMapValues(self, f):\n        """\n        Pass each value in the key-value pair RDD through a flatMap function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([("a", ["x", "y", "z"]), ("b", ["p", "r"])])\n        >>> def f(x): return x\n        >>> x.flatMapValues(f).collect()\n        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n        """\n        flat_map_fn = lambda kv: ((kv[0], x) for x in f(kv[1]))\n        return self.flatMap(flat_map_fn, preservesPartitioning=True)
Language: python
Code Tokens: ['def', 'flatMapValues', '(', 'self', ',', 'f', ')', ':', 'flat_map_fn', '=', 'lambda', 'kv', ':', '(', '(', 'kv', '[', '0', ']', ',', 'x', ')', 'for', 'x', 'in', 'f', '(', 'kv', '[', '1', ']', ')', ')', 'return', 'self', '.', 'flatMap', '(', 'flat_map_fn', ',', 'preservesPartitioning', '=', 'True', ')']
Docstring: Pass each value in the key-value pair RDD through a flatMap function        without changing the keys; this also retains the original RDD's        partitioning.        >>> x = sc.parallelize([("a", ["x", "y", "z"]), ("b", ["p", "r"])])        >>> def f(x): return x        >>> x.flatMapValues(f).collect()        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]
*******__*******
Code:def mapValues(self, f):\n        """\n        Pass each value in the key-value pair RDD through a map function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([("a", ["apple", "banana", "lemon"]), ("b", ["grapes"])])\n        >>> def f(x): return len(x)\n        >>> x.mapValues(f).collect()\n        [('a', 3), ('b', 1)]\n        """\n        map_values_fn = lambda kv: (kv[0], f(kv[1]))\n        return self.map(map_values_fn, preservesPartitioning=True)
Language: python
Code Tokens: ['def', 'mapValues', '(', 'self', ',', 'f', ')', ':', 'map_values_fn', '=', 'lambda', 'kv', ':', '(', 'kv', '[', '0', ']', ',', 'f', '(', 'kv', '[', '1', ']', ')', ')', 'return', 'self', '.', 'map', '(', 'map_values_fn', ',', 'preservesPartitioning', '=', 'True', ')']
Docstring: Pass each value in the key-value pair RDD through a map function        without changing the keys; this also retains the original RDD's        partitioning.        >>> x = sc.parallelize([("a", ["apple", "banana", "lemon"]), ("b", ["grapes"])])        >>> def f(x): return len(x)        >>> x.mapValues(f).collect()        [('a', 3), ('b', 1)]
*******__*******
Code:def sampleByKey(self, withReplacement, fractions, seed=None):\n        """\n        Return a subset of this RDD sampled by key (via stratified sampling).\n        Create a sample of this RDD using variable sampling rates for\n        different keys as specified by fractions, a key to sampling rate map.\n\n        >>> fractions = {"a": 0.2, "b": 0.1}\n        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n        >>> 100 < len(sample["a"]) < 300 and 50 < len(sample["b"]) < 150\n        True\n        >>> max(sample["a"]) <= 999 and min(sample["a"]) >= 0\n        True\n        >>> max(sample["b"]) <= 999 and min(sample["b"]) >= 0\n        True\n        """\n        for fraction in fractions.values():\n            assert fraction >= 0.0, "Negative fraction value: %s" % fraction\n        return self.mapPartitionsWithIndex(\n            RDDStratifiedSampler(withReplacement, fractions, seed).func, True)
Language: python
Code Tokens: ['def', 'sampleByKey', '(', 'self', ',', 'withReplacement', ',', 'fractions', ',', 'seed', '=', 'None', ')', ':', 'for', 'fraction', 'in', 'fractions', '.', 'values', '(', ')', ':', 'assert', 'fraction', '>=', '0.0', ',', '"Negative fraction value: %s"', '%', 'fraction', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'RDDStratifiedSampler', '(', 'withReplacement', ',', 'fractions', ',', 'seed', ')', '.', 'func', ',', 'True', ')']
Docstring: Return a subset of this RDD sampled by key (via stratified sampling).        Create a sample of this RDD using variable sampling rates for        different keys as specified by fractions, a key to sampling rate map.        >>> fractions = {"a": 0.2, "b": 0.1}        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())        >>> 100 < len(sample["a"]) < 300 and 50 < len(sample["b"]) < 150        True        >>> max(sample["a"]) <= 999 and min(sample["a"]) >= 0        True        >>> max(sample["b"]) <= 999 and min(sample["b"]) >= 0        True
*******__*******
Code:def subtractByKey(self, other, numPartitions=None):\n        """\n        Return each (key, value) pair in C{self} that has no pair with matching\n        key in C{other}.\n\n        >>> x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 2)])\n        >>> y = sc.parallelize([("a", 3), ("c", None)])\n        >>> sorted(x.subtractByKey(y).collect())\n        [('b', 4), ('b', 5)]\n        """\n        def filter_func(pair):\n            key, (val1, val2) = pair\n            return val1 and not val2\n        return self.cogroup(other, numPartitions).filter(filter_func).flatMapValues(lambda x: x[0])
Language: python
Code Tokens: ['def', 'subtractByKey', '(', 'self', ',', 'other', ',', 'numPartitions', '=', 'None', ')', ':', 'def', 'filter_func', '(', 'pair', ')', ':', 'key', ',', '(', 'val1', ',', 'val2', ')', '=', 'pair', 'return', 'val1', 'and', 'not', 'val2', 'return', 'self', '.', 'cogroup', '(', 'other', ',', 'numPartitions', ')', '.', 'filter', '(', 'filter_func', ')', '.', 'flatMapValues', '(', 'lambda', 'x', ':', 'x', '[', '0', ']', ')']
Docstring: Return each (key, value) pair in C{self} that has no pair with matching        key in C{other}.        >>> x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 2)])        >>> y = sc.parallelize([("a", 3), ("c", None)])        >>> sorted(x.subtractByKey(y).collect())        [('b', 4), ('b', 5)]
*******__*******
Code:def subtract(self, other, numPartitions=None):\n        """\n        Return each value in C{self} that is not contained in C{other}.\n\n        >>> x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 3)])\n        >>> y = sc.parallelize([("a", 3), ("c", None)])\n        >>> sorted(x.subtract(y).collect())\n        [('a', 1), ('b', 4), ('b', 5)]\n        """\n        # note: here 'True' is just a placeholder\n        rdd = other.map(lambda x: (x, True))\n        return self.map(lambda x: (x, True)).subtractByKey(rdd, numPartitions).keys()
Language: python
Code Tokens: ['def', 'subtract', '(', 'self', ',', 'other', ',', 'numPartitions', '=', 'None', ')', ':', "# note: here 'True' is just a placeholder", 'rdd', '=', 'other', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'True', ')', ')', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'True', ')', ')', '.', 'subtractByKey', '(', 'rdd', ',', 'numPartitions', ')', '.', 'keys', '(', ')']
Docstring: Return each value in C{self} that is not contained in C{other}.        >>> x = sc.parallelize([("a", 1), ("b", 4), ("b", 5), ("a", 3)])        >>> y = sc.parallelize([("a", 3), ("c", None)])        >>> sorted(x.subtract(y).collect())        [('a', 1), ('b', 4), ('b', 5)]
*******__*******
Code:def coalesce(self, numPartitions, shuffle=False):\n        """\n        Return a new RDD that is reduced into `numPartitions` partitions.\n\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n        [[1], [2, 3], [4, 5]]\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n        [[1, 2, 3, 4, 5]]\n        """\n        if shuffle:\n            # Decrease the batch size in order to distribute evenly the elements across output\n            # partitions. Otherwise, repartition will possibly produce highly skewed partitions.\n            batchSize = min(10, self.ctx._batchSize or 1024)\n            ser = BatchedSerializer(PickleSerializer(), batchSize)\n            selfCopy = self._reserialize(ser)\n            jrdd_deserializer = selfCopy._jrdd_deserializer\n            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)\n        else:\n            jrdd_deserializer = self._jrdd_deserializer\n            jrdd = self._jrdd.coalesce(numPartitions, shuffle)\n        return RDD(jrdd, self.ctx, jrdd_deserializer)
Language: python
Code Tokens: ['def', 'coalesce', '(', 'self', ',', 'numPartitions', ',', 'shuffle', '=', 'False', ')', ':', 'if', 'shuffle', ':', '# Decrease the batch size in order to distribute evenly the elements across output', '# partitions. Otherwise, repartition will possibly produce highly skewed partitions.', 'batchSize', '=', 'min', '(', '10', ',', 'self', '.', 'ctx', '.', '_batchSize', 'or', '1024', ')', 'ser', '=', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ',', 'batchSize', ')', 'selfCopy', '=', 'self', '.', '_reserialize', '(', 'ser', ')', 'jrdd_deserializer', '=', 'selfCopy', '.', '_jrdd_deserializer', 'jrdd', '=', 'selfCopy', '.', '_jrdd', '.', 'coalesce', '(', 'numPartitions', ',', 'shuffle', ')', 'else', ':', 'jrdd_deserializer', '=', 'self', '.', '_jrdd_deserializer', 'jrdd', '=', 'self', '.', '_jrdd', '.', 'coalesce', '(', 'numPartitions', ',', 'shuffle', ')', 'return', 'RDD', '(', 'jrdd', ',', 'self', '.', 'ctx', ',', 'jrdd_deserializer', ')']
Docstring: Return a new RDD that is reduced into `numPartitions` partitions.        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()        [[1], [2, 3], [4, 5]]        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()        [[1, 2, 3, 4, 5]]
*******__*******
Code:def zip(self, other):\n        """\n        Zips this RDD with another one, returning key-value pairs with the\n        first element in each RDD second element in each RDD, etc. Assumes\n        that the two RDDs have the same number of partitions and the same\n        number of elements in each partition (e.g. one was made through\n        a map on the other).\n\n        >>> x = sc.parallelize(range(0,5))\n        >>> y = sc.parallelize(range(1000, 1005))\n        >>> x.zip(y).collect()\n        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n        """\n        def get_batch_size(ser):\n            if isinstance(ser, BatchedSerializer):\n                return ser.batchSize\n            return 1  # not batched\n\n        def batch_as(rdd, batchSize):\n            return rdd._reserialize(BatchedSerializer(PickleSerializer(), batchSize))\n\n        my_batch = get_batch_size(self._jrdd_deserializer)\n        other_batch = get_batch_size(other._jrdd_deserializer)\n        if my_batch != other_batch or not my_batch:\n            # use the smallest batchSize for both of them\n            batchSize = min(my_batch, other_batch)\n            if batchSize <= 0:\n                # auto batched or unlimited\n                batchSize = 100\n            other = batch_as(other, batchSize)\n            self = batch_as(self, batchSize)\n\n        if self.getNumPartitions() != other.getNumPartitions():\n            raise ValueError("Can only zip with RDD which has the same number of partitions")\n\n        # There will be an Exception in JVM if there are different number\n        # of items in each partitions.\n        pairRDD = self._jrdd.zip(other._jrdd)\n        deserializer = PairDeserializer(self._jrdd_deserializer,\n                                        other._jrdd_deserializer)\n        return RDD(pairRDD, self.ctx, deserializer)
Language: python
Code Tokens: ['def', 'zip', '(', 'self', ',', 'other', ')', ':', 'def', 'get_batch_size', '(', 'ser', ')', ':', 'if', 'isinstance', '(', 'ser', ',', 'BatchedSerializer', ')', ':', 'return', 'ser', '.', 'batchSize', 'return', '1', '# not batched', 'def', 'batch_as', '(', 'rdd', ',', 'batchSize', ')', ':', 'return', 'rdd', '.', '_reserialize', '(', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ',', 'batchSize', ')', ')', 'my_batch', '=', 'get_batch_size', '(', 'self', '.', '_jrdd_deserializer', ')', 'other_batch', '=', 'get_batch_size', '(', 'other', '.', '_jrdd_deserializer', ')', 'if', 'my_batch', '!=', 'other_batch', 'or', 'not', 'my_batch', ':', '# use the smallest batchSize for both of them', 'batchSize', '=', 'min', '(', 'my_batch', ',', 'other_batch', ')', 'if', 'batchSize', '<=', '0', ':', '# auto batched or unlimited', 'batchSize', '=', '100', 'other', '=', 'batch_as', '(', 'other', ',', 'batchSize', ')', 'self', '=', 'batch_as', '(', 'self', ',', 'batchSize', ')', 'if', 'self', '.', 'getNumPartitions', '(', ')', '!=', 'other', '.', 'getNumPartitions', '(', ')', ':', 'raise', 'ValueError', '(', '"Can only zip with RDD which has the same number of partitions"', ')', '# There will be an Exception in JVM if there are different number', '# of items in each partitions.', 'pairRDD', '=', 'self', '.', '_jrdd', '.', 'zip', '(', 'other', '.', '_jrdd', ')', 'deserializer', '=', 'PairDeserializer', '(', 'self', '.', '_jrdd_deserializer', ',', 'other', '.', '_jrdd_deserializer', ')', 'return', 'RDD', '(', 'pairRDD', ',', 'self', '.', 'ctx', ',', 'deserializer', ')']
Docstring: Zips this RDD with another one, returning key-value pairs with the        first element in each RDD second element in each RDD, etc. Assumes        that the two RDDs have the same number of partitions and the same        number of elements in each partition (e.g. one was made through        a map on the other).        >>> x = sc.parallelize(range(0,5))        >>> y = sc.parallelize(range(1000, 1005))        >>> x.zip(y).collect()        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]
*******__*******
Code:def zipWithIndex(self):\n        """\n        Zips this RDD with its element indices.\n\n        The ordering is first based on the partition index and then the\n        ordering of items within each partition. So the first item in\n        the first partition gets index 0, and the last item in the last\n        partition receives the largest index.\n\n        This method needs to trigger a spark job when this RDD contains\n        more than one partitions.\n\n        >>> sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()\n        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n        """\n        starts = [0]\n        if self.getNumPartitions() > 1:\n            nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n            for i in range(len(nums) - 1):\n                starts.append(starts[-1] + nums[i])\n\n        def func(k, it):\n            for i, v in enumerate(it, starts[k]):\n                yield v, i\n\n        return self.mapPartitionsWithIndex(func)
Language: python
Code Tokens: ['def', 'zipWithIndex', '(', 'self', ')', ':', 'starts', '=', '[', '0', ']', 'if', 'self', '.', 'getNumPartitions', '(', ')', '>', '1', ':', 'nums', '=', 'self', '.', 'mapPartitions', '(', 'lambda', 'it', ':', '[', 'sum', '(', '1', 'for', 'i', 'in', 'it', ')', ']', ')', '.', 'collect', '(', ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'nums', ')', '-', '1', ')', ':', 'starts', '.', 'append', '(', 'starts', '[', '-', '1', ']', '+', 'nums', '[', 'i', ']', ')', 'def', 'func', '(', 'k', ',', 'it', ')', ':', 'for', 'i', ',', 'v', 'in', 'enumerate', '(', 'it', ',', 'starts', '[', 'k', ']', ')', ':', 'yield', 'v', ',', 'i', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ')']
Docstring: Zips this RDD with its element indices.        The ordering is first based on the partition index and then the        ordering of items within each partition. So the first item in        the first partition gets index 0, and the last item in the last        partition receives the largest index.        This method needs to trigger a spark job when this RDD contains        more than one partitions.        >>> sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]
*******__*******
Code:def zipWithUniqueId(self):\n        """\n        Zips this RDD with generated unique Long ids.\n\n        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n        n is the number of partitions. So there may exist gaps, but this\n        method won't trigger a spark job, which is different from\n        L{zipWithIndex}\n\n        >>> sc.parallelize(["a", "b", "c", "d", "e"], 3).zipWithUniqueId().collect()\n        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n        """\n        n = self.getNumPartitions()\n\n        def func(k, it):\n            for i, v in enumerate(it):\n                yield v, i * n + k\n\n        return self.mapPartitionsWithIndex(func)
Language: python
Code Tokens: ['def', 'zipWithUniqueId', '(', 'self', ')', ':', 'n', '=', 'self', '.', 'getNumPartitions', '(', ')', 'def', 'func', '(', 'k', ',', 'it', ')', ':', 'for', 'i', ',', 'v', 'in', 'enumerate', '(', 'it', ')', ':', 'yield', 'v', ',', 'i', '*', 'n', '+', 'k', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ')']
Docstring: Zips this RDD with generated unique Long ids.        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where        n is the number of partitions. So there may exist gaps, but this        method won't trigger a spark job, which is different from        L{zipWithIndex}        >>> sc.parallelize(["a", "b", "c", "d", "e"], 3).zipWithUniqueId().collect()        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]
*******__*******
Code:def getStorageLevel(self):\n        """\n        Get the RDD's current storage level.\n\n        >>> rdd1 = sc.parallelize([1,2])\n        >>> rdd1.getStorageLevel()\n        StorageLevel(False, False, False, False, 1)\n        >>> print(rdd1.getStorageLevel())\n        Serialized 1x Replicated\n        """\n        java_storage_level = self._jrdd.getStorageLevel()\n        storage_level = StorageLevel(java_storage_level.useDisk(),\n                                     java_storage_level.useMemory(),\n                                     java_storage_level.useOffHeap(),\n                                     java_storage_level.deserialized(),\n                                     java_storage_level.replication())\n        return storage_level
Language: python
Code Tokens: ['def', 'getStorageLevel', '(', 'self', ')', ':', 'java_storage_level', '=', 'self', '.', '_jrdd', '.', 'getStorageLevel', '(', ')', 'storage_level', '=', 'StorageLevel', '(', 'java_storage_level', '.', 'useDisk', '(', ')', ',', 'java_storage_level', '.', 'useMemory', '(', ')', ',', 'java_storage_level', '.', 'useOffHeap', '(', ')', ',', 'java_storage_level', '.', 'deserialized', '(', ')', ',', 'java_storage_level', '.', 'replication', '(', ')', ')', 'return', 'storage_level']
Docstring: Get the RDD's current storage level.        >>> rdd1 = sc.parallelize([1,2])        >>> rdd1.getStorageLevel()        StorageLevel(False, False, False, False, 1)        >>> print(rdd1.getStorageLevel())        Serialized 1x Replicated
*******__*******
Code:def _defaultReducePartitions(self):\n        """\n        Returns the default number of partitions to use during reduce tasks (e.g., groupBy).\n        If spark.default.parallelism is set, then we'll use the value from SparkContext\n        defaultParallelism, otherwise we'll use the number of partitions in this RDD.\n\n        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce\n        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will\n        be inherent.\n        """\n        if self.ctx._conf.contains("spark.default.parallelism"):\n            return self.ctx.defaultParallelism\n        else:\n            return self.getNumPartitions()
Language: python
Code Tokens: ['def', '_defaultReducePartitions', '(', 'self', ')', ':', 'if', 'self', '.', 'ctx', '.', '_conf', '.', 'contains', '(', '"spark.default.parallelism"', ')', ':', 'return', 'self', '.', 'ctx', '.', 'defaultParallelism', 'else', ':', 'return', 'self', '.', 'getNumPartitions', '(', ')']
Docstring: Returns the default number of partitions to use during reduce tasks (e.g., groupBy).        If spark.default.parallelism is set, then we'll use the value from SparkContext        defaultParallelism, otherwise we'll use the number of partitions in this RDD.        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will        be inherent.
*******__*******
Code:def lookup(self, key):\n        """\n        Return the list of values in the RDD for key `key`. This operation\n        is done efficiently if the RDD has a known partitioner by only\n        searching the partition that the key maps to.\n\n        >>> l = range(1000)\n        >>> rdd = sc.parallelize(zip(l, l), 10)\n        >>> rdd.lookup(42)  # slow\n        [42]\n        >>> sorted = rdd.sortByKey()\n        >>> sorted.lookup(42)  # fast\n        [42]\n        >>> sorted.lookup(1024)\n        []\n        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n        >>> list(rdd2.lookup(('a', 'b'))[0])\n        ['c']\n        """\n        values = self.filter(lambda kv: kv[0] == key).values()\n\n        if self.partitioner is not None:\n            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])\n\n        return values.collect()
Language: python
Code Tokens: ['def', 'lookup', '(', 'self', ',', 'key', ')', ':', 'values', '=', 'self', '.', 'filter', '(', 'lambda', 'kv', ':', 'kv', '[', '0', ']', '==', 'key', ')', '.', 'values', '(', ')', 'if', 'self', '.', 'partitioner', 'is', 'not', 'None', ':', 'return', 'self', '.', 'ctx', '.', 'runJob', '(', 'values', ',', 'lambda', 'x', ':', 'x', ',', '[', 'self', '.', 'partitioner', '(', 'key', ')', ']', ')', 'return', 'values', '.', 'collect', '(', ')']
Docstring: Return the list of values in the RDD for key `key`. This operation        is done efficiently if the RDD has a known partitioner by only        searching the partition that the key maps to.        >>> l = range(1000)        >>> rdd = sc.parallelize(zip(l, l), 10)        >>> rdd.lookup(42)  # slow        [42]        >>> sorted = rdd.sortByKey()        >>> sorted.lookup(42)  # fast        [42]        >>> sorted.lookup(1024)        []        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()        >>> list(rdd2.lookup(('a', 'b'))[0])        ['c']
*******__*******
Code:def _to_java_object_rdd(self):\n        """ Return a JavaRDD of Object by unpickling\n\n        It will convert each Python object into Java object by Pyrolite, whenever the\n        RDD is serialized in batch or not.\n        """\n        rdd = self._pickled()\n        return self.ctx._jvm.SerDeUtil.pythonToJava(rdd._jrdd, True)
Language: python
Code Tokens: ['def', '_to_java_object_rdd', '(', 'self', ')', ':', 'rdd', '=', 'self', '.', '_pickled', '(', ')', 'return', 'self', '.', 'ctx', '.', '_jvm', '.', 'SerDeUtil', '.', 'pythonToJava', '(', 'rdd', '.', '_jrdd', ',', 'True', ')']
Docstring: Return a JavaRDD of Object by unpickling        It will convert each Python object into Java object by Pyrolite, whenever the        RDD is serialized in batch or not.
*******__*******
Code:def countApprox(self, timeout, confidence=0.95):\n        """\n        .. note:: Experimental\n\n        Approximate version of count() that returns a potentially incomplete\n        result within a timeout, even if not all tasks have finished.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> rdd.countApprox(1000, 1.0)\n        1000\n        """\n        drdd = self.mapPartitions(lambda it: [float(sum(1 for i in it))])\n        return int(drdd.sumApprox(timeout, confidence))
Language: python
Code Tokens: ['def', 'countApprox', '(', 'self', ',', 'timeout', ',', 'confidence', '=', '0.95', ')', ':', 'drdd', '=', 'self', '.', 'mapPartitions', '(', 'lambda', 'it', ':', '[', 'float', '(', 'sum', '(', '1', 'for', 'i', 'in', 'it', ')', ')', ']', ')', 'return', 'int', '(', 'drdd', '.', 'sumApprox', '(', 'timeout', ',', 'confidence', ')', ')']
Docstring: .. note:: Experimental        Approximate version of count() that returns a potentially incomplete        result within a timeout, even if not all tasks have finished.        >>> rdd = sc.parallelize(range(1000), 10)        >>> rdd.countApprox(1000, 1.0)        1000
*******__*******
Code:def sumApprox(self, timeout, confidence=0.95):\n        """\n        .. note:: Experimental\n\n        Approximate operation to return the sum within a timeout\n        or meet the confidence.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000))\n        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n        True\n        """\n        jrdd = self.mapPartitions(lambda it: [float(sum(it))])._to_java_object_rdd()\n        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())\n        r = jdrdd.sumApprox(timeout, confidence).getFinalValue()\n        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())
Language: python
Code Tokens: ['def', 'sumApprox', '(', 'self', ',', 'timeout', ',', 'confidence', '=', '0.95', ')', ':', 'jrdd', '=', 'self', '.', 'mapPartitions', '(', 'lambda', 'it', ':', '[', 'float', '(', 'sum', '(', 'it', ')', ')', ']', ')', '.', '_to_java_object_rdd', '(', ')', 'jdrdd', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'JavaDoubleRDD', '.', 'fromRDD', '(', 'jrdd', '.', 'rdd', '(', ')', ')', 'r', '=', 'jdrdd', '.', 'sumApprox', '(', 'timeout', ',', 'confidence', ')', '.', 'getFinalValue', '(', ')', 'return', 'BoundedFloat', '(', 'r', '.', 'mean', '(', ')', ',', 'r', '.', 'confidence', '(', ')', ',', 'r', '.', 'low', '(', ')', ',', 'r', '.', 'high', '(', ')', ')']
Docstring: .. note:: Experimental        Approximate operation to return the sum within a timeout        or meet the confidence.        >>> rdd = sc.parallelize(range(1000), 10)        >>> r = sum(range(1000))        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05        True
*******__*******
Code:def meanApprox(self, timeout, confidence=0.95):\n        """\n        .. note:: Experimental\n\n        Approximate operation to return the mean within a timeout\n        or meet the confidence.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000)) / 1000.0\n        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n        True\n        """\n        jrdd = self.map(float)._to_java_object_rdd()\n        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())\n        r = jdrdd.meanApprox(timeout, confidence).getFinalValue()\n        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())
Language: python
Code Tokens: ['def', 'meanApprox', '(', 'self', ',', 'timeout', ',', 'confidence', '=', '0.95', ')', ':', 'jrdd', '=', 'self', '.', 'map', '(', 'float', ')', '.', '_to_java_object_rdd', '(', ')', 'jdrdd', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'JavaDoubleRDD', '.', 'fromRDD', '(', 'jrdd', '.', 'rdd', '(', ')', ')', 'r', '=', 'jdrdd', '.', 'meanApprox', '(', 'timeout', ',', 'confidence', ')', '.', 'getFinalValue', '(', ')', 'return', 'BoundedFloat', '(', 'r', '.', 'mean', '(', ')', ',', 'r', '.', 'confidence', '(', ')', ',', 'r', '.', 'low', '(', ')', ',', 'r', '.', 'high', '(', ')', ')']
Docstring: .. note:: Experimental        Approximate operation to return the mean within a timeout        or meet the confidence.        >>> rdd = sc.parallelize(range(1000), 10)        >>> r = sum(range(1000)) / 1000.0        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05        True
*******__*******
Code:def countApproxDistinct(self, relativeSD=0.05):\n        """\n        .. note:: Experimental\n\n        Return approximate number of distinct elements in the RDD.\n\n        The algorithm used is based on streamlib's implementation of\n        `"HyperLogLog in Practice: Algorithmic Engineering of a State\n        of The Art Cardinality Estimation Algorithm", available here\n        <https://doi.org/10.1145/2452376.2452456>`_.\n\n        :param relativeSD: Relative accuracy. Smaller values create\n                           counters that require more space.\n                           It must be greater than 0.000017.\n\n        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n        >>> 900 < n < 1100\n        True\n        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n        >>> 16 < n < 24\n        True\n        """\n        if relativeSD < 0.000017:\n            raise ValueError("relativeSD should be greater than 0.000017")\n        # the hash space in Java is 2^32\n        hashRDD = self.map(lambda x: portable_hash(x) & 0xFFFFFFFF)\n        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)
Language: python
Code Tokens: ['def', 'countApproxDistinct', '(', 'self', ',', 'relativeSD', '=', '0.05', ')', ':', 'if', 'relativeSD', '<', '0.000017', ':', 'raise', 'ValueError', '(', '"relativeSD should be greater than 0.000017"', ')', '# the hash space in Java is 2^32', 'hashRDD', '=', 'self', '.', 'map', '(', 'lambda', 'x', ':', 'portable_hash', '(', 'x', ')', '&', '0xFFFFFFFF', ')', 'return', 'hashRDD', '.', '_to_java_object_rdd', '(', ')', '.', 'countApproxDistinct', '(', 'relativeSD', ')']
Docstring: .. note:: Experimental        Return approximate number of distinct elements in the RDD.        The algorithm used is based on streamlib's implementation of        `"HyperLogLog in Practice: Algorithmic Engineering of a State        of The Art Cardinality Estimation Algorithm", available here        <https://doi.org/10.1145/2452376.2452456>`_.        :param relativeSD: Relative accuracy. Smaller values create                           counters that require more space.                           It must be greater than 0.000017.        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()        >>> 900 < n < 1100        True        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()        >>> 16 < n < 24        True
*******__*******
Code:def toLocalIterator(self):\n        """\n        Return an iterator that contains all of the elements in this RDD.\n        The iterator will consume as much memory as the largest partition in this RDD.\n\n        >>> rdd = sc.parallelize(range(10))\n        >>> [x for x in rdd.toLocalIterator()]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        """\n        with SCCallSiteSync(self.context) as css:\n            sock_info = self.ctx._jvm.PythonRDD.toLocalIteratorAndServe(self._jrdd.rdd())\n        return _load_from_socket(sock_info, self._jrdd_deserializer)
Language: python
Code Tokens: ['def', 'toLocalIterator', '(', 'self', ')', ':', 'with', 'SCCallSiteSync', '(', 'self', '.', 'context', ')', 'as', 'css', ':', 'sock_info', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'toLocalIteratorAndServe', '(', 'self', '.', '_jrdd', '.', 'rdd', '(', ')', ')', 'return', '_load_from_socket', '(', 'sock_info', ',', 'self', '.', '_jrdd_deserializer', ')']
Docstring: Return an iterator that contains all of the elements in this RDD.        The iterator will consume as much memory as the largest partition in this RDD.        >>> rdd = sc.parallelize(range(10))        >>> [x for x in rdd.toLocalIterator()]        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
*******__*******
Code:def mapPartitions(self, f, preservesPartitioning=False):\n        """\n        .. note:: Experimental\n\n        Returns a new RDD by applying a function to each partition of the wrapped RDD,\n        where tasks are launched together in a barrier stage.\n        The interface is the same as :func:`RDD.mapPartitions`.\n        Please see the API doc there.\n\n        .. versionadded:: 2.4.0\n        """\n        def func(s, iterator):\n            return f(iterator)\n        return PipelinedRDD(self.rdd, func, preservesPartitioning, isFromBarrier=True)
Language: python
Code Tokens: ['def', 'mapPartitions', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', 's', ',', 'iterator', ')', ':', 'return', 'f', '(', 'iterator', ')', 'return', 'PipelinedRDD', '(', 'self', '.', 'rdd', ',', 'func', ',', 'preservesPartitioning', ',', 'isFromBarrier', '=', 'True', ')']
Docstring: .. note:: Experimental        Returns a new RDD by applying a function to each partition of the wrapped RDD,        where tasks are launched together in a barrier stage.        The interface is the same as :func:`RDD.mapPartitions`.        Please see the API doc there.        .. versionadded:: 2.4.0
*******__*******
Code:def _to_seq(sc, cols, converter=None):\n    """\n    Convert a list of Column (or names) into a JVM Seq of Column.\n\n    An optional `converter` could be used to convert items in `cols`\n    into JVM Column objects.\n    """\n    if converter:\n        cols = [converter(c) for c in cols]\n    return sc._jvm.PythonUtils.toSeq(cols)
Language: python
Code Tokens: ['def', '_to_seq', '(', 'sc', ',', 'cols', ',', 'converter', '=', 'None', ')', ':', 'if', 'converter', ':', 'cols', '=', '[', 'converter', '(', 'c', ')', 'for', 'c', 'in', 'cols', ']', 'return', 'sc', '.', '_jvm', '.', 'PythonUtils', '.', 'toSeq', '(', 'cols', ')']
Docstring: Convert a list of Column (or names) into a JVM Seq of Column.    An optional `converter` could be used to convert items in `cols`    into JVM Column objects.
*******__*******
Code:def _to_list(sc, cols, converter=None):\n    """\n    Convert a list of Column (or names) into a JVM (Scala) List of Column.\n\n    An optional `converter` could be used to convert items in `cols`\n    into JVM Column objects.\n    """\n    if converter:\n        cols = [converter(c) for c in cols]\n    return sc._jvm.PythonUtils.toList(cols)
Language: python
Code Tokens: ['def', '_to_list', '(', 'sc', ',', 'cols', ',', 'converter', '=', 'None', ')', ':', 'if', 'converter', ':', 'cols', '=', '[', 'converter', '(', 'c', ')', 'for', 'c', 'in', 'cols', ']', 'return', 'sc', '.', '_jvm', '.', 'PythonUtils', '.', 'toList', '(', 'cols', ')']
Docstring: Convert a list of Column (or names) into a JVM (Scala) List of Column.    An optional `converter` could be used to convert items in `cols`    into JVM Column objects.
*******__*******
Code:def _unary_op(name, doc="unary operator"):\n    """ Create a method for given unary operator """\n    def _(self):\n        jc = getattr(self._jc, name)()\n        return Column(jc)\n    _.__doc__ = doc\n    return _
Language: python
Code Tokens: ['def', '_unary_op', '(', 'name', ',', 'doc', '=', '"unary operator"', ')', ':', 'def', '_', '(', 'self', ')', ':', 'jc', '=', 'getattr', '(', 'self', '.', '_jc', ',', 'name', ')', '(', ')', 'return', 'Column', '(', 'jc', ')', '_', '.', '__doc__', '=', 'doc', 'return', '_']
Docstring: Create a method for given unary operator
*******__*******
Code:def _bin_op(name, doc="binary operator"):\n    """ Create a method for given binary operator\n    """\n    def _(self, other):\n        jc = other._jc if isinstance(other, Column) else other\n        njc = getattr(self._jc, name)(jc)\n        return Column(njc)\n    _.__doc__ = doc\n    return _
Language: python
Code Tokens: ['def', '_bin_op', '(', 'name', ',', 'doc', '=', '"binary operator"', ')', ':', 'def', '_', '(', 'self', ',', 'other', ')', ':', 'jc', '=', 'other', '.', '_jc', 'if', 'isinstance', '(', 'other', ',', 'Column', ')', 'else', 'other', 'njc', '=', 'getattr', '(', 'self', '.', '_jc', ',', 'name', ')', '(', 'jc', ')', 'return', 'Column', '(', 'njc', ')', '_', '.', '__doc__', '=', 'doc', 'return', '_']
Docstring: Create a method for given binary operator
*******__*******
Code:def _reverse_op(name, doc="binary operator"):\n    """ Create a method for binary operator (this object is on right side)\n    """\n    def _(self, other):\n        jother = _create_column_from_literal(other)\n        jc = getattr(jother, name)(self._jc)\n        return Column(jc)\n    _.__doc__ = doc\n    return _
Language: python
Code Tokens: ['def', '_reverse_op', '(', 'name', ',', 'doc', '=', '"binary operator"', ')', ':', 'def', '_', '(', 'self', ',', 'other', ')', ':', 'jother', '=', '_create_column_from_literal', '(', 'other', ')', 'jc', '=', 'getattr', '(', 'jother', ',', 'name', ')', '(', 'self', '.', '_jc', ')', 'return', 'Column', '(', 'jc', ')', '_', '.', '__doc__', '=', 'doc', 'return', '_']
Docstring: Create a method for binary operator (this object is on right side)
*******__*******
Code:def substr(self, startPos, length):\n        """\n        Return a :class:`Column` which is a substring of the column.\n\n        :param startPos: start position (int or Column)\n        :param length:  length of the substring (int or Column)\n\n        >>> df.select(df.name.substr(1, 3).alias("col")).collect()\n        [Row(col=u'Ali'), Row(col=u'Bob')]\n        """\n        if type(startPos) != type(length):\n            raise TypeError(\n                "startPos and length must be the same type. "\n                "Got {startPos_t} and {length_t}, respectively."\n                .format(\n                    startPos_t=type(startPos),\n                    length_t=type(length),\n                ))\n        if isinstance(startPos, int):\n            jc = self._jc.substr(startPos, length)\n        elif isinstance(startPos, Column):\n            jc = self._jc.substr(startPos._jc, length._jc)\n        else:\n            raise TypeError("Unexpected type: %s" % type(startPos))\n        return Column(jc)
Language: python
Code Tokens: ['def', 'substr', '(', 'self', ',', 'startPos', ',', 'length', ')', ':', 'if', 'type', '(', 'startPos', ')', '!=', 'type', '(', 'length', ')', ':', 'raise', 'TypeError', '(', '"startPos and length must be the same type. "', '"Got {startPos_t} and {length_t}, respectively."', '.', 'format', '(', 'startPos_t', '=', 'type', '(', 'startPos', ')', ',', 'length_t', '=', 'type', '(', 'length', ')', ',', ')', ')', 'if', 'isinstance', '(', 'startPos', ',', 'int', ')', ':', 'jc', '=', 'self', '.', '_jc', '.', 'substr', '(', 'startPos', ',', 'length', ')', 'elif', 'isinstance', '(', 'startPos', ',', 'Column', ')', ':', 'jc', '=', 'self', '.', '_jc', '.', 'substr', '(', 'startPos', '.', '_jc', ',', 'length', '.', '_jc', ')', 'else', ':', 'raise', 'TypeError', '(', '"Unexpected type: %s"', '%', 'type', '(', 'startPos', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Return a :class:`Column` which is a substring of the column.        :param startPos: start position (int or Column)        :param length:  length of the substring (int or Column)        >>> df.select(df.name.substr(1, 3).alias("col")).collect()        [Row(col=u'Ali'), Row(col=u'Bob')]
*******__*******
Code:def isin(self, *cols):\n        """\n        A boolean expression that is evaluated to true if the value of this\n        expression is contained by the evaluated values of the arguments.\n\n        >>> df[df.name.isin("Bob", "Mike")].collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df[df.age.isin([1, 2, 3])].collect()\n        [Row(age=2, name=u'Alice')]\n        """\n        if len(cols) == 1 and isinstance(cols[0], (list, set)):\n            cols = cols[0]\n        cols = [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols]\n        sc = SparkContext._active_spark_context\n        jc = getattr(self._jc, "isin")(_to_seq(sc, cols))\n        return Column(jc)
Language: python
Code Tokens: ['def', 'isin', '(', 'self', ',', '*', 'cols', ')', ':', 'if', 'len', '(', 'cols', ')', '==', '1', 'and', 'isinstance', '(', 'cols', '[', '0', ']', ',', '(', 'list', ',', 'set', ')', ')', ':', 'cols', '=', 'cols', '[', '0', ']', 'cols', '=', '[', 'c', '.', '_jc', 'if', 'isinstance', '(', 'c', ',', 'Column', ')', 'else', '_create_column_from_literal', '(', 'c', ')', 'for', 'c', 'in', 'cols', ']', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'getattr', '(', 'self', '.', '_jc', ',', '"isin"', ')', '(', '_to_seq', '(', 'sc', ',', 'cols', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: A boolean expression that is evaluated to true if the value of this        expression is contained by the evaluated values of the arguments.        >>> df[df.name.isin("Bob", "Mike")].collect()        [Row(age=5, name=u'Bob')]        >>> df[df.age.isin([1, 2, 3])].collect()        [Row(age=2, name=u'Alice')]
*******__*******
Code:def alias(self, *alias, **kwargs):\n        """\n        Returns this column aliased with a new name or names (in the case of expressions that\n        return more than one column, such as explode).\n\n        :param alias: strings of desired column names (collects all positional arguments passed)\n        :param metadata: a dict of information to be stored in ``metadata`` attribute of the\n            corresponding :class: `StructField` (optional, keyword only argument)\n\n        .. versionchanged:: 2.2\n           Added optional ``metadata`` argument.\n\n        >>> df.select(df.age.alias("age2")).collect()\n        [Row(age2=2), Row(age2=5)]\n        >>> df.select(df.age.alias("age3", metadata={'max': 99})).schema['age3'].metadata['max']\n        99\n        """\n\n        metadata = kwargs.pop('metadata', None)\n        assert not kwargs, 'Unexpected kwargs where passed: %s' % kwargs\n\n        sc = SparkContext._active_spark_context\n        if len(alias) == 1:\n            if metadata:\n                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(\n                    json.dumps(metadata))\n                return Column(getattr(self._jc, "as")(alias[0], jmeta))\n            else:\n                return Column(getattr(self._jc, "as")(alias[0]))\n        else:\n            if metadata:\n                raise ValueError('metadata can only be provided for a single column')\n            return Column(getattr(self._jc, "as")(_to_seq(sc, list(alias))))
Language: python
Code Tokens: ['def', 'alias', '(', 'self', ',', '*', 'alias', ',', '*', '*', 'kwargs', ')', ':', 'metadata', '=', 'kwargs', '.', 'pop', '(', "'metadata'", ',', 'None', ')', 'assert', 'not', 'kwargs', ',', "'Unexpected kwargs where passed: %s'", '%', 'kwargs', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'len', '(', 'alias', ')', '==', '1', ':', 'if', 'metadata', ':', 'jmeta', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'sql', '.', 'types', '.', 'Metadata', '.', 'fromJson', '(', 'json', '.', 'dumps', '(', 'metadata', ')', ')', 'return', 'Column', '(', 'getattr', '(', 'self', '.', '_jc', ',', '"as"', ')', '(', 'alias', '[', '0', ']', ',', 'jmeta', ')', ')', 'else', ':', 'return', 'Column', '(', 'getattr', '(', 'self', '.', '_jc', ',', '"as"', ')', '(', 'alias', '[', '0', ']', ')', ')', 'else', ':', 'if', 'metadata', ':', 'raise', 'ValueError', '(', "'metadata can only be provided for a single column'", ')', 'return', 'Column', '(', 'getattr', '(', 'self', '.', '_jc', ',', '"as"', ')', '(', '_to_seq', '(', 'sc', ',', 'list', '(', 'alias', ')', ')', ')', ')']
Docstring: Returns this column aliased with a new name or names (in the case of expressions that        return more than one column, such as explode).        :param alias: strings of desired column names (collects all positional arguments passed)        :param metadata: a dict of information to be stored in ``metadata`` attribute of the            corresponding :class: `StructField` (optional, keyword only argument)        .. versionchanged:: 2.2           Added optional ``metadata`` argument.        >>> df.select(df.age.alias("age2")).collect()        [Row(age2=2), Row(age2=5)]        >>> df.select(df.age.alias("age3", metadata={'max': 99})).schema['age3'].metadata['max']        99
*******__*******
Code:def cast(self, dataType):\n        """ Convert the column into type ``dataType``.\n\n        >>> df.select(df.age.cast("string").alias('ages')).collect()\n        [Row(ages=u'2'), Row(ages=u'5')]\n        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n        [Row(ages=u'2'), Row(ages=u'5')]\n        """\n        if isinstance(dataType, basestring):\n            jc = self._jc.cast(dataType)\n        elif isinstance(dataType, DataType):\n            from pyspark.sql import SparkSession\n            spark = SparkSession.builder.getOrCreate()\n            jdt = spark._jsparkSession.parseDataType(dataType.json())\n            jc = self._jc.cast(jdt)\n        else:\n            raise TypeError("unexpected type: %s" % type(dataType))\n        return Column(jc)
Language: python
Code Tokens: ['def', 'cast', '(', 'self', ',', 'dataType', ')', ':', 'if', 'isinstance', '(', 'dataType', ',', 'basestring', ')', ':', 'jc', '=', 'self', '.', '_jc', '.', 'cast', '(', 'dataType', ')', 'elif', 'isinstance', '(', 'dataType', ',', 'DataType', ')', ':', 'from', 'pyspark', '.', 'sql', 'import', 'SparkSession', 'spark', '=', 'SparkSession', '.', 'builder', '.', 'getOrCreate', '(', ')', 'jdt', '=', 'spark', '.', '_jsparkSession', '.', 'parseDataType', '(', 'dataType', '.', 'json', '(', ')', ')', 'jc', '=', 'self', '.', '_jc', '.', 'cast', '(', 'jdt', ')', 'else', ':', 'raise', 'TypeError', '(', '"unexpected type: %s"', '%', 'type', '(', 'dataType', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Convert the column into type ``dataType``.        >>> df.select(df.age.cast("string").alias('ages')).collect()        [Row(ages=u'2'), Row(ages=u'5')]        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()        [Row(ages=u'2'), Row(ages=u'5')]
*******__*******
Code:def when(self, condition, value):\n        """\n        Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n        See :func:`pyspark.sql.functions.when` for example usage.\n\n        :param condition: a boolean :class:`Column` expression.\n        :param value: a literal value, or a :class:`Column` expression.\n\n        >>> from pyspark.sql import functions as F\n        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n        +-----+------------------------------------------------------------+\n        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n        +-----+------------------------------------------------------------+\n        |Alice|                                                          -1|\n        |  Bob|                                                           1|\n        +-----+------------------------------------------------------------+\n        """\n        if not isinstance(condition, Column):\n            raise TypeError("condition should be a Column")\n        v = value._jc if isinstance(value, Column) else value\n        jc = self._jc.when(condition._jc, v)\n        return Column(jc)
Language: python
Code Tokens: ['def', 'when', '(', 'self', ',', 'condition', ',', 'value', ')', ':', 'if', 'not', 'isinstance', '(', 'condition', ',', 'Column', ')', ':', 'raise', 'TypeError', '(', '"condition should be a Column"', ')', 'v', '=', 'value', '.', '_jc', 'if', 'isinstance', '(', 'value', ',', 'Column', ')', 'else', 'value', 'jc', '=', 'self', '.', '_jc', '.', 'when', '(', 'condition', '.', '_jc', ',', 'v', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Evaluates a list of conditions and returns one of multiple possible result expressions.        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.        See :func:`pyspark.sql.functions.when` for example usage.        :param condition: a boolean :class:`Column` expression.        :param value: a literal value, or a :class:`Column` expression.        >>> from pyspark.sql import functions as F        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()        +-----+------------------------------------------------------------+        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|        +-----+------------------------------------------------------------+        |Alice|                                                          -1|        |  Bob|                                                           1|        +-----+------------------------------------------------------------+
*******__*******
Code:def otherwise(self, value):\n        """\n        Evaluates a list of conditions and returns one of multiple possible result expressions.\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n        See :func:`pyspark.sql.functions.when` for example usage.\n\n        :param value: a literal value, or a :class:`Column` expression.\n\n        >>> from pyspark.sql import functions as F\n        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n        +-----+-------------------------------------+\n        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n        +-----+-------------------------------------+\n        |Alice|                                    0|\n        |  Bob|                                    1|\n        +-----+-------------------------------------+\n        """\n        v = value._jc if isinstance(value, Column) else value\n        jc = self._jc.otherwise(v)\n        return Column(jc)
Language: python
Code Tokens: ['def', 'otherwise', '(', 'self', ',', 'value', ')', ':', 'v', '=', 'value', '.', '_jc', 'if', 'isinstance', '(', 'value', ',', 'Column', ')', 'else', 'value', 'jc', '=', 'self', '.', '_jc', '.', 'otherwise', '(', 'v', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Evaluates a list of conditions and returns one of multiple possible result expressions.        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.        See :func:`pyspark.sql.functions.when` for example usage.        :param value: a literal value, or a :class:`Column` expression.        >>> from pyspark.sql import functions as F        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()        +-----+-------------------------------------+        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|        +-----+-------------------------------------+        |Alice|                                    0|        |  Bob|                                    1|        +-----+-------------------------------------+
*******__*******
Code:def over(self, window):\n        """\n        Define a windowing column.\n\n        :param window: a :class:`WindowSpec`\n        :return: a Column\n\n        >>> from pyspark.sql import Window\n        >>> window = Window.partitionBy("name").orderBy("age").rowsBetween(-1, 1)\n        >>> from pyspark.sql.functions import rank, min\n        >>> # df.select(rank().over(window), min('age').over(window))\n        """\n        from pyspark.sql.window import WindowSpec\n        if not isinstance(window, WindowSpec):\n            raise TypeError("window should be WindowSpec")\n        jc = self._jc.over(window._jspec)\n        return Column(jc)
Language: python
Code Tokens: ['def', 'over', '(', 'self', ',', 'window', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'window', 'import', 'WindowSpec', 'if', 'not', 'isinstance', '(', 'window', ',', 'WindowSpec', ')', ':', 'raise', 'TypeError', '(', '"window should be WindowSpec"', ')', 'jc', '=', 'self', '.', '_jc', '.', 'over', '(', 'window', '.', '_jspec', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Define a windowing column.        :param window: a :class:`WindowSpec`        :return: a Column        >>> from pyspark.sql import Window        >>> window = Window.partitionBy("name").orderBy("age").rowsBetween(-1, 1)        >>> from pyspark.sql.functions import rank, min        >>> # df.select(rank().over(window), min('age').over(window))
*******__*******
Code:def transform(self, vector):\n        """\n        Applies transformation on a vector or an RDD[Vector].\n\n        .. note:: In Python, transform cannot currently be used within\n            an RDD transformation or action.\n            Call transform directly on the RDD instead.\n\n        :param vector: Vector or RDD of Vector to be transformed.\n        """\n        if isinstance(vector, RDD):\n            vector = vector.map(_convert_to_vector)\n        else:\n            vector = _convert_to_vector(vector)\n        return self.call("transform", vector)
Language: python
Code Tokens: ['def', 'transform', '(', 'self', ',', 'vector', ')', ':', 'if', 'isinstance', '(', 'vector', ',', 'RDD', ')', ':', 'vector', '=', 'vector', '.', 'map', '(', '_convert_to_vector', ')', 'else', ':', 'vector', '=', '_convert_to_vector', '(', 'vector', ')', 'return', 'self', '.', 'call', '(', '"transform"', ',', 'vector', ')']
Docstring: Applies transformation on a vector or an RDD[Vector].        .. note:: In Python, transform cannot currently be used within            an RDD transformation or action.            Call transform directly on the RDD instead.        :param vector: Vector or RDD of Vector to be transformed.
*******__*******
Code:def fit(self, dataset):\n        """\n        Computes the mean and variance and stores as a model to be used\n        for later scaling.\n\n        :param dataset: The data used to compute the mean and variance\n                     to build the transformation model.\n        :return: a StandardScalarModel\n        """\n        dataset = dataset.map(_convert_to_vector)\n        jmodel = callMLlibFunc("fitStandardScaler", self.withMean, self.withStd, dataset)\n        return StandardScalerModel(jmodel)
Language: python
Code Tokens: ['def', 'fit', '(', 'self', ',', 'dataset', ')', ':', 'dataset', '=', 'dataset', '.', 'map', '(', '_convert_to_vector', ')', 'jmodel', '=', 'callMLlibFunc', '(', '"fitStandardScaler"', ',', 'self', '.', 'withMean', ',', 'self', '.', 'withStd', ',', 'dataset', ')', 'return', 'StandardScalerModel', '(', 'jmodel', ')']
Docstring: Computes the mean and variance and stores as a model to be used        for later scaling.        :param dataset: The data used to compute the mean and variance                     to build the transformation model.        :return: a StandardScalarModel
*******__*******
Code:def fit(self, data):\n        """\n        Returns a ChiSquared feature selector.\n\n        :param data: an `RDD[LabeledPoint]` containing the labeled dataset\n                     with categorical features. Real-valued features will be\n                     treated as categorical for each distinct value.\n                     Apply feature discretizer before using this function.\n        """\n        jmodel = callMLlibFunc("fitChiSqSelector", self.selectorType, self.numTopFeatures,\n                               self.percentile, self.fpr, self.fdr, self.fwe, data)\n        return ChiSqSelectorModel(jmodel)
Language: python
Code Tokens: ['def', 'fit', '(', 'self', ',', 'data', ')', ':', 'jmodel', '=', 'callMLlibFunc', '(', '"fitChiSqSelector"', ',', 'self', '.', 'selectorType', ',', 'self', '.', 'numTopFeatures', ',', 'self', '.', 'percentile', ',', 'self', '.', 'fpr', ',', 'self', '.', 'fdr', ',', 'self', '.', 'fwe', ',', 'data', ')', 'return', 'ChiSqSelectorModel', '(', 'jmodel', ')']
Docstring: Returns a ChiSquared feature selector.        :param data: an `RDD[LabeledPoint]` containing the labeled dataset                     with categorical features. Real-valued features will be                     treated as categorical for each distinct value.                     Apply feature discretizer before using this function.
*******__*******
Code:def fit(self, data):\n        """\n        Computes a [[PCAModel]] that contains the principal components of the input vectors.\n        :param data: source vectors\n        """\n        jmodel = callMLlibFunc("fitPCA", self.k, data)\n        return PCAModel(jmodel)
Language: python
Code Tokens: ['def', 'fit', '(', 'self', ',', 'data', ')', ':', 'jmodel', '=', 'callMLlibFunc', '(', '"fitPCA"', ',', 'self', '.', 'k', ',', 'data', ')', 'return', 'PCAModel', '(', 'jmodel', ')']
Docstring: Computes a [[PCAModel]] that contains the principal components of the input vectors.        :param data: source vectors
*******__*******
Code:def transform(self, document):\n        """\n        Transforms the input document (list of terms) to term frequency\n        vectors, or transform the RDD of document to RDD of term\n        frequency vectors.\n        """\n        if isinstance(document, RDD):\n            return document.map(self.transform)\n\n        freq = {}\n        for term in document:\n            i = self.indexOf(term)\n            freq[i] = 1.0 if self.binary else freq.get(i, 0) + 1.0\n        return Vectors.sparse(self.numFeatures, freq.items())
Language: python
Code Tokens: ['def', 'transform', '(', 'self', ',', 'document', ')', ':', 'if', 'isinstance', '(', 'document', ',', 'RDD', ')', ':', 'return', 'document', '.', 'map', '(', 'self', '.', 'transform', ')', 'freq', '=', '{', '}', 'for', 'term', 'in', 'document', ':', 'i', '=', 'self', '.', 'indexOf', '(', 'term', ')', 'freq', '[', 'i', ']', '=', '1.0', 'if', 'self', '.', 'binary', 'else', 'freq', '.', 'get', '(', 'i', ',', '0', ')', '+', '1.0', 'return', 'Vectors', '.', 'sparse', '(', 'self', '.', 'numFeatures', ',', 'freq', '.', 'items', '(', ')', ')']
Docstring: Transforms the input document (list of terms) to term frequency        vectors, or transform the RDD of document to RDD of term        frequency vectors.
*******__*******
Code:def fit(self, dataset):\n        """\n        Computes the inverse document frequency.\n\n        :param dataset: an RDD of term frequency vectors\n        """\n        if not isinstance(dataset, RDD):\n            raise TypeError("dataset should be an RDD of term frequency vectors")\n        jmodel = callMLlibFunc("fitIDF", self.minDocFreq, dataset.map(_convert_to_vector))\n        return IDFModel(jmodel)
Language: python
Code Tokens: ['def', 'fit', '(', 'self', ',', 'dataset', ')', ':', 'if', 'not', 'isinstance', '(', 'dataset', ',', 'RDD', ')', ':', 'raise', 'TypeError', '(', '"dataset should be an RDD of term frequency vectors"', ')', 'jmodel', '=', 'callMLlibFunc', '(', '"fitIDF"', ',', 'self', '.', 'minDocFreq', ',', 'dataset', '.', 'map', '(', '_convert_to_vector', ')', ')', 'return', 'IDFModel', '(', 'jmodel', ')']
Docstring: Computes the inverse document frequency.        :param dataset: an RDD of term frequency vectors
*******__*******
Code:def findSynonyms(self, word, num):\n        """\n        Find synonyms of a word\n\n        :param word: a word or a vector representation of word\n        :param num: number of synonyms to find\n        :return: array of (word, cosineSimilarity)\n\n        .. note:: Local use only\n        """\n        if not isinstance(word, basestring):\n            word = _convert_to_vector(word)\n        words, similarity = self.call("findSynonyms", word, num)\n        return zip(words, similarity)
Language: python
Code Tokens: ['def', 'findSynonyms', '(', 'self', ',', 'word', ',', 'num', ')', ':', 'if', 'not', 'isinstance', '(', 'word', ',', 'basestring', ')', ':', 'word', '=', '_convert_to_vector', '(', 'word', ')', 'words', ',', 'similarity', '=', 'self', '.', 'call', '(', '"findSynonyms"', ',', 'word', ',', 'num', ')', 'return', 'zip', '(', 'words', ',', 'similarity', ')']
Docstring: Find synonyms of a word        :param word: a word or a vector representation of word        :param num: number of synonyms to find        :return: array of (word, cosineSimilarity)        .. note:: Local use only
*******__*******
Code:def load(cls, sc, path):\n        """\n        Load a model from the given path.\n        """\n        jmodel = sc._jvm.org.apache.spark.mllib.feature \\n            .Word2VecModel.load(sc._jsc.sc(), path)\n        model = sc._jvm.org.apache.spark.mllib.api.python.Word2VecModelWrapper(jmodel)\n        return Word2VecModel(model)
Language: python
Code Tokens: ['def', 'load', '(', 'cls', ',', 'sc', ',', 'path', ')', ':', 'jmodel', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'feature', '.', 'Word2VecModel', '.', 'load', '(', 'sc', '.', '_jsc', '.', 'sc', '(', ')', ',', 'path', ')', 'model', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'api', '.', 'python', '.', 'Word2VecModelWrapper', '(', 'jmodel', ')', 'return', 'Word2VecModel', '(', 'model', ')']
Docstring: Load a model from the given path.
*******__*******
Code:def transform(self, vector):\n        """\n        Computes the Hadamard product of the vector.\n        """\n        if isinstance(vector, RDD):\n            vector = vector.map(_convert_to_vector)\n\n        else:\n            vector = _convert_to_vector(vector)\n        return callMLlibFunc("elementwiseProductVector", self.scalingVector, vector)
Language: python
Code Tokens: ['def', 'transform', '(', 'self', ',', 'vector', ')', ':', 'if', 'isinstance', '(', 'vector', ',', 'RDD', ')', ':', 'vector', '=', 'vector', '.', 'map', '(', '_convert_to_vector', ')', 'else', ':', 'vector', '=', '_convert_to_vector', '(', 'vector', ')', 'return', 'callMLlibFunc', '(', '"elementwiseProductVector"', ',', 'self', '.', 'scalingVector', ',', 'vector', ')']
Docstring: Computes the Hadamard product of the vector.
*******__*******
Code:def predict(self, x):\n        """\n        Predict values for a single data point or an RDD of points using\n        the model trained.\n\n        .. note:: In Python, predict cannot currently be used within an RDD\n            transformation or action.\n            Call predict directly on the RDD instead.\n        """\n        if isinstance(x, RDD):\n            return self.call("predict", x.map(_convert_to_vector))\n\n        else:\n            return self.call("predict", _convert_to_vector(x))
Language: python
Code Tokens: ['def', 'predict', '(', 'self', ',', 'x', ')', ':', 'if', 'isinstance', '(', 'x', ',', 'RDD', ')', ':', 'return', 'self', '.', 'call', '(', '"predict"', ',', 'x', '.', 'map', '(', '_convert_to_vector', ')', ')', 'else', ':', 'return', 'self', '.', 'call', '(', '"predict"', ',', '_convert_to_vector', '(', 'x', ')', ')']
Docstring: Predict values for a single data point or an RDD of points using        the model trained.        .. note:: In Python, predict cannot currently be used within an RDD            transformation or action.            Call predict directly on the RDD instead.
*******__*******
Code:def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,\n                        impurity="gini", maxDepth=5, maxBins=32, minInstancesPerNode=1,\n                        minInfoGain=0.0):\n        """\n        Train a decision tree model for classification.\n\n        :param data:\n          Training data: RDD of LabeledPoint. Labels should take values\n          {0, 1, ..., numClasses-1}.\n        :param numClasses:\n          Number of classes for classification.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param impurity:\n          Criterion used for information gain calculation.\n          Supported values: "gini" or "entropy".\n          (default: "gini")\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 5)\n        :param maxBins:\n          Number of bins used for finding splits at each node.\n          (default: 32)\n        :param minInstancesPerNode:\n          Minimum number of instances required at child nodes to create\n          the parent split.\n          (default: 1)\n        :param minInfoGain:\n          Minimum info gain required to create a split.\n          (default: 0.0)\n        :return:\n          DecisionTreeModel.\n\n        Example usage:\n\n        >>> from numpy import array\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import DecisionTree\n        >>>\n        >>> data = [\n        ...     LabeledPoint(0.0, [0.0]),\n        ...     LabeledPoint(1.0, [1.0]),\n        ...     LabeledPoint(1.0, [2.0]),\n        ...     LabeledPoint(1.0, [3.0])\n        ... ]\n        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})\n        >>> print(model)\n        DecisionTreeModel classifier of depth 1 with 3 nodes\n\n        >>> print(model.toDebugString())\n        DecisionTreeModel classifier of depth 1 with 3 nodes\n          If (feature 0 <= 0.5)\n           Predict: 0.0\n          Else (feature 0 > 0.5)\n           Predict: 1.0\n        <BLANKLINE>\n        >>> model.predict(array([1.0]))\n        1.0\n        >>> model.predict(array([0.0]))\n        0.0\n        >>> rdd = sc.parallelize([[1.0], [0.0]])\n        >>> model.predict(rdd).collect()\n        [1.0, 0.0]\n        """\n        return cls._train(data, "classification", numClasses, categoricalFeaturesInfo,\n                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)
Language: python
Code Tokens: ['def', 'trainClassifier', '(', 'cls', ',', 'data', ',', 'numClasses', ',', 'categoricalFeaturesInfo', ',', 'impurity', '=', '"gini"', ',', 'maxDepth', '=', '5', ',', 'maxBins', '=', '32', ',', 'minInstancesPerNode', '=', '1', ',', 'minInfoGain', '=', '0.0', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '"classification"', ',', 'numClasses', ',', 'categoricalFeaturesInfo', ',', 'impurity', ',', 'maxDepth', ',', 'maxBins', ',', 'minInstancesPerNode', ',', 'minInfoGain', ')']
Docstring: Train a decision tree model for classification.        :param data:          Training data: RDD of LabeledPoint. Labels should take values          {0, 1, ..., numClasses-1}.        :param numClasses:          Number of classes for classification.        :param categoricalFeaturesInfo:          Map storing arity of categorical features. An entry (n -> k)          indicates that feature n is categorical with k categories          indexed from 0: {0, 1, ..., k-1}.        :param impurity:          Criterion used for information gain calculation.          Supported values: "gini" or "entropy".          (default: "gini")        :param maxDepth:          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1          means 1 internal node + 2 leaf nodes).          (default: 5)        :param maxBins:          Number of bins used for finding splits at each node.          (default: 32)        :param minInstancesPerNode:          Minimum number of instances required at child nodes to create          the parent split.          (default: 1)        :param minInfoGain:          Minimum info gain required to create a split.          (default: 0.0)        :return:          DecisionTreeModel.        Example usage:        >>> from numpy import array        >>> from pyspark.mllib.regression import LabeledPoint        >>> from pyspark.mllib.tree import DecisionTree        >>>        >>> data = [        ...     LabeledPoint(0.0, [0.0]),        ...     LabeledPoint(1.0, [1.0]),        ...     LabeledPoint(1.0, [2.0]),        ...     LabeledPoint(1.0, [3.0])        ... ]        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})        >>> print(model)        DecisionTreeModel classifier of depth 1 with 3 nodes        >>> print(model.toDebugString())        DecisionTreeModel classifier of depth 1 with 3 nodes          If (feature 0 <= 0.5)           Predict: 0.0          Else (feature 0 > 0.5)           Predict: 1.0        <BLANKLINE>        >>> model.predict(array([1.0]))        1.0        >>> model.predict(array([0.0]))        0.0        >>> rdd = sc.parallelize([[1.0], [0.0]])        >>> model.predict(rdd).collect()        [1.0, 0.0]
*******__*******
Code:def trainRegressor(cls, data, categoricalFeaturesInfo,\n                       impurity="variance", maxDepth=5, maxBins=32, minInstancesPerNode=1,\n                       minInfoGain=0.0):\n        """\n        Train a decision tree model for regression.\n\n        :param data:\n          Training data: RDD of LabeledPoint. Labels are real numbers.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param impurity:\n          Criterion used for information gain calculation.\n          The only supported value for regression is "variance".\n          (default: "variance")\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 5)\n        :param maxBins:\n          Number of bins used for finding splits at each node.\n          (default: 32)\n        :param minInstancesPerNode:\n          Minimum number of instances required at child nodes to create\n          the parent split.\n          (default: 1)\n        :param minInfoGain:\n          Minimum info gain required to create a split.\n          (default: 0.0)\n        :return:\n          DecisionTreeModel.\n\n        Example usage:\n\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import DecisionTree\n        >>> from pyspark.mllib.linalg import SparseVector\n        >>>\n        >>> sparse_data = [\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\n        ... ]\n        >>>\n        >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})\n        >>> model.predict(SparseVector(2, {1: 1.0}))\n        1.0\n        >>> model.predict(SparseVector(2, {1: 0.0}))\n        0.0\n        >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])\n        >>> model.predict(rdd).collect()\n        [1.0, 0.0]\n        """\n        return cls._train(data, "regression", 0, categoricalFeaturesInfo,\n                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)
Language: python
Code Tokens: ['def', 'trainRegressor', '(', 'cls', ',', 'data', ',', 'categoricalFeaturesInfo', ',', 'impurity', '=', '"variance"', ',', 'maxDepth', '=', '5', ',', 'maxBins', '=', '32', ',', 'minInstancesPerNode', '=', '1', ',', 'minInfoGain', '=', '0.0', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '"regression"', ',', '0', ',', 'categoricalFeaturesInfo', ',', 'impurity', ',', 'maxDepth', ',', 'maxBins', ',', 'minInstancesPerNode', ',', 'minInfoGain', ')']
Docstring: Train a decision tree model for regression.        :param data:          Training data: RDD of LabeledPoint. Labels are real numbers.        :param categoricalFeaturesInfo:          Map storing arity of categorical features. An entry (n -> k)          indicates that feature n is categorical with k categories          indexed from 0: {0, 1, ..., k-1}.        :param impurity:          Criterion used for information gain calculation.          The only supported value for regression is "variance".          (default: "variance")        :param maxDepth:          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1          means 1 internal node + 2 leaf nodes).          (default: 5)        :param maxBins:          Number of bins used for finding splits at each node.          (default: 32)        :param minInstancesPerNode:          Minimum number of instances required at child nodes to create          the parent split.          (default: 1)        :param minInfoGain:          Minimum info gain required to create a split.          (default: 0.0)        :return:          DecisionTreeModel.        Example usage:        >>> from pyspark.mllib.regression import LabeledPoint        >>> from pyspark.mllib.tree import DecisionTree        >>> from pyspark.mllib.linalg import SparseVector        >>>        >>> sparse_data = [        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))        ... ]        >>>        >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})        >>> model.predict(SparseVector(2, {1: 1.0}))        1.0        >>> model.predict(SparseVector(2, {1: 0.0}))        0.0        >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])        >>> model.predict(rdd).collect()        [1.0, 0.0]
*******__*******
Code:def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, numTrees,\n                        featureSubsetStrategy="auto", impurity="gini", maxDepth=4, maxBins=32,\n                        seed=None):\n        """\n        Train a random forest model for binary or multiclass\n        classification.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels should take values\n          {0, 1, ..., numClasses-1}.\n        :param numClasses:\n          Number of classes for classification.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param numTrees:\n          Number of trees in the random forest.\n        :param featureSubsetStrategy:\n          Number of features to consider for splits at each node.\n          Supported values: "auto", "all", "sqrt", "log2", "onethird".\n          If "auto" is set, this parameter is set based on numTrees:\n          if numTrees == 1, set to "all";\n          if numTrees > 1 (forest) set to "sqrt".\n          (default: "auto")\n        :param impurity:\n          Criterion used for information gain calculation.\n          Supported values: "gini" or "entropy".\n          (default: "gini")\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 4)\n        :param maxBins:\n          Maximum number of bins used for splitting features.\n          (default: 32)\n        :param seed:\n          Random seed for bootstrapping and choosing feature subsets.\n          Set as None to generate seed based on system time.\n          (default: None)\n        :return:\n          RandomForestModel that can be used for prediction.\n\n        Example usage:\n\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import RandomForest\n        >>>\n        >>> data = [\n        ...     LabeledPoint(0.0, [0.0]),\n        ...     LabeledPoint(0.0, [1.0]),\n        ...     LabeledPoint(1.0, [2.0]),\n        ...     LabeledPoint(1.0, [3.0])\n        ... ]\n        >>> model = RandomForest.trainClassifier(sc.parallelize(data), 2, {}, 3, seed=42)\n        >>> model.numTrees()\n        3\n        >>> model.totalNumNodes()\n        7\n        >>> print(model)\n        TreeEnsembleModel classifier with 3 trees\n        <BLANKLINE>\n        >>> print(model.toDebugString())\n        TreeEnsembleModel classifier with 3 trees\n        <BLANKLINE>\n          Tree 0:\n            Predict: 1.0\n          Tree 1:\n            If (feature 0 <= 1.5)\n             Predict: 0.0\n            Else (feature 0 > 1.5)\n             Predict: 1.0\n          Tree 2:\n            If (feature 0 <= 1.5)\n             Predict: 0.0\n            Else (feature 0 > 1.5)\n             Predict: 1.0\n        <BLANKLINE>\n        >>> model.predict([2.0])\n        1.0\n        >>> model.predict([0.0])\n        0.0\n        >>> rdd = sc.parallelize([[3.0], [1.0]])\n        >>> model.predict(rdd).collect()\n        [1.0, 0.0]\n        """\n        return cls._train(data, "classification", numClasses,\n                          categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity,\n                          maxDepth, maxBins, seed)
Language: python
Code Tokens: ['def', 'trainClassifier', '(', 'cls', ',', 'data', ',', 'numClasses', ',', 'categoricalFeaturesInfo', ',', 'numTrees', ',', 'featureSubsetStrategy', '=', '"auto"', ',', 'impurity', '=', '"gini"', ',', 'maxDepth', '=', '4', ',', 'maxBins', '=', '32', ',', 'seed', '=', 'None', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '"classification"', ',', 'numClasses', ',', 'categoricalFeaturesInfo', ',', 'numTrees', ',', 'featureSubsetStrategy', ',', 'impurity', ',', 'maxDepth', ',', 'maxBins', ',', 'seed', ')']
Docstring: Train a random forest model for binary or multiclass        classification.        :param data:          Training dataset: RDD of LabeledPoint. Labels should take values          {0, 1, ..., numClasses-1}.        :param numClasses:          Number of classes for classification.        :param categoricalFeaturesInfo:          Map storing arity of categorical features. An entry (n -> k)          indicates that feature n is categorical with k categories          indexed from 0: {0, 1, ..., k-1}.        :param numTrees:          Number of trees in the random forest.        :param featureSubsetStrategy:          Number of features to consider for splits at each node.          Supported values: "auto", "all", "sqrt", "log2", "onethird".          If "auto" is set, this parameter is set based on numTrees:          if numTrees == 1, set to "all";          if numTrees > 1 (forest) set to "sqrt".          (default: "auto")        :param impurity:          Criterion used for information gain calculation.          Supported values: "gini" or "entropy".          (default: "gini")        :param maxDepth:          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1          means 1 internal node + 2 leaf nodes).          (default: 4)        :param maxBins:          Maximum number of bins used for splitting features.          (default: 32)        :param seed:          Random seed for bootstrapping and choosing feature subsets.          Set as None to generate seed based on system time.          (default: None)        :return:          RandomForestModel that can be used for prediction.        Example usage:        >>> from pyspark.mllib.regression import LabeledPoint        >>> from pyspark.mllib.tree import RandomForest        >>>        >>> data = [        ...     LabeledPoint(0.0, [0.0]),        ...     LabeledPoint(0.0, [1.0]),        ...     LabeledPoint(1.0, [2.0]),        ...     LabeledPoint(1.0, [3.0])        ... ]        >>> model = RandomForest.trainClassifier(sc.parallelize(data), 2, {}, 3, seed=42)        >>> model.numTrees()        3        >>> model.totalNumNodes()        7        >>> print(model)        TreeEnsembleModel classifier with 3 trees        <BLANKLINE>        >>> print(model.toDebugString())        TreeEnsembleModel classifier with 3 trees        <BLANKLINE>          Tree 0:            Predict: 1.0          Tree 1:            If (feature 0 <= 1.5)             Predict: 0.0            Else (feature 0 > 1.5)             Predict: 1.0          Tree 2:            If (feature 0 <= 1.5)             Predict: 0.0            Else (feature 0 > 1.5)             Predict: 1.0        <BLANKLINE>        >>> model.predict([2.0])        1.0        >>> model.predict([0.0])        0.0        >>> rdd = sc.parallelize([[3.0], [1.0]])        >>> model.predict(rdd).collect()        [1.0, 0.0]
*******__*******
Code:def trainRegressor(cls, data, categoricalFeaturesInfo, numTrees, featureSubsetStrategy="auto",\n                       impurity="variance", maxDepth=4, maxBins=32, seed=None):\n        """\n        Train a random forest model for regression.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels are real numbers.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param numTrees:\n          Number of trees in the random forest.\n        :param featureSubsetStrategy:\n          Number of features to consider for splits at each node.\n          Supported values: "auto", "all", "sqrt", "log2", "onethird".\n          If "auto" is set, this parameter is set based on numTrees:\n          if numTrees == 1, set to "all";\n          if numTrees > 1 (forest) set to "onethird" for regression.\n          (default: "auto")\n        :param impurity:\n          Criterion used for information gain calculation.\n          The only supported value for regression is "variance".\n          (default: "variance")\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 4)\n        :param maxBins:\n          Maximum number of bins used for splitting features.\n          (default: 32)\n        :param seed:\n          Random seed for bootstrapping and choosing feature subsets.\n          Set as None to generate seed based on system time.\n          (default: None)\n        :return:\n          RandomForestModel that can be used for prediction.\n\n        Example usage:\n\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import RandomForest\n        >>> from pyspark.mllib.linalg import SparseVector\n        >>>\n        >>> sparse_data = [\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\n        ... ]\n        >>>\n        >>> model = RandomForest.trainRegressor(sc.parallelize(sparse_data), {}, 2, seed=42)\n        >>> model.numTrees()\n        2\n        >>> model.totalNumNodes()\n        4\n        >>> model.predict(SparseVector(2, {1: 1.0}))\n        1.0\n        >>> model.predict(SparseVector(2, {0: 1.0}))\n        0.5\n        >>> rdd = sc.parallelize([[0.0, 1.0], [1.0, 0.0]])\n        >>> model.predict(rdd).collect()\n        [1.0, 0.5]\n        """\n        return cls._train(data, "regression", 0, categoricalFeaturesInfo, numTrees,\n                          featureSubsetStrategy, impurity, maxDepth, maxBins, seed)
Language: python
Code Tokens: ['def', 'trainRegressor', '(', 'cls', ',', 'data', ',', 'categoricalFeaturesInfo', ',', 'numTrees', ',', 'featureSubsetStrategy', '=', '"auto"', ',', 'impurity', '=', '"variance"', ',', 'maxDepth', '=', '4', ',', 'maxBins', '=', '32', ',', 'seed', '=', 'None', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '"regression"', ',', '0', ',', 'categoricalFeaturesInfo', ',', 'numTrees', ',', 'featureSubsetStrategy', ',', 'impurity', ',', 'maxDepth', ',', 'maxBins', ',', 'seed', ')']
Docstring: Train a random forest model for regression.        :param data:          Training dataset: RDD of LabeledPoint. Labels are real numbers.        :param categoricalFeaturesInfo:          Map storing arity of categorical features. An entry (n -> k)          indicates that feature n is categorical with k categories          indexed from 0: {0, 1, ..., k-1}.        :param numTrees:          Number of trees in the random forest.        :param featureSubsetStrategy:          Number of features to consider for splits at each node.          Supported values: "auto", "all", "sqrt", "log2", "onethird".          If "auto" is set, this parameter is set based on numTrees:          if numTrees == 1, set to "all";          if numTrees > 1 (forest) set to "onethird" for regression.          (default: "auto")        :param impurity:          Criterion used for information gain calculation.          The only supported value for regression is "variance".          (default: "variance")        :param maxDepth:          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1          means 1 internal node + 2 leaf nodes).          (default: 4)        :param maxBins:          Maximum number of bins used for splitting features.          (default: 32)        :param seed:          Random seed for bootstrapping and choosing feature subsets.          Set as None to generate seed based on system time.          (default: None)        :return:          RandomForestModel that can be used for prediction.        Example usage:        >>> from pyspark.mllib.regression import LabeledPoint        >>> from pyspark.mllib.tree import RandomForest        >>> from pyspark.mllib.linalg import SparseVector        >>>        >>> sparse_data = [        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))        ... ]        >>>        >>> model = RandomForest.trainRegressor(sc.parallelize(sparse_data), {}, 2, seed=42)        >>> model.numTrees()        2        >>> model.totalNumNodes()        4        >>> model.predict(SparseVector(2, {1: 1.0}))        1.0        >>> model.predict(SparseVector(2, {0: 1.0}))        0.5        >>> rdd = sc.parallelize([[0.0, 1.0], [1.0, 0.0]])        >>> model.predict(rdd).collect()        [1.0, 0.5]
*******__*******
Code:def trainClassifier(cls, data, categoricalFeaturesInfo,\n                        loss="logLoss", numIterations=100, learningRate=0.1, maxDepth=3,\n                        maxBins=32):\n        """\n        Train a gradient-boosted trees model for classification.\n\n        :param data:\n          Training dataset: RDD of LabeledPoint. Labels should take values\n          {0, 1}.\n        :param categoricalFeaturesInfo:\n          Map storing arity of categorical features. An entry (n -> k)\n          indicates that feature n is categorical with k categories\n          indexed from 0: {0, 1, ..., k-1}.\n        :param loss:\n          Loss function used for minimization during gradient boosting.\n          Supported values: "logLoss", "leastSquaresError",\n          "leastAbsoluteError".\n          (default: "logLoss")\n        :param numIterations:\n          Number of iterations of boosting.\n          (default: 100)\n        :param learningRate:\n          Learning rate for shrinking the contribution of each estimator.\n          The learning rate should be between in the interval (0, 1].\n          (default: 0.1)\n        :param maxDepth:\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\n          means 1 internal node + 2 leaf nodes).\n          (default: 3)\n        :param maxBins:\n          Maximum number of bins used for splitting features. DecisionTree\n          requires maxBins >= max categories.\n          (default: 32)\n        :return:\n          GradientBoostedTreesModel that can be used for prediction.\n\n        Example usage:\n\n        >>> from pyspark.mllib.regression import LabeledPoint\n        >>> from pyspark.mllib.tree import GradientBoostedTrees\n        >>>\n        >>> data = [\n        ...     LabeledPoint(0.0, [0.0]),\n        ...     LabeledPoint(0.0, [1.0]),\n        ...     LabeledPoint(1.0, [2.0]),\n        ...     LabeledPoint(1.0, [3.0])\n        ... ]\n        >>>\n        >>> model = GradientBoostedTrees.trainClassifier(sc.parallelize(data), {}, numIterations=10)\n        >>> model.numTrees()\n        10\n        >>> model.totalNumNodes()\n        30\n        >>> print(model)  # it already has newline\n        TreeEnsembleModel classifier with 10 trees\n        <BLANKLINE>\n        >>> model.predict([2.0])\n        1.0\n        >>> model.predict([0.0])\n        0.0\n        >>> rdd = sc.parallelize([[2.0], [0.0]])\n        >>> model.predict(rdd).collect()\n        [1.0, 0.0]\n        """\n        return cls._train(data, "classification", categoricalFeaturesInfo,\n                          loss, numIterations, learningRate, maxDepth, maxBins)
Language: python
Code Tokens: ['def', 'trainClassifier', '(', 'cls', ',', 'data', ',', 'categoricalFeaturesInfo', ',', 'loss', '=', '"logLoss"', ',', 'numIterations', '=', '100', ',', 'learningRate', '=', '0.1', ',', 'maxDepth', '=', '3', ',', 'maxBins', '=', '32', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '"classification"', ',', 'categoricalFeaturesInfo', ',', 'loss', ',', 'numIterations', ',', 'learningRate', ',', 'maxDepth', ',', 'maxBins', ')']
Docstring: Train a gradient-boosted trees model for classification.        :param data:          Training dataset: RDD of LabeledPoint. Labels should take values          {0, 1}.        :param categoricalFeaturesInfo:          Map storing arity of categorical features. An entry (n -> k)          indicates that feature n is categorical with k categories          indexed from 0: {0, 1, ..., k-1}.        :param loss:          Loss function used for minimization during gradient boosting.          Supported values: "logLoss", "leastSquaresError",          "leastAbsoluteError".          (default: "logLoss")        :param numIterations:          Number of iterations of boosting.          (default: 100)        :param learningRate:          Learning rate for shrinking the contribution of each estimator.          The learning rate should be between in the interval (0, 1].          (default: 0.1)        :param maxDepth:          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1          means 1 internal node + 2 leaf nodes).          (default: 3)        :param maxBins:          Maximum number of bins used for splitting features. DecisionTree          requires maxBins >= max categories.          (default: 32)        :return:          GradientBoostedTreesModel that can be used for prediction.        Example usage:        >>> from pyspark.mllib.regression import LabeledPoint        >>> from pyspark.mllib.tree import GradientBoostedTrees        >>>        >>> data = [        ...     LabeledPoint(0.0, [0.0]),        ...     LabeledPoint(0.0, [1.0]),        ...     LabeledPoint(1.0, [2.0]),        ...     LabeledPoint(1.0, [3.0])        ... ]        >>>        >>> model = GradientBoostedTrees.trainClassifier(sc.parallelize(data), {}, numIterations=10)        >>> model.numTrees()        10        >>> model.totalNumNodes()        30        >>> print(model)  # it already has newline        TreeEnsembleModel classifier with 10 trees        <BLANKLINE>        >>> model.predict([2.0])        1.0        >>> model.predict([0.0])        0.0        >>> rdd = sc.parallelize([[2.0], [0.0]])        >>> model.predict(rdd).collect()        [1.0, 0.0]
*******__*******
Code:def set(self, key, value):\n        """Set a configuration property."""\n        # Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet.\n        if self._jconf is not None:\n            self._jconf.set(key, unicode(value))\n        else:\n            self._conf[key] = unicode(value)\n        return self
Language: python
Code Tokens: ['def', 'set', '(', 'self', ',', 'key', ',', 'value', ')', ':', '# Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet.', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'self', '.', '_jconf', '.', 'set', '(', 'key', ',', 'unicode', '(', 'value', ')', ')', 'else', ':', 'self', '.', '_conf', '[', 'key', ']', '=', 'unicode', '(', 'value', ')', 'return', 'self']
Docstring: Set a configuration property.
*******__*******
Code:def setIfMissing(self, key, value):\n        """Set a configuration property, if not already set."""\n        if self.get(key) is None:\n            self.set(key, value)\n        return self
Language: python
Code Tokens: ['def', 'setIfMissing', '(', 'self', ',', 'key', ',', 'value', ')', ':', 'if', 'self', '.', 'get', '(', 'key', ')', 'is', 'None', ':', 'self', '.', 'set', '(', 'key', ',', 'value', ')', 'return', 'self']
Docstring: Set a configuration property, if not already set.
*******__*******
Code:def setExecutorEnv(self, key=None, value=None, pairs=None):\n        """Set an environment variable to be passed to executors."""\n        if (key is not None and pairs is not None) or (key is None and pairs is None):\n            raise Exception("Either pass one key-value pair or a list of pairs")\n        elif key is not None:\n            self.set("spark.executorEnv." + key, value)\n        elif pairs is not None:\n            for (k, v) in pairs:\n                self.set("spark.executorEnv." + k, v)\n        return self
Language: python
Code Tokens: ['def', 'setExecutorEnv', '(', 'self', ',', 'key', '=', 'None', ',', 'value', '=', 'None', ',', 'pairs', '=', 'None', ')', ':', 'if', '(', 'key', 'is', 'not', 'None', 'and', 'pairs', 'is', 'not', 'None', ')', 'or', '(', 'key', 'is', 'None', 'and', 'pairs', 'is', 'None', ')', ':', 'raise', 'Exception', '(', '"Either pass one key-value pair or a list of pairs"', ')', 'elif', 'key', 'is', 'not', 'None', ':', 'self', '.', 'set', '(', '"spark.executorEnv."', '+', 'key', ',', 'value', ')', 'elif', 'pairs', 'is', 'not', 'None', ':', 'for', '(', 'k', ',', 'v', ')', 'in', 'pairs', ':', 'self', '.', 'set', '(', '"spark.executorEnv."', '+', 'k', ',', 'v', ')', 'return', 'self']
Docstring: Set an environment variable to be passed to executors.
*******__*******
Code:def setAll(self, pairs):\n        """\n        Set multiple parameters, passed as a list of key-value pairs.\n\n        :param pairs: list of key-value pairs to set\n        """\n        for (k, v) in pairs:\n            self.set(k, v)\n        return self
Language: python
Code Tokens: ['def', 'setAll', '(', 'self', ',', 'pairs', ')', ':', 'for', '(', 'k', ',', 'v', ')', 'in', 'pairs', ':', 'self', '.', 'set', '(', 'k', ',', 'v', ')', 'return', 'self']
Docstring: Set multiple parameters, passed as a list of key-value pairs.        :param pairs: list of key-value pairs to set
*******__*******
Code:def get(self, key, defaultValue=None):\n        """Get the configured value for some key, or return a default otherwise."""\n        if defaultValue is None:   # Py4J doesn't call the right get() if we pass None\n            if self._jconf is not None:\n                if not self._jconf.contains(key):\n                    return None\n                return self._jconf.get(key)\n            else:\n                if key not in self._conf:\n                    return None\n                return self._conf[key]\n        else:\n            if self._jconf is not None:\n                return self._jconf.get(key, defaultValue)\n            else:\n                return self._conf.get(key, defaultValue)
Language: python
Code Tokens: ['def', 'get', '(', 'self', ',', 'key', ',', 'defaultValue', '=', 'None', ')', ':', 'if', 'defaultValue', 'is', 'None', ':', "# Py4J doesn't call the right get() if we pass None", 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'if', 'not', 'self', '.', '_jconf', '.', 'contains', '(', 'key', ')', ':', 'return', 'None', 'return', 'self', '.', '_jconf', '.', 'get', '(', 'key', ')', 'else', ':', 'if', 'key', 'not', 'in', 'self', '.', '_conf', ':', 'return', 'None', 'return', 'self', '.', '_conf', '[', 'key', ']', 'else', ':', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'return', 'self', '.', '_jconf', '.', 'get', '(', 'key', ',', 'defaultValue', ')', 'else', ':', 'return', 'self', '.', '_conf', '.', 'get', '(', 'key', ',', 'defaultValue', ')']
Docstring: Get the configured value for some key, or return a default otherwise.
*******__*******
Code:def getAll(self):\n        """Get all values as a list of key-value pairs."""\n        if self._jconf is not None:\n            return [(elem._1(), elem._2()) for elem in self._jconf.getAll()]\n        else:\n            return self._conf.items()
Language: python
Code Tokens: ['def', 'getAll', '(', 'self', ')', ':', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'return', '[', '(', 'elem', '.', '_1', '(', ')', ',', 'elem', '.', '_2', '(', ')', ')', 'for', 'elem', 'in', 'self', '.', '_jconf', '.', 'getAll', '(', ')', ']', 'else', ':', 'return', 'self', '.', '_conf', '.', 'items', '(', ')']
Docstring: Get all values as a list of key-value pairs.
*******__*******
Code:def contains(self, key):\n        """Does this configuration contain a given key?"""\n        if self._jconf is not None:\n            return self._jconf.contains(key)\n        else:\n            return key in self._conf
Language: python
Code Tokens: ['def', 'contains', '(', 'self', ',', 'key', ')', ':', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'return', 'self', '.', '_jconf', '.', 'contains', '(', 'key', ')', 'else', ':', 'return', 'key', 'in', 'self', '.', '_conf']
Docstring: Does this configuration contain a given key?
*******__*******
Code:def toDebugString(self):\n        """\n        Returns a printable version of the configuration, as a list of\n        key=value pairs, one per line.\n        """\n        if self._jconf is not None:\n            return self._jconf.toDebugString()\n        else:\n            return '\n'.join('%s=%s' % (k, v) for k, v in self._conf.items())
Language: python
Code Tokens: ['def', 'toDebugString', '(', 'self', ')', ':', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'return', 'self', '.', '_jconf', '.', 'toDebugString', '(', ')', 'else', ':', 'return', "'\\n'", '.', 'join', '(', "'%s=%s'", '%', '(', 'k', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'self', '.', '_conf', '.', 'items', '(', ')', ')']
Docstring: Returns a printable version of the configuration, as a list of        key=value pairs, one per line.
*******__*******
Code:def listDatabases(self):\n        """Returns a list of databases available across all sessions."""\n        iter = self._jcatalog.listDatabases().toLocalIterator()\n        databases = []\n        while iter.hasNext():\n            jdb = iter.next()\n            databases.append(Database(\n                name=jdb.name(),\n                description=jdb.description(),\n                locationUri=jdb.locationUri()))\n        return databases
Language: python
Code Tokens: ['def', 'listDatabases', '(', 'self', ')', ':', 'iter', '=', 'self', '.', '_jcatalog', '.', 'listDatabases', '(', ')', '.', 'toLocalIterator', '(', ')', 'databases', '=', '[', ']', 'while', 'iter', '.', 'hasNext', '(', ')', ':', 'jdb', '=', 'iter', '.', 'next', '(', ')', 'databases', '.', 'append', '(', 'Database', '(', 'name', '=', 'jdb', '.', 'name', '(', ')', ',', 'description', '=', 'jdb', '.', 'description', '(', ')', ',', 'locationUri', '=', 'jdb', '.', 'locationUri', '(', ')', ')', ')', 'return', 'databases']
Docstring: Returns a list of databases available across all sessions.
*******__*******
Code:def listTables(self, dbName=None):\n        """Returns a list of tables/views in the specified database.\n\n        If no database is specified, the current database is used.\n        This includes all temporary views.\n        """\n        if dbName is None:\n            dbName = self.currentDatabase()\n        iter = self._jcatalog.listTables(dbName).toLocalIterator()\n        tables = []\n        while iter.hasNext():\n            jtable = iter.next()\n            tables.append(Table(\n                name=jtable.name(),\n                database=jtable.database(),\n                description=jtable.description(),\n                tableType=jtable.tableType(),\n                isTemporary=jtable.isTemporary()))\n        return tables
Language: python
Code Tokens: ['def', 'listTables', '(', 'self', ',', 'dbName', '=', 'None', ')', ':', 'if', 'dbName', 'is', 'None', ':', 'dbName', '=', 'self', '.', 'currentDatabase', '(', ')', 'iter', '=', 'self', '.', '_jcatalog', '.', 'listTables', '(', 'dbName', ')', '.', 'toLocalIterator', '(', ')', 'tables', '=', '[', ']', 'while', 'iter', '.', 'hasNext', '(', ')', ':', 'jtable', '=', 'iter', '.', 'next', '(', ')', 'tables', '.', 'append', '(', 'Table', '(', 'name', '=', 'jtable', '.', 'name', '(', ')', ',', 'database', '=', 'jtable', '.', 'database', '(', ')', ',', 'description', '=', 'jtable', '.', 'description', '(', ')', ',', 'tableType', '=', 'jtable', '.', 'tableType', '(', ')', ',', 'isTemporary', '=', 'jtable', '.', 'isTemporary', '(', ')', ')', ')', 'return', 'tables']
Docstring: Returns a list of tables/views in the specified database.        If no database is specified, the current database is used.        This includes all temporary views.
*******__*******
Code:def listFunctions(self, dbName=None):\n        """Returns a list of functions registered in the specified database.\n\n        If no database is specified, the current database is used.\n        This includes all temporary functions.\n        """\n        if dbName is None:\n            dbName = self.currentDatabase()\n        iter = self._jcatalog.listFunctions(dbName).toLocalIterator()\n        functions = []\n        while iter.hasNext():\n            jfunction = iter.next()\n            functions.append(Function(\n                name=jfunction.name(),\n                description=jfunction.description(),\n                className=jfunction.className(),\n                isTemporary=jfunction.isTemporary()))\n        return functions
Language: python
Code Tokens: ['def', 'listFunctions', '(', 'self', ',', 'dbName', '=', 'None', ')', ':', 'if', 'dbName', 'is', 'None', ':', 'dbName', '=', 'self', '.', 'currentDatabase', '(', ')', 'iter', '=', 'self', '.', '_jcatalog', '.', 'listFunctions', '(', 'dbName', ')', '.', 'toLocalIterator', '(', ')', 'functions', '=', '[', ']', 'while', 'iter', '.', 'hasNext', '(', ')', ':', 'jfunction', '=', 'iter', '.', 'next', '(', ')', 'functions', '.', 'append', '(', 'Function', '(', 'name', '=', 'jfunction', '.', 'name', '(', ')', ',', 'description', '=', 'jfunction', '.', 'description', '(', ')', ',', 'className', '=', 'jfunction', '.', 'className', '(', ')', ',', 'isTemporary', '=', 'jfunction', '.', 'isTemporary', '(', ')', ')', ')', 'return', 'functions']
Docstring: Returns a list of functions registered in the specified database.        If no database is specified, the current database is used.        This includes all temporary functions.
*******__*******
Code:def listColumns(self, tableName, dbName=None):\n        """Returns a list of columns for the given table/view in the specified database.\n\n        If no database is specified, the current database is used.\n\n        Note: the order of arguments here is different from that of its JVM counterpart\n        because Python does not support method overloading.\n        """\n        if dbName is None:\n            dbName = self.currentDatabase()\n        iter = self._jcatalog.listColumns(dbName, tableName).toLocalIterator()\n        columns = []\n        while iter.hasNext():\n            jcolumn = iter.next()\n            columns.append(Column(\n                name=jcolumn.name(),\n                description=jcolumn.description(),\n                dataType=jcolumn.dataType(),\n                nullable=jcolumn.nullable(),\n                isPartition=jcolumn.isPartition(),\n                isBucket=jcolumn.isBucket()))\n        return columns
Language: python
Code Tokens: ['def', 'listColumns', '(', 'self', ',', 'tableName', ',', 'dbName', '=', 'None', ')', ':', 'if', 'dbName', 'is', 'None', ':', 'dbName', '=', 'self', '.', 'currentDatabase', '(', ')', 'iter', '=', 'self', '.', '_jcatalog', '.', 'listColumns', '(', 'dbName', ',', 'tableName', ')', '.', 'toLocalIterator', '(', ')', 'columns', '=', '[', ']', 'while', 'iter', '.', 'hasNext', '(', ')', ':', 'jcolumn', '=', 'iter', '.', 'next', '(', ')', 'columns', '.', 'append', '(', 'Column', '(', 'name', '=', 'jcolumn', '.', 'name', '(', ')', ',', 'description', '=', 'jcolumn', '.', 'description', '(', ')', ',', 'dataType', '=', 'jcolumn', '.', 'dataType', '(', ')', ',', 'nullable', '=', 'jcolumn', '.', 'nullable', '(', ')', ',', 'isPartition', '=', 'jcolumn', '.', 'isPartition', '(', ')', ',', 'isBucket', '=', 'jcolumn', '.', 'isBucket', '(', ')', ')', ')', 'return', 'columns']
Docstring: Returns a list of columns for the given table/view in the specified database.        If no database is specified, the current database is used.        Note: the order of arguments here is different from that of its JVM counterpart        because Python does not support method overloading.
*******__*******
Code:def createExternalTable(self, tableName, path=None, source=None, schema=None, **options):\n        """Creates a table based on the dataset in a data source.\n\n        It returns the DataFrame associated with the external table.\n\n        The data source is specified by the ``source`` and a set of ``options``.\n        If ``source`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used.\n\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n        created external table.\n\n        :return: :class:`DataFrame`\n        """\n        warnings.warn(\n            "createExternalTable is deprecated since Spark 2.2, please use createTable instead.",\n            DeprecationWarning)\n        return self.createTable(tableName, path, source, schema, **options)
Language: python
Code Tokens: ['def', 'createExternalTable', '(', 'self', ',', 'tableName', ',', 'path', '=', 'None', ',', 'source', '=', 'None', ',', 'schema', '=', 'None', ',', '*', '*', 'options', ')', ':', 'warnings', '.', 'warn', '(', '"createExternalTable is deprecated since Spark 2.2, please use createTable instead."', ',', 'DeprecationWarning', ')', 'return', 'self', '.', 'createTable', '(', 'tableName', ',', 'path', ',', 'source', ',', 'schema', ',', '*', '*', 'options', ')']
Docstring: Creates a table based on the dataset in a data source.        It returns the DataFrame associated with the external table.        The data source is specified by the ``source`` and a set of ``options``.        If ``source`` is not specified, the default data source configured by        ``spark.sql.sources.default`` will be used.        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and        created external table.        :return: :class:`DataFrame`
*******__*******
Code:def createTable(self, tableName, path=None, source=None, schema=None, **options):\n        """Creates a table based on the dataset in a data source.\n\n        It returns the DataFrame associated with the table.\n\n        The data source is specified by the ``source`` and a set of ``options``.\n        If ``source`` is not specified, the default data source configured by\n        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n        created from the data at the given path. Otherwise a managed table is created.\n\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n        created table.\n\n        :return: :class:`DataFrame`\n        """\n        if path is not None:\n            options["path"] = path\n        if source is None:\n            source = self._sparkSession._wrapped._conf.defaultDataSourceName()\n        if schema is None:\n            df = self._jcatalog.createTable(tableName, source, options)\n        else:\n            if not isinstance(schema, StructType):\n                raise TypeError("schema should be StructType")\n            scala_datatype = self._jsparkSession.parseDataType(schema.json())\n            df = self._jcatalog.createTable(tableName, source, scala_datatype, options)\n        return DataFrame(df, self._sparkSession._wrapped)
Language: python
Code Tokens: ['def', 'createTable', '(', 'self', ',', 'tableName', ',', 'path', '=', 'None', ',', 'source', '=', 'None', ',', 'schema', '=', 'None', ',', '*', '*', 'options', ')', ':', 'if', 'path', 'is', 'not', 'None', ':', 'options', '[', '"path"', ']', '=', 'path', 'if', 'source', 'is', 'None', ':', 'source', '=', 'self', '.', '_sparkSession', '.', '_wrapped', '.', '_conf', '.', 'defaultDataSourceName', '(', ')', 'if', 'schema', 'is', 'None', ':', 'df', '=', 'self', '.', '_jcatalog', '.', 'createTable', '(', 'tableName', ',', 'source', ',', 'options', ')', 'else', ':', 'if', 'not', 'isinstance', '(', 'schema', ',', 'StructType', ')', ':', 'raise', 'TypeError', '(', '"schema should be StructType"', ')', 'scala_datatype', '=', 'self', '.', '_jsparkSession', '.', 'parseDataType', '(', 'schema', '.', 'json', '(', ')', ')', 'df', '=', 'self', '.', '_jcatalog', '.', 'createTable', '(', 'tableName', ',', 'source', ',', 'scala_datatype', ',', 'options', ')', 'return', 'DataFrame', '(', 'df', ',', 'self', '.', '_sparkSession', '.', '_wrapped', ')']
Docstring: Creates a table based on the dataset in a data source.        It returns the DataFrame associated with the table.        The data source is specified by the ``source`` and a set of ``options``.        If ``source`` is not specified, the default data source configured by        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is        created from the data at the given path. Otherwise a managed table is created.        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and        created table.        :return: :class:`DataFrame`
*******__*******
Code:def _load_from_socket(port, auth_secret):\n    """\n    Load data from a given socket, this is a blocking method thus only return when the socket\n    connection has been closed.\n    """\n    (sockfile, sock) = local_connect_and_auth(port, auth_secret)\n    # The barrier() call may block forever, so no timeout\n    sock.settimeout(None)\n    # Make a barrier() function call.\n    write_int(BARRIER_FUNCTION, sockfile)\n    sockfile.flush()\n\n    # Collect result.\n    res = UTF8Deserializer().loads(sockfile)\n\n    # Release resources.\n    sockfile.close()\n    sock.close()\n\n    return res
Language: python
Code Tokens: ['def', '_load_from_socket', '(', 'port', ',', 'auth_secret', ')', ':', '(', 'sockfile', ',', 'sock', ')', '=', 'local_connect_and_auth', '(', 'port', ',', 'auth_secret', ')', '# The barrier() call may block forever, so no timeout', 'sock', '.', 'settimeout', '(', 'None', ')', '# Make a barrier() function call.', 'write_int', '(', 'BARRIER_FUNCTION', ',', 'sockfile', ')', 'sockfile', '.', 'flush', '(', ')', '# Collect result.', 'res', '=', 'UTF8Deserializer', '(', ')', '.', 'loads', '(', 'sockfile', ')', '# Release resources.', 'sockfile', '.', 'close', '(', ')', 'sock', '.', 'close', '(', ')', 'return', 'res']
Docstring: Load data from a given socket, this is a blocking method thus only return when the socket    connection has been closed.
*******__*******
Code:def _getOrCreate(cls):\n        """\n        Internal function to get or create global BarrierTaskContext. We need to make sure\n        BarrierTaskContext is returned from here because it is needed in python worker reuse\n        scenario, see SPARK-25921 for more details.\n        """\n        if not isinstance(cls._taskContext, BarrierTaskContext):\n            cls._taskContext = object.__new__(cls)\n        return cls._taskContext
Language: python
Code Tokens: ['def', '_getOrCreate', '(', 'cls', ')', ':', 'if', 'not', 'isinstance', '(', 'cls', '.', '_taskContext', ',', 'BarrierTaskContext', ')', ':', 'cls', '.', '_taskContext', '=', 'object', '.', '__new__', '(', 'cls', ')', 'return', 'cls', '.', '_taskContext']
Docstring: Internal function to get or create global BarrierTaskContext. We need to make sure        BarrierTaskContext is returned from here because it is needed in python worker reuse        scenario, see SPARK-25921 for more details.
*******__*******
Code:def _initialize(cls, port, secret):\n        """\n        Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called\n        after BarrierTaskContext is initialized.\n        """\n        cls._port = port\n        cls._secret = secret
Language: python
Code Tokens: ['def', '_initialize', '(', 'cls', ',', 'port', ',', 'secret', ')', ':', 'cls', '.', '_port', '=', 'port', 'cls', '.', '_secret', '=', 'secret']
Docstring: Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called        after BarrierTaskContext is initialized.
*******__*******
Code:def barrier(self):\n        """\n        .. note:: Experimental\n\n        Sets a global barrier and waits until all tasks in this stage hit this barrier.\n        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks\n        in the same stage have reached this routine.\n\n        .. warning:: In a barrier stage, each task much have the same number of `barrier()`\n            calls, in all possible code branches.\n            Otherwise, you may get the job hanging or a SparkException after timeout.\n\n        .. versionadded:: 2.4.0\n        """\n        if self._port is None or self._secret is None:\n            raise Exception("Not supported to call barrier() before initialize " +\n                            "BarrierTaskContext.")\n        else:\n            _load_from_socket(self._port, self._secret)
Language: python
Code Tokens: ['def', 'barrier', '(', 'self', ')', ':', 'if', 'self', '.', '_port', 'is', 'None', 'or', 'self', '.', '_secret', 'is', 'None', ':', 'raise', 'Exception', '(', '"Not supported to call barrier() before initialize "', '+', '"BarrierTaskContext."', ')', 'else', ':', '_load_from_socket', '(', 'self', '.', '_port', ',', 'self', '.', '_secret', ')']
Docstring: .. note:: Experimental        Sets a global barrier and waits until all tasks in this stage hit this barrier.        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks        in the same stage have reached this routine.        .. warning:: In a barrier stage, each task much have the same number of `barrier()`            calls, in all possible code branches.            Otherwise, you may get the job hanging or a SparkException after timeout.        .. versionadded:: 2.4.0
*******__*******
Code:def getTaskInfos(self):\n        """\n        .. note:: Experimental\n\n        Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,\n        ordered by partition ID.\n\n        .. versionadded:: 2.4.0\n        """\n        if self._port is None or self._secret is None:\n            raise Exception("Not supported to call getTaskInfos() before initialize " +\n                            "BarrierTaskContext.")\n        else:\n            addresses = self._localProperties.get("addresses", "")\n            return [BarrierTaskInfo(h.strip()) for h in addresses.split(",")]
Language: python
Code Tokens: ['def', 'getTaskInfos', '(', 'self', ')', ':', 'if', 'self', '.', '_port', 'is', 'None', 'or', 'self', '.', '_secret', 'is', 'None', ':', 'raise', 'Exception', '(', '"Not supported to call getTaskInfos() before initialize "', '+', '"BarrierTaskContext."', ')', 'else', ':', 'addresses', '=', 'self', '.', '_localProperties', '.', 'get', '(', '"addresses"', ',', '""', ')', 'return', '[', 'BarrierTaskInfo', '(', 'h', '.', 'strip', '(', ')', ')', 'for', 'h', 'in', 'addresses', '.', 'split', '(', '","', ')', ']']
Docstring: .. note:: Experimental        Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,        ordered by partition ID.        .. versionadded:: 2.4.0
*******__*******
Code:def since(version):\n    """\n    A decorator that annotates a function to append the version of Spark the function was added.\n    """\n    import re\n    indent_p = re.compile(r'\n( +)')\n\n    def deco(f):\n        indents = indent_p.findall(f.__doc__)\n        indent = ' ' * (min(len(m) for m in indents) if indents else 0)\n        f.__doc__ = f.__doc__.rstrip() + "\n\n%s.. versionadded:: %s" % (indent, version)\n        return f\n    return deco
Language: python
Code Tokens: ['def', 'since', '(', 'version', ')', ':', 'import', 're', 'indent_p', '=', 're', '.', 'compile', '(', "r'\\n( +)'", ')', 'def', 'deco', '(', 'f', ')', ':', 'indents', '=', 'indent_p', '.', 'findall', '(', 'f', '.', '__doc__', ')', 'indent', '=', "' '", '*', '(', 'min', '(', 'len', '(', 'm', ')', 'for', 'm', 'in', 'indents', ')', 'if', 'indents', 'else', '0', ')', 'f', '.', '__doc__', '=', 'f', '.', '__doc__', '.', 'rstrip', '(', ')', '+', '"\\n\\n%s.. versionadded:: %s"', '%', '(', 'indent', ',', 'version', ')', 'return', 'f', 'return', 'deco']
Docstring: A decorator that annotates a function to append the version of Spark the function was added.
*******__*******
Code:def copy_func(f, name=None, sinceversion=None, doc=None):\n    """\n    Returns a function with same code, globals, defaults, closure, and\n    name (or provide a new name).\n    """\n    # See\n    # http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python\n    fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__,\n                            f.__closure__)\n    # in case f was given attrs (note this dict is a shallow copy):\n    fn.__dict__.update(f.__dict__)\n    if doc is not None:\n        fn.__doc__ = doc\n    if sinceversion is not None:\n        fn = since(sinceversion)(fn)\n    return fn
Language: python
Code Tokens: ['def', 'copy_func', '(', 'f', ',', 'name', '=', 'None', ',', 'sinceversion', '=', 'None', ',', 'doc', '=', 'None', ')', ':', '# See', '# http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python', 'fn', '=', 'types', '.', 'FunctionType', '(', 'f', '.', '__code__', ',', 'f', '.', '__globals__', ',', 'name', 'or', 'f', '.', '__name__', ',', 'f', '.', '__defaults__', ',', 'f', '.', '__closure__', ')', '# in case f was given attrs (note this dict is a shallow copy):', 'fn', '.', '__dict__', '.', 'update', '(', 'f', '.', '__dict__', ')', 'if', 'doc', 'is', 'not', 'None', ':', 'fn', '.', '__doc__', '=', 'doc', 'if', 'sinceversion', 'is', 'not', 'None', ':', 'fn', '=', 'since', '(', 'sinceversion', ')', '(', 'fn', ')', 'return', 'fn']
Docstring: Returns a function with same code, globals, defaults, closure, and    name (or provide a new name).
*******__*******
Code:def keyword_only(func):\n    """\n    A decorator that forces keyword arguments in the wrapped method\n    and saves actual input keyword arguments in `_input_kwargs`.\n\n    .. note:: Should only be used to wrap a method where first arg is `self`\n    """\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        if len(args) > 0:\n            raise TypeError("Method %s forces keyword arguments." % func.__name__)\n        self._input_kwargs = kwargs\n        return func(self, **kwargs)\n    return wrapper
Language: python
Code Tokens: ['def', 'keyword_only', '(', 'func', ')', ':', '@', 'wraps', '(', 'func', ')', 'def', 'wrapper', '(', 'self', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'if', 'len', '(', 'args', ')', '>', '0', ':', 'raise', 'TypeError', '(', '"Method %s forces keyword arguments."', '%', 'func', '.', '__name__', ')', 'self', '.', '_input_kwargs', '=', 'kwargs', 'return', 'func', '(', 'self', ',', '*', '*', 'kwargs', ')', 'return', 'wrapper']
Docstring: A decorator that forces keyword arguments in the wrapped method    and saves actual input keyword arguments in `_input_kwargs`.    .. note:: Should only be used to wrap a method where first arg is `self`
*******__*******
Code:def _gen_param_header(name, doc, defaultValueStr, typeConverter):\n    """\n    Generates the header part for shared variables\n\n    :param name: param name\n    :param doc: param doc\n    """\n    template = '''class Has$Name(Params):\n    """\n    Mixin for param $name: $doc\n    """\n\n    $name = Param(Params._dummy(), "$name", "$doc", typeConverter=$typeConverter)\n\n    def __init__(self):\n        super(Has$Name, self).__init__()'''\n\n    if defaultValueStr is not None:\n        template += '''\n        self._setDefault($name=$defaultValueStr)'''\n\n    Name = name[0].upper() + name[1:]\n    if typeConverter is None:\n        typeConverter = str(None)\n    return template \\n        .replace("$name", name) \\n        .replace("$Name", Name) \\n        .replace("$doc", doc) \\n        .replace("$defaultValueStr", str(defaultValueStr)) \\n        .replace("$typeConverter", typeConverter)
Language: python
Code Tokens: ['def', '_gen_param_header', '(', 'name', ',', 'doc', ',', 'defaultValueStr', ',', 'typeConverter', ')', ':', 'template', '=', '\'\'\'class Has$Name(Params):\n    """\n    Mixin for param $name: $doc\n    """\n\n    $name = Param(Params._dummy(), "$name", "$doc", typeConverter=$typeConverter)\n\n    def __init__(self):\n        super(Has$Name, self).__init__()\'\'\'', 'if', 'defaultValueStr', 'is', 'not', 'None', ':', 'template', '+=', "'''\n        self._setDefault($name=$defaultValueStr)'''", 'Name', '=', 'name', '[', '0', ']', '.', 'upper', '(', ')', '+', 'name', '[', '1', ':', ']', 'if', 'typeConverter', 'is', 'None', ':', 'typeConverter', '=', 'str', '(', 'None', ')', 'return', 'template', '.', 'replace', '(', '"$name"', ',', 'name', ')', '.', 'replace', '(', '"$Name"', ',', 'Name', ')', '.', 'replace', '(', '"$doc"', ',', 'doc', ')', '.', 'replace', '(', '"$defaultValueStr"', ',', 'str', '(', 'defaultValueStr', ')', ')', '.', 'replace', '(', '"$typeConverter"', ',', 'typeConverter', ')']
Docstring: Generates the header part for shared variables    :param name: param name    :param doc: param doc
*******__*******
Code:def _gen_param_code(name, doc, defaultValueStr):\n    """\n    Generates Python code for a shared param class.\n\n    :param name: param name\n    :param doc: param doc\n    :param defaultValueStr: string representation of the default value\n    :return: code string\n    """\n    # TODO: How to correctly inherit instance attributes?\n    template = '''\n    def set$Name(self, value):\n        """\n        Sets the value of :py:attr:`$name`.\n        """\n        return self._set($name=value)\n\n    def get$Name(self):\n        """\n        Gets the value of $name or its default value.\n        """\n        return self.getOrDefault(self.$name)'''\n\n    Name = name[0].upper() + name[1:]\n    return template \\n        .replace("$name", name) \\n        .replace("$Name", Name) \\n        .replace("$doc", doc) \\n        .replace("$defaultValueStr", str(defaultValueStr))
Language: python
Code Tokens: ['def', '_gen_param_code', '(', 'name', ',', 'doc', ',', 'defaultValueStr', ')', ':', '# TODO: How to correctly inherit instance attributes?', 'template', '=', '\'\'\'\n    def set$Name(self, value):\n        """\n        Sets the value of :py:attr:`$name`.\n        """\n        return self._set($name=value)\n\n    def get$Name(self):\n        """\n        Gets the value of $name or its default value.\n        """\n        return self.getOrDefault(self.$name)\'\'\'', 'Name', '=', 'name', '[', '0', ']', '.', 'upper', '(', ')', '+', 'name', '[', '1', ':', ']', 'return', 'template', '.', 'replace', '(', '"$name"', ',', 'name', ')', '.', 'replace', '(', '"$Name"', ',', 'Name', ')', '.', 'replace', '(', '"$doc"', ',', 'doc', ')', '.', 'replace', '(', '"$defaultValueStr"', ',', 'str', '(', 'defaultValueStr', ')', ')']
Docstring: Generates Python code for a shared param class.    :param name: param name    :param doc: param doc    :param defaultValueStr: string representation of the default value    :return: code string
*******__*******
Code:def train(self, rdd, k=4, maxIterations=20, minDivisibleClusterSize=1.0, seed=-1888008604):\n        """\n        Runs the bisecting k-means algorithm return the model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          The desired number of leaf clusters. The actual number could\n          be smaller if there are no divisible leaf clusters.\n          (default: 4)\n        :param maxIterations:\n          Maximum number of iterations allowed to split clusters.\n          (default: 20)\n        :param minDivisibleClusterSize:\n          Minimum number of points (if >= 1.0) or the minimum proportion\n          of points (if < 1.0) of a divisible cluster.\n          (default: 1)\n        :param seed:\n          Random seed value for cluster initialization.\n          (default: -1888008604 from classOf[BisectingKMeans].getName.##)\n        """\n        java_model = callMLlibFunc(\n            "trainBisectingKMeans", rdd.map(_convert_to_vector),\n            k, maxIterations, minDivisibleClusterSize, seed)\n        return BisectingKMeansModel(java_model)
Language: python
Code Tokens: ['def', 'train', '(', 'self', ',', 'rdd', ',', 'k', '=', '4', ',', 'maxIterations', '=', '20', ',', 'minDivisibleClusterSize', '=', '1.0', ',', 'seed', '=', '-', '1888008604', ')', ':', 'java_model', '=', 'callMLlibFunc', '(', '"trainBisectingKMeans"', ',', 'rdd', '.', 'map', '(', '_convert_to_vector', ')', ',', 'k', ',', 'maxIterations', ',', 'minDivisibleClusterSize', ',', 'seed', ')', 'return', 'BisectingKMeansModel', '(', 'java_model', ')']
Docstring: Runs the bisecting k-means algorithm return the model.        :param rdd:          Training points as an `RDD` of `Vector` or convertible          sequence types.        :param k:          The desired number of leaf clusters. The actual number could          be smaller if there are no divisible leaf clusters.          (default: 4)        :param maxIterations:          Maximum number of iterations allowed to split clusters.          (default: 20)        :param minDivisibleClusterSize:          Minimum number of points (if >= 1.0) or the minimum proportion          of points (if < 1.0) of a divisible cluster.          (default: 1)        :param seed:          Random seed value for cluster initialization.          (default: -1888008604 from classOf[BisectingKMeans].getName.##)
*******__*******
Code:def train(cls, rdd, k, maxIterations=100, runs=1, initializationMode="k-means||",\n              seed=None, initializationSteps=2, epsilon=1e-4, initialModel=None):\n        """\n        Train a k-means clustering model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          Number of clusters to create.\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 100)\n        :param runs:\n          This param has no effect since Spark 2.0.0.\n        :param initializationMode:\n          The initialization algorithm. This can be either "random" or\n          "k-means||".\n          (default: "k-means||")\n        :param seed:\n          Random seed value for cluster initialization. Set as None to\n          generate seed based on system time.\n          (default: None)\n        :param initializationSteps:\n          Number of steps for the k-means|| initialization mode.\n          This is an advanced setting -- the default of 2 is almost\n          always enough.\n          (default: 2)\n        :param epsilon:\n          Distance threshold within which a center will be considered to\n          have converged. If all centers move less than this Euclidean\n          distance, iterations are stopped.\n          (default: 1e-4)\n        :param initialModel:\n          Initial cluster centers can be provided as a KMeansModel object\n          rather than using the random or k-means|| initializationModel.\n          (default: None)\n        """\n        if runs != 1:\n            warnings.warn("The param `runs` has no effect since Spark 2.0.0.")\n        clusterInitialModel = []\n        if initialModel is not None:\n            if not isinstance(initialModel, KMeansModel):\n                raise Exception("initialModel is of "+str(type(initialModel))+". It needs "\n                                "to be of <type 'KMeansModel'>")\n            clusterInitialModel = [_convert_to_vector(c) for c in initialModel.clusterCenters]\n        model = callMLlibFunc("trainKMeansModel", rdd.map(_convert_to_vector), k, maxIterations,\n                              runs, initializationMode, seed, initializationSteps, epsilon,\n                              clusterInitialModel)\n        centers = callJavaFunc(rdd.context, model.clusterCenters)\n        return KMeansModel([c.toArray() for c in centers])
Language: python
Code Tokens: ['def', 'train', '(', 'cls', ',', 'rdd', ',', 'k', ',', 'maxIterations', '=', '100', ',', 'runs', '=', '1', ',', 'initializationMode', '=', '"k-means||"', ',', 'seed', '=', 'None', ',', 'initializationSteps', '=', '2', ',', 'epsilon', '=', '1e-4', ',', 'initialModel', '=', 'None', ')', ':', 'if', 'runs', '!=', '1', ':', 'warnings', '.', 'warn', '(', '"The param `runs` has no effect since Spark 2.0.0."', ')', 'clusterInitialModel', '=', '[', ']', 'if', 'initialModel', 'is', 'not', 'None', ':', 'if', 'not', 'isinstance', '(', 'initialModel', ',', 'KMeansModel', ')', ':', 'raise', 'Exception', '(', '"initialModel is of "', '+', 'str', '(', 'type', '(', 'initialModel', ')', ')', '+', '". It needs "', '"to be of <type \'KMeansModel\'>"', ')', 'clusterInitialModel', '=', '[', '_convert_to_vector', '(', 'c', ')', 'for', 'c', 'in', 'initialModel', '.', 'clusterCenters', ']', 'model', '=', 'callMLlibFunc', '(', '"trainKMeansModel"', ',', 'rdd', '.', 'map', '(', '_convert_to_vector', ')', ',', 'k', ',', 'maxIterations', ',', 'runs', ',', 'initializationMode', ',', 'seed', ',', 'initializationSteps', ',', 'epsilon', ',', 'clusterInitialModel', ')', 'centers', '=', 'callJavaFunc', '(', 'rdd', '.', 'context', ',', 'model', '.', 'clusterCenters', ')', 'return', 'KMeansModel', '(', '[', 'c', '.', 'toArray', '(', ')', 'for', 'c', 'in', 'centers', ']', ')']
Docstring: Train a k-means clustering model.        :param rdd:          Training points as an `RDD` of `Vector` or convertible          sequence types.        :param k:          Number of clusters to create.        :param maxIterations:          Maximum number of iterations allowed.          (default: 100)        :param runs:          This param has no effect since Spark 2.0.0.        :param initializationMode:          The initialization algorithm. This can be either "random" or          "k-means||".          (default: "k-means||")        :param seed:          Random seed value for cluster initialization. Set as None to          generate seed based on system time.          (default: None)        :param initializationSteps:          Number of steps for the k-means|| initialization mode.          This is an advanced setting -- the default of 2 is almost          always enough.          (default: 2)        :param epsilon:          Distance threshold within which a center will be considered to          have converged. If all centers move less than this Euclidean          distance, iterations are stopped.          (default: 1e-4)        :param initialModel:          Initial cluster centers can be provided as a KMeansModel object          rather than using the random or k-means|| initializationModel.          (default: None)
*******__*******
Code:def train(cls, rdd, k, convergenceTol=1e-3, maxIterations=100, seed=None, initialModel=None):\n        """\n        Train a Gaussian Mixture clustering model.\n\n        :param rdd:\n          Training points as an `RDD` of `Vector` or convertible\n          sequence types.\n        :param k:\n          Number of independent Gaussians in the mixture model.\n        :param convergenceTol:\n          Maximum change in log-likelihood at which convergence is\n          considered to have occurred.\n          (default: 1e-3)\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 100)\n        :param seed:\n          Random seed for initial Gaussian distribution. Set as None to\n          generate seed based on system time.\n          (default: None)\n        :param initialModel:\n          Initial GMM starting point, bypassing the random\n          initialization.\n          (default: None)\n        """\n        initialModelWeights = None\n        initialModelMu = None\n        initialModelSigma = None\n        if initialModel is not None:\n            if initialModel.k != k:\n                raise Exception("Mismatched cluster count, initialModel.k = %s, however k = %s"\n                                % (initialModel.k, k))\n            initialModelWeights = list(initialModel.weights)\n            initialModelMu = [initialModel.gaussians[i].mu for i in range(initialModel.k)]\n            initialModelSigma = [initialModel.gaussians[i].sigma for i in range(initialModel.k)]\n        java_model = callMLlibFunc("trainGaussianMixtureModel", rdd.map(_convert_to_vector),\n                                   k, convergenceTol, maxIterations, seed,\n                                   initialModelWeights, initialModelMu, initialModelSigma)\n        return GaussianMixtureModel(java_model)
Language: python
Code Tokens: ['def', 'train', '(', 'cls', ',', 'rdd', ',', 'k', ',', 'convergenceTol', '=', '1e-3', ',', 'maxIterations', '=', '100', ',', 'seed', '=', 'None', ',', 'initialModel', '=', 'None', ')', ':', 'initialModelWeights', '=', 'None', 'initialModelMu', '=', 'None', 'initialModelSigma', '=', 'None', 'if', 'initialModel', 'is', 'not', 'None', ':', 'if', 'initialModel', '.', 'k', '!=', 'k', ':', 'raise', 'Exception', '(', '"Mismatched cluster count, initialModel.k = %s, however k = %s"', '%', '(', 'initialModel', '.', 'k', ',', 'k', ')', ')', 'initialModelWeights', '=', 'list', '(', 'initialModel', '.', 'weights', ')', 'initialModelMu', '=', '[', 'initialModel', '.', 'gaussians', '[', 'i', ']', '.', 'mu', 'for', 'i', 'in', 'range', '(', 'initialModel', '.', 'k', ')', ']', 'initialModelSigma', '=', '[', 'initialModel', '.', 'gaussians', '[', 'i', ']', '.', 'sigma', 'for', 'i', 'in', 'range', '(', 'initialModel', '.', 'k', ')', ']', 'java_model', '=', 'callMLlibFunc', '(', '"trainGaussianMixtureModel"', ',', 'rdd', '.', 'map', '(', '_convert_to_vector', ')', ',', 'k', ',', 'convergenceTol', ',', 'maxIterations', ',', 'seed', ',', 'initialModelWeights', ',', 'initialModelMu', ',', 'initialModelSigma', ')', 'return', 'GaussianMixtureModel', '(', 'java_model', ')']
Docstring: Train a Gaussian Mixture clustering model.        :param rdd:          Training points as an `RDD` of `Vector` or convertible          sequence types.        :param k:          Number of independent Gaussians in the mixture model.        :param convergenceTol:          Maximum change in log-likelihood at which convergence is          considered to have occurred.          (default: 1e-3)        :param maxIterations:          Maximum number of iterations allowed.          (default: 100)        :param seed:          Random seed for initial Gaussian distribution. Set as None to          generate seed based on system time.          (default: None)        :param initialModel:          Initial GMM starting point, bypassing the random          initialization.          (default: None)
*******__*******
Code:def load(cls, sc, path):\n        """\n        Load a model from the given path.\n        """\n        model = cls._load_java(sc, path)\n        wrapper =\\n            sc._jvm.org.apache.spark.mllib.api.python.PowerIterationClusteringModelWrapper(model)\n        return PowerIterationClusteringModel(wrapper)
Language: python
Code Tokens: ['def', 'load', '(', 'cls', ',', 'sc', ',', 'path', ')', ':', 'model', '=', 'cls', '.', '_load_java', '(', 'sc', ',', 'path', ')', 'wrapper', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'api', '.', 'python', '.', 'PowerIterationClusteringModelWrapper', '(', 'model', ')', 'return', 'PowerIterationClusteringModel', '(', 'wrapper', ')']
Docstring: Load a model from the given path.
*******__*******
Code:def train(cls, rdd, k, maxIterations=100, initMode="random"):\n        r"""\n        :param rdd:\n          An RDD of (i, j, s\ :sub:`ij`\) tuples representing the\n          affinity matrix, which is the matrix A in the PIC paper.  The\n          similarity s\ :sub:`ij`\ must be nonnegative.  This is a symmetric\n          matrix and hence s\ :sub:`ij`\ = s\ :sub:`ji`\  For any (i, j) with\n          nonzero similarity, there should be either (i, j, s\ :sub:`ij`\) or\n          (j, i, s\ :sub:`ji`\) in the input.  Tuples with i = j are ignored,\n          because it is assumed s\ :sub:`ij`\ = 0.0.\n        :param k:\n          Number of clusters.\n        :param maxIterations:\n          Maximum number of iterations of the PIC algorithm.\n          (default: 100)\n        :param initMode:\n          Initialization mode. This can be either "random" to use\n          a random vector as vertex properties, or "degree" to use\n          normalized sum similarities.\n          (default: "random")\n        """\n        model = callMLlibFunc("trainPowerIterationClusteringModel",\n                              rdd.map(_convert_to_vector), int(k), int(maxIterations), initMode)\n        return PowerIterationClusteringModel(model)
Language: python
Code Tokens: ['def', 'train', '(', 'cls', ',', 'rdd', ',', 'k', ',', 'maxIterations', '=', '100', ',', 'initMode', '=', '"random"', ')', ':', 'model', '=', 'callMLlibFunc', '(', '"trainPowerIterationClusteringModel"', ',', 'rdd', '.', 'map', '(', '_convert_to_vector', ')', ',', 'int', '(', 'k', ')', ',', 'int', '(', 'maxIterations', ')', ',', 'initMode', ')', 'return', 'PowerIterationClusteringModel', '(', 'model', ')']
Docstring: r"""        :param rdd:          An RDD of (i, j, s\ :sub:`ij`\) tuples representing the          affinity matrix, which is the matrix A in the PIC paper.  The          similarity s\ :sub:`ij`\ must be nonnegative.  This is a symmetric          matrix and hence s\ :sub:`ij`\ = s\ :sub:`ji`\  For any (i, j) with          nonzero similarity, there should be either (i, j, s\ :sub:`ij`\) or          (j, i, s\ :sub:`ji`\) in the input.  Tuples with i = j are ignored,          because it is assumed s\ :sub:`ij`\ = 0.0.        :param k:          Number of clusters.        :param maxIterations:          Maximum number of iterations of the PIC algorithm.          (default: 100)        :param initMode:          Initialization mode. This can be either "random" to use          a random vector as vertex properties, or "degree" to use          normalized sum similarities.          (default: "random")
*******__*******
Code:def update(self, data, decayFactor, timeUnit):\n        """Update the centroids, according to data\n\n        :param data:\n          RDD with new data for the model update.\n        :param decayFactor:\n          Forgetfulness of the previous centroids.\n        :param timeUnit:\n          Can be "batches" or "points". If points, then the decay factor\n          is raised to the power of number of new points and if batches,\n          then decay factor will be used as is.\n        """\n        if not isinstance(data, RDD):\n            raise TypeError("Data should be of an RDD, got %s." % type(data))\n        data = data.map(_convert_to_vector)\n        decayFactor = float(decayFactor)\n        if timeUnit not in ["batches", "points"]:\n            raise ValueError(\n                "timeUnit should be 'batches' or 'points', got %s." % timeUnit)\n        vectorCenters = [_convert_to_vector(center) for center in self.centers]\n        updatedModel = callMLlibFunc(\n            "updateStreamingKMeansModel", vectorCenters, self._clusterWeights,\n            data, decayFactor, timeUnit)\n        self.centers = array(updatedModel[0])\n        self._clusterWeights = list(updatedModel[1])\n        return self
Language: python
Code Tokens: ['def', 'update', '(', 'self', ',', 'data', ',', 'decayFactor', ',', 'timeUnit', ')', ':', 'if', 'not', 'isinstance', '(', 'data', ',', 'RDD', ')', ':', 'raise', 'TypeError', '(', '"Data should be of an RDD, got %s."', '%', 'type', '(', 'data', ')', ')', 'data', '=', 'data', '.', 'map', '(', '_convert_to_vector', ')', 'decayFactor', '=', 'float', '(', 'decayFactor', ')', 'if', 'timeUnit', 'not', 'in', '[', '"batches"', ',', '"points"', ']', ':', 'raise', 'ValueError', '(', '"timeUnit should be \'batches\' or \'points\', got %s."', '%', 'timeUnit', ')', 'vectorCenters', '=', '[', '_convert_to_vector', '(', 'center', ')', 'for', 'center', 'in', 'self', '.', 'centers', ']', 'updatedModel', '=', 'callMLlibFunc', '(', '"updateStreamingKMeansModel"', ',', 'vectorCenters', ',', 'self', '.', '_clusterWeights', ',', 'data', ',', 'decayFactor', ',', 'timeUnit', ')', 'self', '.', 'centers', '=', 'array', '(', 'updatedModel', '[', '0', ']', ')', 'self', '.', '_clusterWeights', '=', 'list', '(', 'updatedModel', '[', '1', ']', ')', 'return', 'self']
Docstring: Update the centroids, according to data        :param data:          RDD with new data for the model update.        :param decayFactor:          Forgetfulness of the previous centroids.        :param timeUnit:          Can be "batches" or "points". If points, then the decay factor          is raised to the power of number of new points and if batches,          then decay factor will be used as is.
*******__*******
Code:def setHalfLife(self, halfLife, timeUnit):\n        """\n        Set number of batches after which the centroids of that\n        particular batch has half the weightage.\n        """\n        self._timeUnit = timeUnit\n        self._decayFactor = exp(log(0.5) / halfLife)\n        return self
Language: python
Code Tokens: ['def', 'setHalfLife', '(', 'self', ',', 'halfLife', ',', 'timeUnit', ')', ':', 'self', '.', '_timeUnit', '=', 'timeUnit', 'self', '.', '_decayFactor', '=', 'exp', '(', 'log', '(', '0.5', ')', '/', 'halfLife', ')', 'return', 'self']
Docstring: Set number of batches after which the centroids of that        particular batch has half the weightage.
*******__*******
Code:def setInitialCenters(self, centers, weights):\n        """\n        Set initial centers. Should be set before calling trainOn.\n        """\n        self._model = StreamingKMeansModel(centers, weights)\n        return self
Language: python
Code Tokens: ['def', 'setInitialCenters', '(', 'self', ',', 'centers', ',', 'weights', ')', ':', 'self', '.', '_model', '=', 'StreamingKMeansModel', '(', 'centers', ',', 'weights', ')', 'return', 'self']
Docstring: Set initial centers. Should be set before calling trainOn.
*******__*******
Code:def setRandomCenters(self, dim, weight, seed):\n        """\n        Set the initial centres to be random samples from\n        a gaussian population with constant weights.\n        """\n        rng = random.RandomState(seed)\n        clusterCenters = rng.randn(self._k, dim)\n        clusterWeights = tile(weight, self._k)\n        self._model = StreamingKMeansModel(clusterCenters, clusterWeights)\n        return self
Language: python
Code Tokens: ['def', 'setRandomCenters', '(', 'self', ',', 'dim', ',', 'weight', ',', 'seed', ')', ':', 'rng', '=', 'random', '.', 'RandomState', '(', 'seed', ')', 'clusterCenters', '=', 'rng', '.', 'randn', '(', 'self', '.', '_k', ',', 'dim', ')', 'clusterWeights', '=', 'tile', '(', 'weight', ',', 'self', '.', '_k', ')', 'self', '.', '_model', '=', 'StreamingKMeansModel', '(', 'clusterCenters', ',', 'clusterWeights', ')', 'return', 'self']
Docstring: Set the initial centres to be random samples from        a gaussian population with constant weights.
*******__*******
Code:def trainOn(self, dstream):\n        """Train the model on the incoming dstream."""\n        self._validate(dstream)\n\n        def update(rdd):\n            self._model.update(rdd, self._decayFactor, self._timeUnit)\n\n        dstream.foreachRDD(update)
Language: python
Code Tokens: ['def', 'trainOn', '(', 'self', ',', 'dstream', ')', ':', 'self', '.', '_validate', '(', 'dstream', ')', 'def', 'update', '(', 'rdd', ')', ':', 'self', '.', '_model', '.', 'update', '(', 'rdd', ',', 'self', '.', '_decayFactor', ',', 'self', '.', '_timeUnit', ')', 'dstream', '.', 'foreachRDD', '(', 'update', ')']
Docstring: Train the model on the incoming dstream.
*******__*******
Code:def predictOn(self, dstream):\n        """\n        Make predictions on a dstream.\n        Returns a transformed dstream object\n        """\n        self._validate(dstream)\n        return dstream.map(lambda x: self._model.predict(x))
Language: python
Code Tokens: ['def', 'predictOn', '(', 'self', ',', 'dstream', ')', ':', 'self', '.', '_validate', '(', 'dstream', ')', 'return', 'dstream', '.', 'map', '(', 'lambda', 'x', ':', 'self', '.', '_model', '.', 'predict', '(', 'x', ')', ')']
Docstring: Make predictions on a dstream.        Returns a transformed dstream object
*******__*******
Code:def predictOnValues(self, dstream):\n        """\n        Make predictions on a keyed dstream.\n        Returns a transformed dstream object.\n        """\n        self._validate(dstream)\n        return dstream.mapValues(lambda x: self._model.predict(x))
Language: python
Code Tokens: ['def', 'predictOnValues', '(', 'self', ',', 'dstream', ')', ':', 'self', '.', '_validate', '(', 'dstream', ')', 'return', 'dstream', '.', 'mapValues', '(', 'lambda', 'x', ':', 'self', '.', '_model', '.', 'predict', '(', 'x', ')', ')']
Docstring: Make predictions on a keyed dstream.        Returns a transformed dstream object.
*******__*******
Code:def describeTopics(self, maxTermsPerTopic=None):\n        """Return the topics described by weighted terms.\n\n        WARNING: If vocabSize and k are large, this can return a large object!\n\n        :param maxTermsPerTopic:\n          Maximum number of terms to collect for each topic.\n          (default: vocabulary size)\n        :return:\n          Array over topics. Each topic is represented as a pair of\n          matching arrays: (term indices, term weights in topic).\n          Each topic's terms are sorted in order of decreasing weight.\n        """\n        if maxTermsPerTopic is None:\n            topics = self.call("describeTopics")\n        else:\n            topics = self.call("describeTopics", maxTermsPerTopic)\n        return topics
Language: python
Code Tokens: ['def', 'describeTopics', '(', 'self', ',', 'maxTermsPerTopic', '=', 'None', ')', ':', 'if', 'maxTermsPerTopic', 'is', 'None', ':', 'topics', '=', 'self', '.', 'call', '(', '"describeTopics"', ')', 'else', ':', 'topics', '=', 'self', '.', 'call', '(', '"describeTopics"', ',', 'maxTermsPerTopic', ')', 'return', 'topics']
Docstring: Return the topics described by weighted terms.        WARNING: If vocabSize and k are large, this can return a large object!        :param maxTermsPerTopic:          Maximum number of terms to collect for each topic.          (default: vocabulary size)        :return:          Array over topics. Each topic is represented as a pair of          matching arrays: (term indices, term weights in topic).          Each topic's terms are sorted in order of decreasing weight.
*******__*******
Code:def load(cls, sc, path):\n        """Load the LDAModel from disk.\n\n        :param sc:\n          SparkContext.\n        :param path:\n          Path to where the model is stored.\n        """\n        if not isinstance(sc, SparkContext):\n            raise TypeError("sc should be a SparkContext, got type %s" % type(sc))\n        if not isinstance(path, basestring):\n            raise TypeError("path should be a basestring, got type %s" % type(path))\n        model = callMLlibFunc("loadLDAModel", sc, path)\n        return LDAModel(model)
Language: python
Code Tokens: ['def', 'load', '(', 'cls', ',', 'sc', ',', 'path', ')', ':', 'if', 'not', 'isinstance', '(', 'sc', ',', 'SparkContext', ')', ':', 'raise', 'TypeError', '(', '"sc should be a SparkContext, got type %s"', '%', 'type', '(', 'sc', ')', ')', 'if', 'not', 'isinstance', '(', 'path', ',', 'basestring', ')', ':', 'raise', 'TypeError', '(', '"path should be a basestring, got type %s"', '%', 'type', '(', 'path', ')', ')', 'model', '=', 'callMLlibFunc', '(', '"loadLDAModel"', ',', 'sc', ',', 'path', ')', 'return', 'LDAModel', '(', 'model', ')']
Docstring: Load the LDAModel from disk.        :param sc:          SparkContext.        :param path:          Path to where the model is stored.
*******__*******
Code:def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,\n              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer="em"):\n        """Train a LDA model.\n\n        :param rdd:\n          RDD of documents, which are tuples of document IDs and term\n          (word) count vectors. The term count vectors are "bags of\n          words" with a fixed-size vocabulary (where the vocabulary size\n          is the length of the vector). Document IDs must be unique\n          and >= 0.\n        :param k:\n          Number of topics to infer, i.e., the number of soft cluster\n          centers.\n          (default: 10)\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 20)\n        :param docConcentration:\n          Concentration parameter (commonly named "alpha") for the prior\n          placed on documents' distributions over topics ("theta").\n          (default: -1.0)\n        :param topicConcentration:\n          Concentration parameter (commonly named "beta" or "eta") for\n          the prior placed on topics' distributions over terms.\n          (default: -1.0)\n        :param seed:\n          Random seed for cluster initialization. Set as None to generate\n          seed based on system time.\n          (default: None)\n        :param checkpointInterval:\n          Period (in iterations) between checkpoints.\n          (default: 10)\n        :param optimizer:\n          LDAOptimizer used to perform the actual calculation. Currently\n          "em", "online" are supported.\n          (default: "em")\n        """\n        model = callMLlibFunc("trainLDAModel", rdd, k, maxIterations,\n                              docConcentration, topicConcentration, seed,\n                              checkpointInterval, optimizer)\n        return LDAModel(model)
Language: python
Code Tokens: ['def', 'train', '(', 'cls', ',', 'rdd', ',', 'k', '=', '10', ',', 'maxIterations', '=', '20', ',', 'docConcentration', '=', '-', '1.0', ',', 'topicConcentration', '=', '-', '1.0', ',', 'seed', '=', 'None', ',', 'checkpointInterval', '=', '10', ',', 'optimizer', '=', '"em"', ')', ':', 'model', '=', 'callMLlibFunc', '(', '"trainLDAModel"', ',', 'rdd', ',', 'k', ',', 'maxIterations', ',', 'docConcentration', ',', 'topicConcentration', ',', 'seed', ',', 'checkpointInterval', ',', 'optimizer', ')', 'return', 'LDAModel', '(', 'model', ')']
Docstring: Train a LDA model.        :param rdd:          RDD of documents, which are tuples of document IDs and term          (word) count vectors. The term count vectors are "bags of          words" with a fixed-size vocabulary (where the vocabulary size          is the length of the vector). Document IDs must be unique          and >= 0.        :param k:          Number of topics to infer, i.e., the number of soft cluster          centers.          (default: 10)        :param maxIterations:          Maximum number of iterations allowed.          (default: 20)        :param docConcentration:          Concentration parameter (commonly named "alpha") for the prior          placed on documents' distributions over topics ("theta").          (default: -1.0)        :param topicConcentration:          Concentration parameter (commonly named "beta" or "eta") for          the prior placed on topics' distributions over terms.          (default: -1.0)        :param seed:          Random seed for cluster initialization. Set as None to generate          seed based on system time.          (default: None)        :param checkpointInterval:          Period (in iterations) between checkpoints.          (default: 10)        :param optimizer:          LDAOptimizer used to perform the actual calculation. Currently          "em", "online" are supported.          (default: "em")
*******__*******
Code:def _to_java_object_rdd(rdd):\n    """ Return a JavaRDD of Object by unpickling\n\n    It will convert each Python object into Java object by Pyrolite, whenever the\n    RDD is serialized in batch or not.\n    """\n    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)
Language: python
Code Tokens: ['def', '_to_java_object_rdd', '(', 'rdd', ')', ':', 'rdd', '=', 'rdd', '.', '_reserialize', '(', 'AutoBatchedSerializer', '(', 'PickleSerializer', '(', ')', ')', ')', 'return', 'rdd', '.', 'ctx', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'api', '.', 'python', '.', 'SerDe', '.', 'pythonToJava', '(', 'rdd', '.', '_jrdd', ',', 'True', ')']
Docstring: Return a JavaRDD of Object by unpickling    It will convert each Python object into Java object by Pyrolite, whenever the    RDD is serialized in batch or not.
*******__*******
Code:def _py2java(sc, obj):\n    """ Convert Python object into Java """\n    if isinstance(obj, RDD):\n        obj = _to_java_object_rdd(obj)\n    elif isinstance(obj, DataFrame):\n        obj = obj._jdf\n    elif isinstance(obj, SparkContext):\n        obj = obj._jsc\n    elif isinstance(obj, list):\n        obj = [_py2java(sc, x) for x in obj]\n    elif isinstance(obj, JavaObject):\n        pass\n    elif isinstance(obj, (int, long, float, bool, bytes, unicode)):\n        pass\n    else:\n        data = bytearray(PickleSerializer().dumps(obj))\n        obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)\n    return obj
Language: python
Code Tokens: ['def', '_py2java', '(', 'sc', ',', 'obj', ')', ':', 'if', 'isinstance', '(', 'obj', ',', 'RDD', ')', ':', 'obj', '=', '_to_java_object_rdd', '(', 'obj', ')', 'elif', 'isinstance', '(', 'obj', ',', 'DataFrame', ')', ':', 'obj', '=', 'obj', '.', '_jdf', 'elif', 'isinstance', '(', 'obj', ',', 'SparkContext', ')', ':', 'obj', '=', 'obj', '.', '_jsc', 'elif', 'isinstance', '(', 'obj', ',', 'list', ')', ':', 'obj', '=', '[', '_py2java', '(', 'sc', ',', 'x', ')', 'for', 'x', 'in', 'obj', ']', 'elif', 'isinstance', '(', 'obj', ',', 'JavaObject', ')', ':', 'pass', 'elif', 'isinstance', '(', 'obj', ',', '(', 'int', ',', 'long', ',', 'float', ',', 'bool', ',', 'bytes', ',', 'unicode', ')', ')', ':', 'pass', 'else', ':', 'data', '=', 'bytearray', '(', 'PickleSerializer', '(', ')', '.', 'dumps', '(', 'obj', ')', ')', 'obj', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'api', '.', 'python', '.', 'SerDe', '.', 'loads', '(', 'data', ')', 'return', 'obj']
Docstring: Convert Python object into Java
*******__*******
Code:def callJavaFunc(sc, func, *args):\n    """ Call Java Function """\n    args = [_py2java(sc, a) for a in args]\n    return _java2py(sc, func(*args))
Language: python
Code Tokens: ['def', 'callJavaFunc', '(', 'sc', ',', 'func', ',', '*', 'args', ')', ':', 'args', '=', '[', '_py2java', '(', 'sc', ',', 'a', ')', 'for', 'a', 'in', 'args', ']', 'return', '_java2py', '(', 'sc', ',', 'func', '(', '*', 'args', ')', ')']
Docstring: Call Java Function
*******__*******
Code:def callMLlibFunc(name, *args):\n    """ Call API in PythonMLLibAPI """\n    sc = SparkContext.getOrCreate()\n    api = getattr(sc._jvm.PythonMLLibAPI(), name)\n    return callJavaFunc(sc, api, *args)
Language: python
Code Tokens: ['def', 'callMLlibFunc', '(', 'name', ',', '*', 'args', ')', ':', 'sc', '=', 'SparkContext', '.', 'getOrCreate', '(', ')', 'api', '=', 'getattr', '(', 'sc', '.', '_jvm', '.', 'PythonMLLibAPI', '(', ')', ',', 'name', ')', 'return', 'callJavaFunc', '(', 'sc', ',', 'api', ',', '*', 'args', ')']
Docstring: Call API in PythonMLLibAPI
*******__*******
Code:def inherit_doc(cls):\n    """\n    A decorator that makes a class inherit documentation from its parents.\n    """\n    for name, func in vars(cls).items():\n        # only inherit docstring for public functions\n        if name.startswith("_"):\n            continue\n        if not func.__doc__:\n            for parent in cls.__bases__:\n                parent_func = getattr(parent, name, None)\n                if parent_func and getattr(parent_func, "__doc__", None):\n                    func.__doc__ = parent_func.__doc__\n                    break\n    return cls
Language: python
Code Tokens: ['def', 'inherit_doc', '(', 'cls', ')', ':', 'for', 'name', ',', 'func', 'in', 'vars', '(', 'cls', ')', '.', 'items', '(', ')', ':', '# only inherit docstring for public functions', 'if', 'name', '.', 'startswith', '(', '"_"', ')', ':', 'continue', 'if', 'not', 'func', '.', '__doc__', ':', 'for', 'parent', 'in', 'cls', '.', '__bases__', ':', 'parent_func', '=', 'getattr', '(', 'parent', ',', 'name', ',', 'None', ')', 'if', 'parent_func', 'and', 'getattr', '(', 'parent_func', ',', '"__doc__"', ',', 'None', ')', ':', 'func', '.', '__doc__', '=', 'parent_func', '.', '__doc__', 'break', 'return', 'cls']
Docstring: A decorator that makes a class inherit documentation from its parents.
*******__*******
Code:def call(self, name, *a):\n        """Call method of java_model"""\n        return callJavaFunc(self._sc, getattr(self._java_model, name), *a)
Language: python
Code Tokens: ['def', 'call', '(', 'self', ',', 'name', ',', '*', 'a', ')', ':', 'return', 'callJavaFunc', '(', 'self', '.', '_sc', ',', 'getattr', '(', 'self', '.', '_java_model', ',', 'name', ')', ',', '*', 'a', ')']
Docstring: Call method of java_model
*******__*******
Code:def count(self):\n        """\n        Return a new DStream in which each RDD has a single element\n        generated by counting each RDD of this DStream.\n        """\n        return self.mapPartitions(lambda i: [sum(1 for _ in i)]).reduce(operator.add)
Language: python
Code Tokens: ['def', 'count', '(', 'self', ')', ':', 'return', 'self', '.', 'mapPartitions', '(', 'lambda', 'i', ':', '[', 'sum', '(', '1', 'for', '_', 'in', 'i', ')', ']', ')', '.', 'reduce', '(', 'operator', '.', 'add', ')']
Docstring: Return a new DStream in which each RDD has a single element        generated by counting each RDD of this DStream.
*******__*******
Code:def filter(self, f):\n        """\n        Return a new DStream containing only the elements that satisfy predicate.\n        """\n        def func(iterator):\n            return filter(f, iterator)\n        return self.mapPartitions(func, True)
Language: python
Code Tokens: ['def', 'filter', '(', 'self', ',', 'f', ')', ':', 'def', 'func', '(', 'iterator', ')', ':', 'return', 'filter', '(', 'f', ',', 'iterator', ')', 'return', 'self', '.', 'mapPartitions', '(', 'func', ',', 'True', ')']
Docstring: Return a new DStream containing only the elements that satisfy predicate.
*******__*******
Code:def map(self, f, preservesPartitioning=False):\n        """\n        Return a new DStream by applying a function to each element of DStream.\n        """\n        def func(iterator):\n            return map(f, iterator)\n        return self.mapPartitions(func, preservesPartitioning)
Language: python
Code Tokens: ['def', 'map', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', 'iterator', ')', ':', 'return', 'map', '(', 'f', ',', 'iterator', ')', 'return', 'self', '.', 'mapPartitions', '(', 'func', ',', 'preservesPartitioning', ')']
Docstring: Return a new DStream by applying a function to each element of DStream.
*******__*******
Code:def mapPartitionsWithIndex(self, f, preservesPartitioning=False):\n        """\n        Return a new DStream in which each RDD is generated by applying\n        mapPartitionsWithIndex() to each RDDs of this DStream.\n        """\n        return self.transform(lambda rdd: rdd.mapPartitionsWithIndex(f, preservesPartitioning))
Language: python
Code Tokens: ['def', 'mapPartitionsWithIndex', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'return', 'self', '.', 'transform', '(', 'lambda', 'rdd', ':', 'rdd', '.', 'mapPartitionsWithIndex', '(', 'f', ',', 'preservesPartitioning', ')', ')']
Docstring: Return a new DStream in which each RDD is generated by applying        mapPartitionsWithIndex() to each RDDs of this DStream.
*******__*******
Code:def reduce(self, func):\n        """\n        Return a new DStream in which each RDD has a single element\n        generated by reducing each RDD of this DStream.\n        """\n        return self.map(lambda x: (None, x)).reduceByKey(func, 1).map(lambda x: x[1])
Language: python
Code Tokens: ['def', 'reduce', '(', 'self', ',', 'func', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'None', ',', 'x', ')', ')', '.', 'reduceByKey', '(', 'func', ',', '1', ')', '.', 'map', '(', 'lambda', 'x', ':', 'x', '[', '1', ']', ')']
Docstring: Return a new DStream in which each RDD has a single element        generated by reducing each RDD of this DStream.
*******__*******
Code:def reduceByKey(self, func, numPartitions=None):\n        """\n        Return a new DStream by applying reduceByKey to each RDD.\n        """\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.combineByKey(lambda x: x, func, func, numPartitions)
Language: python
Code Tokens: ['def', 'reduceByKey', '(', 'self', ',', 'func', ',', 'numPartitions', '=', 'None', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_sc', '.', 'defaultParallelism', 'return', 'self', '.', 'combineByKey', '(', 'lambda', 'x', ':', 'x', ',', 'func', ',', 'func', ',', 'numPartitions', ')']
Docstring: Return a new DStream by applying reduceByKey to each RDD.
*******__*******
Code:def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\n                     numPartitions=None):\n        """\n        Return a new DStream by applying combineByKey to each RDD.\n        """\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        def func(rdd):\n            return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)\n        return self.transform(func)
Language: python
Code Tokens: ['def', 'combineByKey', '(', 'self', ',', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ',', 'numPartitions', '=', 'None', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_sc', '.', 'defaultParallelism', 'def', 'func', '(', 'rdd', ')', ':', 'return', 'rdd', '.', 'combineByKey', '(', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ',', 'numPartitions', ')', 'return', 'self', '.', 'transform', '(', 'func', ')']
Docstring: Return a new DStream by applying combineByKey to each RDD.
*******__*******
Code:def partitionBy(self, numPartitions, partitionFunc=portable_hash):\n        """\n        Return a copy of the DStream in which each RDD are partitioned\n        using the specified partitioner.\n        """\n        return self.transform(lambda rdd: rdd.partitionBy(numPartitions, partitionFunc))
Language: python
Code Tokens: ['def', 'partitionBy', '(', 'self', ',', 'numPartitions', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'return', 'self', '.', 'transform', '(', 'lambda', 'rdd', ':', 'rdd', '.', 'partitionBy', '(', 'numPartitions', ',', 'partitionFunc', ')', ')']
Docstring: Return a copy of the DStream in which each RDD are partitioned        using the specified partitioner.
*******__*******
Code:def foreachRDD(self, func):\n        """\n        Apply a function to each RDD in this DStream.\n        """\n        if func.__code__.co_argcount == 1:\n            old_func = func\n            func = lambda t, rdd: old_func(rdd)\n        jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer)\n        api = self._ssc._jvm.PythonDStream\n        api.callForeachRDD(self._jdstream, jfunc)
Language: python
Code Tokens: ['def', 'foreachRDD', '(', 'self', ',', 'func', ')', ':', 'if', 'func', '.', '__code__', '.', 'co_argcount', '==', '1', ':', 'old_func', '=', 'func', 'func', '=', 'lambda', 't', ',', 'rdd', ':', 'old_func', '(', 'rdd', ')', 'jfunc', '=', 'TransformFunction', '(', 'self', '.', '_sc', ',', 'func', ',', 'self', '.', '_jrdd_deserializer', ')', 'api', '=', 'self', '.', '_ssc', '.', '_jvm', '.', 'PythonDStream', 'api', '.', 'callForeachRDD', '(', 'self', '.', '_jdstream', ',', 'jfunc', ')']
Docstring: Apply a function to each RDD in this DStream.
*******__*******
Code:def pprint(self, num=10):\n        """\n        Print the first num elements of each RDD generated in this DStream.\n\n        @param num: the number of elements from the first will be printed.\n        """\n        def takeAndPrint(time, rdd):\n            taken = rdd.take(num + 1)\n            print("-------------------------------------------")\n            print("Time: %s" % time)\n            print("-------------------------------------------")\n            for record in taken[:num]:\n                print(record)\n            if len(taken) > num:\n                print("...")\n            print("")\n\n        self.foreachRDD(takeAndPrint)
Language: python
Code Tokens: ['def', 'pprint', '(', 'self', ',', 'num', '=', '10', ')', ':', 'def', 'takeAndPrint', '(', 'time', ',', 'rdd', ')', ':', 'taken', '=', 'rdd', '.', 'take', '(', 'num', '+', '1', ')', 'print', '(', '"-------------------------------------------"', ')', 'print', '(', '"Time: %s"', '%', 'time', ')', 'print', '(', '"-------------------------------------------"', ')', 'for', 'record', 'in', 'taken', '[', ':', 'num', ']', ':', 'print', '(', 'record', ')', 'if', 'len', '(', 'taken', ')', '>', 'num', ':', 'print', '(', '"..."', ')', 'print', '(', '""', ')', 'self', '.', 'foreachRDD', '(', 'takeAndPrint', ')']
Docstring: Print the first num elements of each RDD generated in this DStream.        @param num: the number of elements from the first will be printed.
*******__*******
Code:def persist(self, storageLevel):\n        """\n        Persist the RDDs of this DStream with the given storage level\n        """\n        self.is_cached = True\n        javaStorageLevel = self._sc._getJavaStorageLevel(storageLevel)\n        self._jdstream.persist(javaStorageLevel)\n        return self
Language: python
Code Tokens: ['def', 'persist', '(', 'self', ',', 'storageLevel', ')', ':', 'self', '.', 'is_cached', '=', 'True', 'javaStorageLevel', '=', 'self', '.', '_sc', '.', '_getJavaStorageLevel', '(', 'storageLevel', ')', 'self', '.', '_jdstream', '.', 'persist', '(', 'javaStorageLevel', ')', 'return', 'self']
Docstring: Persist the RDDs of this DStream with the given storage level
*******__*******
Code:def checkpoint(self, interval):\n        """\n        Enable periodic checkpointing of RDDs of this DStream\n\n        @param interval: time in seconds, after each period of that, generated\n                         RDD will be checkpointed\n        """\n        self.is_checkpointed = True\n        self._jdstream.checkpoint(self._ssc._jduration(interval))\n        return self
Language: python
Code Tokens: ['def', 'checkpoint', '(', 'self', ',', 'interval', ')', ':', 'self', '.', 'is_checkpointed', '=', 'True', 'self', '.', '_jdstream', '.', 'checkpoint', '(', 'self', '.', '_ssc', '.', '_jduration', '(', 'interval', ')', ')', 'return', 'self']
Docstring: Enable periodic checkpointing of RDDs of this DStream        @param interval: time in seconds, after each period of that, generated                         RDD will be checkpointed
*******__*******
Code:def groupByKey(self, numPartitions=None):\n        """\n        Return a new DStream by applying groupByKey on each RDD.\n        """\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.transform(lambda rdd: rdd.groupByKey(numPartitions))
Language: python
Code Tokens: ['def', 'groupByKey', '(', 'self', ',', 'numPartitions', '=', 'None', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_sc', '.', 'defaultParallelism', 'return', 'self', '.', 'transform', '(', 'lambda', 'rdd', ':', 'rdd', '.', 'groupByKey', '(', 'numPartitions', ')', ')']
Docstring: Return a new DStream by applying groupByKey on each RDD.
*******__*******
Code:def countByValue(self):\n        """\n        Return a new DStream in which each RDD contains the counts of each\n        distinct value in each RDD of this DStream.\n        """\n        return self.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)
Language: python
Code Tokens: ['def', 'countByValue', '(', 'self', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', '1', ')', ')', '.', 'reduceByKey', '(', 'lambda', 'x', ',', 'y', ':', 'x', '+', 'y', ')']
Docstring: Return a new DStream in which each RDD contains the counts of each        distinct value in each RDD of this DStream.
*******__*******
Code:def saveAsTextFiles(self, prefix, suffix=None):\n        """\n        Save each RDD in this DStream as at text file, using string\n        representation of elements.\n        """\n        def saveAsTextFile(t, rdd):\n            path = rddToFileName(prefix, suffix, t)\n            try:\n                rdd.saveAsTextFile(path)\n            except Py4JJavaError as e:\n                # after recovered from checkpointing, the foreachRDD may\n                # be called twice\n                if 'FileAlreadyExistsException' not in str(e):\n                    raise\n        return self.foreachRDD(saveAsTextFile)
Language: python
Code Tokens: ['def', 'saveAsTextFiles', '(', 'self', ',', 'prefix', ',', 'suffix', '=', 'None', ')', ':', 'def', 'saveAsTextFile', '(', 't', ',', 'rdd', ')', ':', 'path', '=', 'rddToFileName', '(', 'prefix', ',', 'suffix', ',', 't', ')', 'try', ':', 'rdd', '.', 'saveAsTextFile', '(', 'path', ')', 'except', 'Py4JJavaError', 'as', 'e', ':', '# after recovered from checkpointing, the foreachRDD may', '# be called twice', 'if', "'FileAlreadyExistsException'", 'not', 'in', 'str', '(', 'e', ')', ':', 'raise', 'return', 'self', '.', 'foreachRDD', '(', 'saveAsTextFile', ')']
Docstring: Save each RDD in this DStream as at text file, using string        representation of elements.
*******__*******
Code:def transform(self, func):\n        """\n        Return a new DStream in which each RDD is generated by applying a function\n        on each RDD of this DStream.\n\n        `func` can have one argument of `rdd`, or have two arguments of\n        (`time`, `rdd`)\n        """\n        if func.__code__.co_argcount == 1:\n            oldfunc = func\n            func = lambda t, rdd: oldfunc(rdd)\n        assert func.__code__.co_argcount == 2, "func should take one or two arguments"\n        return TransformedDStream(self, func)
Language: python
Code Tokens: ['def', 'transform', '(', 'self', ',', 'func', ')', ':', 'if', 'func', '.', '__code__', '.', 'co_argcount', '==', '1', ':', 'oldfunc', '=', 'func', 'func', '=', 'lambda', 't', ',', 'rdd', ':', 'oldfunc', '(', 'rdd', ')', 'assert', 'func', '.', '__code__', '.', 'co_argcount', '==', '2', ',', '"func should take one or two arguments"', 'return', 'TransformedDStream', '(', 'self', ',', 'func', ')']
Docstring: Return a new DStream in which each RDD is generated by applying a function        on each RDD of this DStream.        `func` can have one argument of `rdd`, or have two arguments of        (`time`, `rdd`)
*******__*******
Code:def transformWith(self, func, other, keepSerializer=False):\n        """\n        Return a new DStream in which each RDD is generated by applying a function\n        on each RDD of this DStream and 'other' DStream.\n\n        `func` can have two arguments of (`rdd_a`, `rdd_b`) or have three\n        arguments of (`time`, `rdd_a`, `rdd_b`)\n        """\n        if func.__code__.co_argcount == 2:\n            oldfunc = func\n            func = lambda t, a, b: oldfunc(a, b)\n        assert func.__code__.co_argcount == 3, "func should take two or three arguments"\n        jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer, other._jrdd_deserializer)\n        dstream = self._sc._jvm.PythonTransformed2DStream(self._jdstream.dstream(),\n                                                          other._jdstream.dstream(), jfunc)\n        jrdd_serializer = self._jrdd_deserializer if keepSerializer else self._sc.serializer\n        return DStream(dstream.asJavaDStream(), self._ssc, jrdd_serializer)
Language: python
Code Tokens: ['def', 'transformWith', '(', 'self', ',', 'func', ',', 'other', ',', 'keepSerializer', '=', 'False', ')', ':', 'if', 'func', '.', '__code__', '.', 'co_argcount', '==', '2', ':', 'oldfunc', '=', 'func', 'func', '=', 'lambda', 't', ',', 'a', ',', 'b', ':', 'oldfunc', '(', 'a', ',', 'b', ')', 'assert', 'func', '.', '__code__', '.', 'co_argcount', '==', '3', ',', '"func should take two or three arguments"', 'jfunc', '=', 'TransformFunction', '(', 'self', '.', '_sc', ',', 'func', ',', 'self', '.', '_jrdd_deserializer', ',', 'other', '.', '_jrdd_deserializer', ')', 'dstream', '=', 'self', '.', '_sc', '.', '_jvm', '.', 'PythonTransformed2DStream', '(', 'self', '.', '_jdstream', '.', 'dstream', '(', ')', ',', 'other', '.', '_jdstream', '.', 'dstream', '(', ')', ',', 'jfunc', ')', 'jrdd_serializer', '=', 'self', '.', '_jrdd_deserializer', 'if', 'keepSerializer', 'else', 'self', '.', '_sc', '.', 'serializer', 'return', 'DStream', '(', 'dstream', '.', 'asJavaDStream', '(', ')', ',', 'self', '.', '_ssc', ',', 'jrdd_serializer', ')']
Docstring: Return a new DStream in which each RDD is generated by applying a function        on each RDD of this DStream and 'other' DStream.        `func` can have two arguments of (`rdd_a`, `rdd_b`) or have three        arguments of (`time`, `rdd_a`, `rdd_b`)
*******__*******
Code:def union(self, other):\n        """\n        Return a new DStream by unifying data of another DStream with this DStream.\n\n        @param other: Another DStream having the same interval (i.e., slideDuration)\n                     as this DStream.\n        """\n        if self._slideDuration != other._slideDuration:\n            raise ValueError("the two DStream should have same slide duration")\n        return self.transformWith(lambda a, b: a.union(b), other, True)
Language: python
Code Tokens: ['def', 'union', '(', 'self', ',', 'other', ')', ':', 'if', 'self', '.', '_slideDuration', '!=', 'other', '.', '_slideDuration', ':', 'raise', 'ValueError', '(', '"the two DStream should have same slide duration"', ')', 'return', 'self', '.', 'transformWith', '(', 'lambda', 'a', ',', 'b', ':', 'a', '.', 'union', '(', 'b', ')', ',', 'other', ',', 'True', ')']
Docstring: Return a new DStream by unifying data of another DStream with this DStream.        @param other: Another DStream having the same interval (i.e., slideDuration)                     as this DStream.
*******__*******
Code:def cogroup(self, other, numPartitions=None):\n        """\n        Return a new DStream by applying 'cogroup' between RDDs of this\n        DStream and `other` DStream.\n\n        Hash partitioning is used to generate the RDDs with `numPartitions` partitions.\n        """\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.transformWith(lambda a, b: a.cogroup(b, numPartitions), other)
Language: python
Code Tokens: ['def', 'cogroup', '(', 'self', ',', 'other', ',', 'numPartitions', '=', 'None', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_sc', '.', 'defaultParallelism', 'return', 'self', '.', 'transformWith', '(', 'lambda', 'a', ',', 'b', ':', 'a', '.', 'cogroup', '(', 'b', ',', 'numPartitions', ')', ',', 'other', ')']
Docstring: Return a new DStream by applying 'cogroup' between RDDs of this        DStream and `other` DStream.        Hash partitioning is used to generate the RDDs with `numPartitions` partitions.
*******__*******
Code:def _jtime(self, timestamp):\n        """ Convert datetime or unix_timestamp into Time\n        """\n        if isinstance(timestamp, datetime):\n            timestamp = time.mktime(timestamp.timetuple())\n        return self._sc._jvm.Time(long(timestamp * 1000))
Language: python
Code Tokens: ['def', '_jtime', '(', 'self', ',', 'timestamp', ')', ':', 'if', 'isinstance', '(', 'timestamp', ',', 'datetime', ')', ':', 'timestamp', '=', 'time', '.', 'mktime', '(', 'timestamp', '.', 'timetuple', '(', ')', ')', 'return', 'self', '.', '_sc', '.', '_jvm', '.', 'Time', '(', 'long', '(', 'timestamp', '*', '1000', ')', ')']
Docstring: Convert datetime or unix_timestamp into Time
*******__*******
Code:def slice(self, begin, end):\n        """\n        Return all the RDDs between 'begin' to 'end' (both included)\n\n        `begin`, `end` could be datetime.datetime() or unix_timestamp\n        """\n        jrdds = self._jdstream.slice(self._jtime(begin), self._jtime(end))\n        return [RDD(jrdd, self._sc, self._jrdd_deserializer) for jrdd in jrdds]
Language: python
Code Tokens: ['def', 'slice', '(', 'self', ',', 'begin', ',', 'end', ')', ':', 'jrdds', '=', 'self', '.', '_jdstream', '.', 'slice', '(', 'self', '.', '_jtime', '(', 'begin', ')', ',', 'self', '.', '_jtime', '(', 'end', ')', ')', 'return', '[', 'RDD', '(', 'jrdd', ',', 'self', '.', '_sc', ',', 'self', '.', '_jrdd_deserializer', ')', 'for', 'jrdd', 'in', 'jrdds', ']']
Docstring: Return all the RDDs between 'begin' to 'end' (both included)        `begin`, `end` could be datetime.datetime() or unix_timestamp
*******__*******
Code:def window(self, windowDuration, slideDuration=None):\n        """\n        Return a new DStream in which each RDD contains all the elements in seen in a\n        sliding window of time over this DStream.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batching interval\n        """\n        self._validate_window_param(windowDuration, slideDuration)\n        d = self._ssc._jduration(windowDuration)\n        if slideDuration is None:\n            return DStream(self._jdstream.window(d), self._ssc, self._jrdd_deserializer)\n        s = self._ssc._jduration(slideDuration)\n        return DStream(self._jdstream.window(d, s), self._ssc, self._jrdd_deserializer)
Language: python
Code Tokens: ['def', 'window', '(', 'self', ',', 'windowDuration', ',', 'slideDuration', '=', 'None', ')', ':', 'self', '.', '_validate_window_param', '(', 'windowDuration', ',', 'slideDuration', ')', 'd', '=', 'self', '.', '_ssc', '.', '_jduration', '(', 'windowDuration', ')', 'if', 'slideDuration', 'is', 'None', ':', 'return', 'DStream', '(', 'self', '.', '_jdstream', '.', 'window', '(', 'd', ')', ',', 'self', '.', '_ssc', ',', 'self', '.', '_jrdd_deserializer', ')', 's', '=', 'self', '.', '_ssc', '.', '_jduration', '(', 'slideDuration', ')', 'return', 'DStream', '(', 'self', '.', '_jdstream', '.', 'window', '(', 'd', ',', 's', ')', ',', 'self', '.', '_ssc', ',', 'self', '.', '_jrdd_deserializer', ')']
Docstring: Return a new DStream in which each RDD contains all the elements in seen in a        sliding window of time over this DStream.        @param windowDuration: width of the window; must be a multiple of this DStream's                              batching interval        @param slideDuration:  sliding interval of the window (i.e., the interval after which                              the new DStream will generate RDDs); must be a multiple of this                              DStream's batching interval
*******__*******
Code:def reduceByWindow(self, reduceFunc, invReduceFunc, windowDuration, slideDuration):\n        """\n        Return a new DStream in which each RDD has a single element generated by reducing all\n        elements in a sliding window over this DStream.\n\n        if `invReduceFunc` is not None, the reduction is done incrementally\n        using the old window's reduced value :\n\n        1. reduce the new values that entered the window (e.g., adding new counts)\n\n        2. "inverse reduce" the old values that left the window (e.g., subtracting old counts)\n        This is more efficient than `invReduceFunc` is None.\n\n        @param reduceFunc:     associative and commutative reduce function\n        @param invReduceFunc:  inverse reduce function of `reduceFunc`; such that for all y,\n                               and invertible x:\n                               `invReduceFunc(reduceFunc(x, y), x) = y`\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                               batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                               the new DStream will generate RDDs); must be a multiple of this\n                               DStream's batching interval\n        """\n        keyed = self.map(lambda x: (1, x))\n        reduced = keyed.reduceByKeyAndWindow(reduceFunc, invReduceFunc,\n                                             windowDuration, slideDuration, 1)\n        return reduced.map(lambda kv: kv[1])
Language: python
Code Tokens: ['def', 'reduceByWindow', '(', 'self', ',', 'reduceFunc', ',', 'invReduceFunc', ',', 'windowDuration', ',', 'slideDuration', ')', ':', 'keyed', '=', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', '1', ',', 'x', ')', ')', 'reduced', '=', 'keyed', '.', 'reduceByKeyAndWindow', '(', 'reduceFunc', ',', 'invReduceFunc', ',', 'windowDuration', ',', 'slideDuration', ',', '1', ')', 'return', 'reduced', '.', 'map', '(', 'lambda', 'kv', ':', 'kv', '[', '1', ']', ')']
Docstring: Return a new DStream in which each RDD has a single element generated by reducing all        elements in a sliding window over this DStream.        if `invReduceFunc` is not None, the reduction is done incrementally        using the old window's reduced value :        1. reduce the new values that entered the window (e.g., adding new counts)        2. "inverse reduce" the old values that left the window (e.g., subtracting old counts)        This is more efficient than `invReduceFunc` is None.        @param reduceFunc:     associative and commutative reduce function        @param invReduceFunc:  inverse reduce function of `reduceFunc`; such that for all y,                               and invertible x:                               `invReduceFunc(reduceFunc(x, y), x) = y`        @param windowDuration: width of the window; must be a multiple of this DStream's                               batching interval        @param slideDuration:  sliding interval of the window (i.e., the interval after which                               the new DStream will generate RDDs); must be a multiple of this                               DStream's batching interval
*******__*******
Code:def countByWindow(self, windowDuration, slideDuration):\n        """\n        Return a new DStream in which each RDD has a single element generated\n        by counting the number of elements in a window over this DStream.\n        windowDuration and slideDuration are as defined in the window() operation.\n\n        This is equivalent to window(windowDuration, slideDuration).count(),\n        but will be more efficient if window is large.\n        """\n        return self.map(lambda x: 1).reduceByWindow(operator.add, operator.sub,\n                                                    windowDuration, slideDuration)
Language: python
Code Tokens: ['def', 'countByWindow', '(', 'self', ',', 'windowDuration', ',', 'slideDuration', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '1', ')', '.', 'reduceByWindow', '(', 'operator', '.', 'add', ',', 'operator', '.', 'sub', ',', 'windowDuration', ',', 'slideDuration', ')']
Docstring: Return a new DStream in which each RDD has a single element generated        by counting the number of elements in a window over this DStream.        windowDuration and slideDuration are as defined in the window() operation.        This is equivalent to window(windowDuration, slideDuration).count(),        but will be more efficient if window is large.
*******__*******
Code:def countByValueAndWindow(self, windowDuration, slideDuration, numPartitions=None):\n        """\n        Return a new DStream in which each RDD contains the count of distinct elements in\n        RDDs in a sliding window over this DStream.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batching interval\n        @param numPartitions:  number of partitions of each RDD in the new DStream.\n        """\n        keyed = self.map(lambda x: (x, 1))\n        counted = keyed.reduceByKeyAndWindow(operator.add, operator.sub,\n                                             windowDuration, slideDuration, numPartitions)\n        return counted.filter(lambda kv: kv[1] > 0)
Language: python
Code Tokens: ['def', 'countByValueAndWindow', '(', 'self', ',', 'windowDuration', ',', 'slideDuration', ',', 'numPartitions', '=', 'None', ')', ':', 'keyed', '=', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', '1', ')', ')', 'counted', '=', 'keyed', '.', 'reduceByKeyAndWindow', '(', 'operator', '.', 'add', ',', 'operator', '.', 'sub', ',', 'windowDuration', ',', 'slideDuration', ',', 'numPartitions', ')', 'return', 'counted', '.', 'filter', '(', 'lambda', 'kv', ':', 'kv', '[', '1', ']', '>', '0', ')']
Docstring: Return a new DStream in which each RDD contains the count of distinct elements in        RDDs in a sliding window over this DStream.        @param windowDuration: width of the window; must be a multiple of this DStream's                              batching interval        @param slideDuration:  sliding interval of the window (i.e., the interval after which                              the new DStream will generate RDDs); must be a multiple of this                              DStream's batching interval        @param numPartitions:  number of partitions of each RDD in the new DStream.
*******__*******
Code:def groupByKeyAndWindow(self, windowDuration, slideDuration, numPartitions=None):\n        """\n        Return a new DStream by applying `groupByKey` over a sliding window.\n        Similar to `DStream.groupByKey()`, but applies it over a sliding window.\n\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batching interval\n        @param numPartitions:  Number of partitions of each RDD in the new DStream.\n        """\n        ls = self.mapValues(lambda x: [x])\n        grouped = ls.reduceByKeyAndWindow(lambda a, b: a.extend(b) or a, lambda a, b: a[len(b):],\n                                          windowDuration, slideDuration, numPartitions)\n        return grouped.mapValues(ResultIterable)
Language: python
Code Tokens: ['def', 'groupByKeyAndWindow', '(', 'self', ',', 'windowDuration', ',', 'slideDuration', ',', 'numPartitions', '=', 'None', ')', ':', 'ls', '=', 'self', '.', 'mapValues', '(', 'lambda', 'x', ':', '[', 'x', ']', ')', 'grouped', '=', 'ls', '.', 'reduceByKeyAndWindow', '(', 'lambda', 'a', ',', 'b', ':', 'a', '.', 'extend', '(', 'b', ')', 'or', 'a', ',', 'lambda', 'a', ',', 'b', ':', 'a', '[', 'len', '(', 'b', ')', ':', ']', ',', 'windowDuration', ',', 'slideDuration', ',', 'numPartitions', ')', 'return', 'grouped', '.', 'mapValues', '(', 'ResultIterable', ')']
Docstring: Return a new DStream by applying `groupByKey` over a sliding window.        Similar to `DStream.groupByKey()`, but applies it over a sliding window.        @param windowDuration: width of the window; must be a multiple of this DStream's                              batching interval        @param slideDuration:  sliding interval of the window (i.e., the interval after which                              the new DStream will generate RDDs); must be a multiple of this                              DStream's batching interval        @param numPartitions:  Number of partitions of each RDD in the new DStream.
*******__*******
Code:def reduceByKeyAndWindow(self, func, invFunc, windowDuration, slideDuration=None,\n                             numPartitions=None, filterFunc=None):\n        """\n        Return a new DStream by applying incremental `reduceByKey` over a sliding window.\n\n        The reduced value of over a new window is calculated using the old window's reduce value :\n         1. reduce the new values that entered the window (e.g., adding new counts)\n         2. "inverse reduce" the old values that left the window (e.g., subtracting old counts)\n\n        `invFunc` can be None, then it will reduce all the RDDs in window, could be slower\n        than having `invFunc`.\n\n        @param func:           associative and commutative reduce function\n        @param invFunc:        inverse function of `reduceFunc`\n        @param windowDuration: width of the window; must be a multiple of this DStream's\n                              batching interval\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\n                              the new DStream will generate RDDs); must be a multiple of this\n                              DStream's batching interval\n        @param numPartitions:  number of partitions of each RDD in the new DStream.\n        @param filterFunc:     function to filter expired key-value pairs;\n                              only pairs that satisfy the function are retained\n                              set this to null if you do not want to filter\n        """\n        self._validate_window_param(windowDuration, slideDuration)\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        reduced = self.reduceByKey(func, numPartitions)\n\n        if invFunc:\n            def reduceFunc(t, a, b):\n                b = b.reduceByKey(func, numPartitions)\n                r = a.union(b).reduceByKey(func, numPartitions) if a else b\n                if filterFunc:\n                    r = r.filter(filterFunc)\n                return r\n\n            def invReduceFunc(t, a, b):\n                b = b.reduceByKey(func, numPartitions)\n                joined = a.leftOuterJoin(b, numPartitions)\n                return joined.mapValues(lambda kv: invFunc(kv[0], kv[1])\n                                        if kv[1] is not None else kv[0])\n\n            jreduceFunc = TransformFunction(self._sc, reduceFunc, reduced._jrdd_deserializer)\n            jinvReduceFunc = TransformFunction(self._sc, invReduceFunc, reduced._jrdd_deserializer)\n            if slideDuration is None:\n                slideDuration = self._slideDuration\n            dstream = self._sc._jvm.PythonReducedWindowedDStream(\n                reduced._jdstream.dstream(),\n                jreduceFunc, jinvReduceFunc,\n                self._ssc._jduration(windowDuration),\n                self._ssc._jduration(slideDuration))\n            return DStream(dstream.asJavaDStream(), self._ssc, self._sc.serializer)\n        else:\n            return reduced.window(windowDuration, slideDuration).reduceByKey(func, numPartitions)
Language: python
Code Tokens: ['def', 'reduceByKeyAndWindow', '(', 'self', ',', 'func', ',', 'invFunc', ',', 'windowDuration', ',', 'slideDuration', '=', 'None', ',', 'numPartitions', '=', 'None', ',', 'filterFunc', '=', 'None', ')', ':', 'self', '.', '_validate_window_param', '(', 'windowDuration', ',', 'slideDuration', ')', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_sc', '.', 'defaultParallelism', 'reduced', '=', 'self', '.', 'reduceByKey', '(', 'func', ',', 'numPartitions', ')', 'if', 'invFunc', ':', 'def', 'reduceFunc', '(', 't', ',', 'a', ',', 'b', ')', ':', 'b', '=', 'b', '.', 'reduceByKey', '(', 'func', ',', 'numPartitions', ')', 'r', '=', 'a', '.', 'union', '(', 'b', ')', '.', 'reduceByKey', '(', 'func', ',', 'numPartitions', ')', 'if', 'a', 'else', 'b', 'if', 'filterFunc', ':', 'r', '=', 'r', '.', 'filter', '(', 'filterFunc', ')', 'return', 'r', 'def', 'invReduceFunc', '(', 't', ',', 'a', ',', 'b', ')', ':', 'b', '=', 'b', '.', 'reduceByKey', '(', 'func', ',', 'numPartitions', ')', 'joined', '=', 'a', '.', 'leftOuterJoin', '(', 'b', ',', 'numPartitions', ')', 'return', 'joined', '.', 'mapValues', '(', 'lambda', 'kv', ':', 'invFunc', '(', 'kv', '[', '0', ']', ',', 'kv', '[', '1', ']', ')', 'if', 'kv', '[', '1', ']', 'is', 'not', 'None', 'else', 'kv', '[', '0', ']', ')', 'jreduceFunc', '=', 'TransformFunction', '(', 'self', '.', '_sc', ',', 'reduceFunc', ',', 'reduced', '.', '_jrdd_deserializer', ')', 'jinvReduceFunc', '=', 'TransformFunction', '(', 'self', '.', '_sc', ',', 'invReduceFunc', ',', 'reduced', '.', '_jrdd_deserializer', ')', 'if', 'slideDuration', 'is', 'None', ':', 'slideDuration', '=', 'self', '.', '_slideDuration', 'dstream', '=', 'self', '.', '_sc', '.', '_jvm', '.', 'PythonReducedWindowedDStream', '(', 'reduced', '.', '_jdstream', '.', 'dstream', '(', ')', ',', 'jreduceFunc', ',', 'jinvReduceFunc', ',', 'self', '.', '_ssc', '.', '_jduration', '(', 'windowDuration', ')', ',', 'self', '.', '_ssc', '.', '_jduration', '(', 'slideDuration', ')', ')', 'return', 'DStream', '(', 'dstream', '.', 'asJavaDStream', '(', ')', ',', 'self', '.', '_ssc', ',', 'self', '.', '_sc', '.', 'serializer', ')', 'else', ':', 'return', 'reduced', '.', 'window', '(', 'windowDuration', ',', 'slideDuration', ')', '.', 'reduceByKey', '(', 'func', ',', 'numPartitions', ')']
Docstring: Return a new DStream by applying incremental `reduceByKey` over a sliding window.        The reduced value of over a new window is calculated using the old window's reduce value :         1. reduce the new values that entered the window (e.g., adding new counts)         2. "inverse reduce" the old values that left the window (e.g., subtracting old counts)        `invFunc` can be None, then it will reduce all the RDDs in window, could be slower        than having `invFunc`.        @param func:           associative and commutative reduce function        @param invFunc:        inverse function of `reduceFunc`        @param windowDuration: width of the window; must be a multiple of this DStream's                              batching interval        @param slideDuration:  sliding interval of the window (i.e., the interval after which                              the new DStream will generate RDDs); must be a multiple of this                              DStream's batching interval        @param numPartitions:  number of partitions of each RDD in the new DStream.        @param filterFunc:     function to filter expired key-value pairs;                              only pairs that satisfy the function are retained                              set this to null if you do not want to filter
*******__*******
Code:def updateStateByKey(self, updateFunc, numPartitions=None, initialRDD=None):\n        """\n        Return a new "state" DStream where the state for each key is updated by applying\n        the given function on the previous state of the key and the new values of the key.\n\n        @param updateFunc: State update function. If this function returns None, then\n                           corresponding state key-value pair will be eliminated.\n        """\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        if initialRDD and not isinstance(initialRDD, RDD):\n            initialRDD = self._sc.parallelize(initialRDD)\n\n        def reduceFunc(t, a, b):\n            if a is None:\n                g = b.groupByKey(numPartitions).mapValues(lambda vs: (list(vs), None))\n            else:\n                g = a.cogroup(b.partitionBy(numPartitions), numPartitions)\n                g = g.mapValues(lambda ab: (list(ab[1]), list(ab[0])[0] if len(ab[0]) else None))\n            state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))\n            return state.filter(lambda k_v: k_v[1] is not None)\n\n        jreduceFunc = TransformFunction(self._sc, reduceFunc,\n                                        self._sc.serializer, self._jrdd_deserializer)\n        if initialRDD:\n            initialRDD = initialRDD._reserialize(self._jrdd_deserializer)\n            dstream = self._sc._jvm.PythonStateDStream(self._jdstream.dstream(), jreduceFunc,\n                                                       initialRDD._jrdd)\n        else:\n            dstream = self._sc._jvm.PythonStateDStream(self._jdstream.dstream(), jreduceFunc)\n\n        return DStream(dstream.asJavaDStream(), self._ssc, self._sc.serializer)
Language: python
Code Tokens: ['def', 'updateStateByKey', '(', 'self', ',', 'updateFunc', ',', 'numPartitions', '=', 'None', ',', 'initialRDD', '=', 'None', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_sc', '.', 'defaultParallelism', 'if', 'initialRDD', 'and', 'not', 'isinstance', '(', 'initialRDD', ',', 'RDD', ')', ':', 'initialRDD', '=', 'self', '.', '_sc', '.', 'parallelize', '(', 'initialRDD', ')', 'def', 'reduceFunc', '(', 't', ',', 'a', ',', 'b', ')', ':', 'if', 'a', 'is', 'None', ':', 'g', '=', 'b', '.', 'groupByKey', '(', 'numPartitions', ')', '.', 'mapValues', '(', 'lambda', 'vs', ':', '(', 'list', '(', 'vs', ')', ',', 'None', ')', ')', 'else', ':', 'g', '=', 'a', '.', 'cogroup', '(', 'b', '.', 'partitionBy', '(', 'numPartitions', ')', ',', 'numPartitions', ')', 'g', '=', 'g', '.', 'mapValues', '(', 'lambda', 'ab', ':', '(', 'list', '(', 'ab', '[', '1', ']', ')', ',', 'list', '(', 'ab', '[', '0', ']', ')', '[', '0', ']', 'if', 'len', '(', 'ab', '[', '0', ']', ')', 'else', 'None', ')', ')', 'state', '=', 'g', '.', 'mapValues', '(', 'lambda', 'vs_s', ':', 'updateFunc', '(', 'vs_s', '[', '0', ']', ',', 'vs_s', '[', '1', ']', ')', ')', 'return', 'state', '.', 'filter', '(', 'lambda', 'k_v', ':', 'k_v', '[', '1', ']', 'is', 'not', 'None', ')', 'jreduceFunc', '=', 'TransformFunction', '(', 'self', '.', '_sc', ',', 'reduceFunc', ',', 'self', '.', '_sc', '.', 'serializer', ',', 'self', '.', '_jrdd_deserializer', ')', 'if', 'initialRDD', ':', 'initialRDD', '=', 'initialRDD', '.', '_reserialize', '(', 'self', '.', '_jrdd_deserializer', ')', 'dstream', '=', 'self', '.', '_sc', '.', '_jvm', '.', 'PythonStateDStream', '(', 'self', '.', '_jdstream', '.', 'dstream', '(', ')', ',', 'jreduceFunc', ',', 'initialRDD', '.', '_jrdd', ')', 'else', ':', 'dstream', '=', 'self', '.', '_sc', '.', '_jvm', '.', 'PythonStateDStream', '(', 'self', '.', '_jdstream', '.', 'dstream', '(', ')', ',', 'jreduceFunc', ')', 'return', 'DStream', '(', 'dstream', '.', 'asJavaDStream', '(', ')', ',', 'self', '.', '_ssc', ',', 'self', '.', '_sc', '.', 'serializer', ')']
Docstring: Return a new "state" DStream where the state for each key is updated by applying        the given function on the previous state of the key and the new values of the key.        @param updateFunc: State update function. If this function returns None, then                           corresponding state key-value pair will be eliminated.
*******__*******
Code:def setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol="items",\n                  predictionCol="prediction", numPartitions=None):\n        """\n        setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol="items", \\n                  predictionCol="prediction", numPartitions=None)\n        """\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)
Language: python
Code Tokens: ['def', 'setParams', '(', 'self', ',', 'minSupport', '=', '0.3', ',', 'minConfidence', '=', '0.8', ',', 'itemsCol', '=', '"items"', ',', 'predictionCol', '=', '"prediction"', ',', 'numPartitions', '=', 'None', ')', ':', 'kwargs', '=', 'self', '.', '_input_kwargs', 'return', 'self', '.', '_set', '(', '*', '*', 'kwargs', ')']
Docstring: setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol="items", \                  predictionCol="prediction", numPartitions=None)
*******__*******
Code:def setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000,\n                  sequenceCol="sequence"):\n        """\n        setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000, \\n                  sequenceCol="sequence")\n        """\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)
Language: python
Code Tokens: ['def', 'setParams', '(', 'self', ',', 'minSupport', '=', '0.1', ',', 'maxPatternLength', '=', '10', ',', 'maxLocalProjDBSize', '=', '32000000', ',', 'sequenceCol', '=', '"sequence"', ')', ':', 'kwargs', '=', 'self', '.', '_input_kwargs', 'return', 'self', '.', '_set', '(', '*', '*', 'kwargs', ')']
Docstring: setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000, \                  sequenceCol="sequence")
*******__*******
Code:def findFrequentSequentialPatterns(self, dataset):\n        """\n        .. note:: Experimental\n\n        Finds the complete set of frequent sequential patterns in the input sequences of itemsets.\n\n        :param dataset: A dataframe containing a sequence column which is\n                        `ArrayType(ArrayType(T))` type, T is the item type for the input dataset.\n        :return: A `DataFrame` that contains columns of sequence and corresponding frequency.\n                 The schema of it will be:\n                 - `sequence: ArrayType(ArrayType(T))` (T is the item type)\n                 - `freq: Long`\n\n        >>> from pyspark.ml.fpm import PrefixSpan\n        >>> from pyspark.sql import Row\n        >>> df = sc.parallelize([Row(sequence=[[1, 2], [3]]),\n        ...                      Row(sequence=[[1], [3, 2], [1, 2]]),\n        ...                      Row(sequence=[[1, 2], [5]]),\n        ...                      Row(sequence=[[6]])]).toDF()\n        >>> prefixSpan = PrefixSpan(minSupport=0.5, maxPatternLength=5)\n        >>> prefixSpan.findFrequentSequentialPatterns(df).sort("sequence").show(truncate=False)\n        +----------+----+\n        |sequence  |freq|\n        +----------+----+\n        |[[1]]     |3   |\n        |[[1], [3]]|2   |\n        |[[1, 2]]  |3   |\n        |[[2]]     |3   |\n        |[[3]]     |2   |\n        +----------+----+\n\n        .. versionadded:: 2.4.0\n        """\n        self._transfer_params_to_java()\n        jdf = self._java_obj.findFrequentSequentialPatterns(dataset._jdf)\n        return DataFrame(jdf, dataset.sql_ctx)
Language: python
Code Tokens: ['def', 'findFrequentSequentialPatterns', '(', 'self', ',', 'dataset', ')', ':', 'self', '.', '_transfer_params_to_java', '(', ')', 'jdf', '=', 'self', '.', '_java_obj', '.', 'findFrequentSequentialPatterns', '(', 'dataset', '.', '_jdf', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'dataset', '.', 'sql_ctx', ')']
Docstring: .. note:: Experimental        Finds the complete set of frequent sequential patterns in the input sequences of itemsets.        :param dataset: A dataframe containing a sequence column which is                        `ArrayType(ArrayType(T))` type, T is the item type for the input dataset.        :return: A `DataFrame` that contains columns of sequence and corresponding frequency.                 The schema of it will be:                 - `sequence: ArrayType(ArrayType(T))` (T is the item type)                 - `freq: Long`        >>> from pyspark.ml.fpm import PrefixSpan        >>> from pyspark.sql import Row        >>> df = sc.parallelize([Row(sequence=[[1, 2], [3]]),        ...                      Row(sequence=[[1], [3, 2], [1, 2]]),        ...                      Row(sequence=[[1, 2], [5]]),        ...                      Row(sequence=[[6]])]).toDF()        >>> prefixSpan = PrefixSpan(minSupport=0.5, maxPatternLength=5)        >>> prefixSpan.findFrequentSequentialPatterns(df).sort("sequence").show(truncate=False)        +----------+----+        |sequence  |freq|        +----------+----+        |[[1]]     |3   |        |[[1], [3]]|2   |        |[[1, 2]]  |3   |        |[[2]]     |3   |        |[[3]]     |2   |        +----------+----+        .. versionadded:: 2.4.0
*******__*******
Code:def first_spark_call():\n    """\n    Return a CallSite representing the first Spark call in the current call stack.\n    """\n    tb = traceback.extract_stack()\n    if len(tb) == 0:\n        return None\n    file, line, module, what = tb[len(tb) - 1]\n    sparkpath = os.path.dirname(file)\n    first_spark_frame = len(tb) - 1\n    for i in range(0, len(tb)):\n        file, line, fun, what = tb[i]\n        if file.startswith(sparkpath):\n            first_spark_frame = i\n            break\n    if first_spark_frame == 0:\n        file, line, fun, what = tb[0]\n        return CallSite(function=fun, file=file, linenum=line)\n    sfile, sline, sfun, swhat = tb[first_spark_frame]\n    ufile, uline, ufun, uwhat = tb[first_spark_frame - 1]\n    return CallSite(function=sfun, file=ufile, linenum=uline)
Language: python
Code Tokens: ['def', 'first_spark_call', '(', ')', ':', 'tb', '=', 'traceback', '.', 'extract_stack', '(', ')', 'if', 'len', '(', 'tb', ')', '==', '0', ':', 'return', 'None', 'file', ',', 'line', ',', 'module', ',', 'what', '=', 'tb', '[', 'len', '(', 'tb', ')', '-', '1', ']', 'sparkpath', '=', 'os', '.', 'path', '.', 'dirname', '(', 'file', ')', 'first_spark_frame', '=', 'len', '(', 'tb', ')', '-', '1', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tb', ')', ')', ':', 'file', ',', 'line', ',', 'fun', ',', 'what', '=', 'tb', '[', 'i', ']', 'if', 'file', '.', 'startswith', '(', 'sparkpath', ')', ':', 'first_spark_frame', '=', 'i', 'break', 'if', 'first_spark_frame', '==', '0', ':', 'file', ',', 'line', ',', 'fun', ',', 'what', '=', 'tb', '[', '0', ']', 'return', 'CallSite', '(', 'function', '=', 'fun', ',', 'file', '=', 'file', ',', 'linenum', '=', 'line', ')', 'sfile', ',', 'sline', ',', 'sfun', ',', 'swhat', '=', 'tb', '[', 'first_spark_frame', ']', 'ufile', ',', 'uline', ',', 'ufun', ',', 'uwhat', '=', 'tb', '[', 'first_spark_frame', '-', '1', ']', 'return', 'CallSite', '(', 'function', '=', 'sfun', ',', 'file', '=', 'ufile', ',', 'linenum', '=', 'uline', ')']
Docstring: Return a CallSite representing the first Spark call in the current call stack.
*******__*******
Code:def parsePoint(line):\n    """\n    Parse a line of text into an MLlib LabeledPoint object.\n    """\n    values = [float(s) for s in line.split(' ')]\n    if values[0] == -1:   # Convert -1 labels to 0 for MLlib\n        values[0] = 0\n    return LabeledPoint(values[0], values[1:])
Language: python
Code Tokens: ['def', 'parsePoint', '(', 'line', ')', ':', 'values', '=', '[', 'float', '(', 's', ')', 'for', 's', 'in', 'line', '.', 'split', '(', "' '", ')', ']', 'if', 'values', '[', '0', ']', '==', '-', '1', ':', '# Convert -1 labels to 0 for MLlib', 'values', '[', '0', ']', '=', '0', 'return', 'LabeledPoint', '(', 'values', '[', '0', ']', ',', 'values', '[', '1', ':', ']', ')']
Docstring: Parse a line of text into an MLlib LabeledPoint object.
*******__*******
Code:def fMeasure(self, label, beta=None):\n        """\n        Returns f-measure.\n        """\n        if beta is None:\n            return self.call("fMeasure", label)\n        else:\n            return self.call("fMeasure", label, beta)
Language: python
Code Tokens: ['def', 'fMeasure', '(', 'self', ',', 'label', ',', 'beta', '=', 'None', ')', ':', 'if', 'beta', 'is', 'None', ':', 'return', 'self', '.', 'call', '(', '"fMeasure"', ',', 'label', ')', 'else', ':', 'return', 'self', '.', 'call', '(', '"fMeasure"', ',', 'label', ',', 'beta', ')']
Docstring: Returns f-measure.
*******__*******
Code:def precision(self, label=None):\n        """\n        Returns precision or precision for a given label (category) if specified.\n        """\n        if label is None:\n            return self.call("precision")\n        else:\n            return self.call("precision", float(label))
Language: python
Code Tokens: ['def', 'precision', '(', 'self', ',', 'label', '=', 'None', ')', ':', 'if', 'label', 'is', 'None', ':', 'return', 'self', '.', 'call', '(', '"precision"', ')', 'else', ':', 'return', 'self', '.', 'call', '(', '"precision"', ',', 'float', '(', 'label', ')', ')']
Docstring: Returns precision or precision for a given label (category) if specified.
*******__*******
Code:def recall(self, label=None):\n        """\n        Returns recall or recall for a given label (category) if specified.\n        """\n        if label is None:\n            return self.call("recall")\n        else:\n            return self.call("recall", float(label))
Language: python
Code Tokens: ['def', 'recall', '(', 'self', ',', 'label', '=', 'None', ')', ':', 'if', 'label', 'is', 'None', ':', 'return', 'self', '.', 'call', '(', '"recall"', ')', 'else', ':', 'return', 'self', '.', 'call', '(', '"recall"', ',', 'float', '(', 'label', ')', ')']
Docstring: Returns recall or recall for a given label (category) if specified.
*******__*******
Code:def f1Measure(self, label=None):\n        """\n        Returns f1Measure or f1Measure for a given label (category) if specified.\n        """\n        if label is None:\n            return self.call("f1Measure")\n        else:\n            return self.call("f1Measure", float(label))
Language: python
Code Tokens: ['def', 'f1Measure', '(', 'self', ',', 'label', '=', 'None', ')', ':', 'if', 'label', 'is', 'None', ':', 'return', 'self', '.', 'call', '(', '"f1Measure"', ')', 'else', ':', 'return', 'self', '.', 'call', '(', '"f1Measure"', ',', 'float', '(', 'label', ')', ')']
Docstring: Returns f1Measure or f1Measure for a given label (category) if specified.
*******__*******
Code:def _to_corrected_pandas_type(dt):\n    """\n    When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.\n    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.\n    """\n    import numpy as np\n    if type(dt) == ByteType:\n        return np.int8\n    elif type(dt) == ShortType:\n        return np.int16\n    elif type(dt) == IntegerType:\n        return np.int32\n    elif type(dt) == FloatType:\n        return np.float32\n    else:\n        return None
Language: python
Code Tokens: ['def', '_to_corrected_pandas_type', '(', 'dt', ')', ':', 'import', 'numpy', 'as', 'np', 'if', 'type', '(', 'dt', ')', '==', 'ByteType', ':', 'return', 'np', '.', 'int8', 'elif', 'type', '(', 'dt', ')', '==', 'ShortType', ':', 'return', 'np', '.', 'int16', 'elif', 'type', '(', 'dt', ')', '==', 'IntegerType', ':', 'return', 'np', '.', 'int32', 'elif', 'type', '(', 'dt', ')', '==', 'FloatType', ':', 'return', 'np', '.', 'float32', 'else', ':', 'return', 'None']
Docstring: When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.
*******__*******
Code:def rdd(self):\n        """Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n        """\n        if self._lazy_rdd is None:\n            jrdd = self._jdf.javaToPython()\n            self._lazy_rdd = RDD(jrdd, self.sql_ctx._sc, BatchedSerializer(PickleSerializer()))\n        return self._lazy_rdd
Language: python
Code Tokens: ['def', 'rdd', '(', 'self', ')', ':', 'if', 'self', '.', '_lazy_rdd', 'is', 'None', ':', 'jrdd', '=', 'self', '.', '_jdf', '.', 'javaToPython', '(', ')', 'self', '.', '_lazy_rdd', '=', 'RDD', '(', 'jrdd', ',', 'self', '.', 'sql_ctx', '.', '_sc', ',', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ')', ')', 'return', 'self', '.', '_lazy_rdd']
Docstring: Returns the content as an :class:`pyspark.RDD` of :class:`Row`.
*******__*******
Code:def toJSON(self, use_unicode=True):\n        """Converts a :class:`DataFrame` into a :class:`RDD` of string.\n\n        Each row is turned into a JSON document as one element in the returned RDD.\n\n        >>> df.toJSON().first()\n        u'{"age":2,"name":"Alice"}'\n        """\n        rdd = self._jdf.toJSON()\n        return RDD(rdd.toJavaRDD(), self._sc, UTF8Deserializer(use_unicode))
Language: python
Code Tokens: ['def', 'toJSON', '(', 'self', ',', 'use_unicode', '=', 'True', ')', ':', 'rdd', '=', 'self', '.', '_jdf', '.', 'toJSON', '(', ')', 'return', 'RDD', '(', 'rdd', '.', 'toJavaRDD', '(', ')', ',', 'self', '.', '_sc', ',', 'UTF8Deserializer', '(', 'use_unicode', ')', ')']
Docstring: Converts a :class:`DataFrame` into a :class:`RDD` of string.        Each row is turned into a JSON document as one element in the returned RDD.        >>> df.toJSON().first()        u'{"age":2,"name":"Alice"}'
*******__*******
Code:def schema(self):\n        """Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n\n        >>> df.schema\n        StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n        """\n        if self._schema is None:\n            try:\n                self._schema = _parse_datatype_json_string(self._jdf.schema().json())\n            except AttributeError as e:\n                raise Exception(\n                    "Unable to parse datatype from schema. %s" % e)\n        return self._schema
Language: python
Code Tokens: ['def', 'schema', '(', 'self', ')', ':', 'if', 'self', '.', '_schema', 'is', 'None', ':', 'try', ':', 'self', '.', '_schema', '=', '_parse_datatype_json_string', '(', 'self', '.', '_jdf', '.', 'schema', '(', ')', '.', 'json', '(', ')', ')', 'except', 'AttributeError', 'as', 'e', ':', 'raise', 'Exception', '(', '"Unable to parse datatype from schema. %s"', '%', 'e', ')', 'return', 'self', '.', '_schema']
Docstring: Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.        >>> df.schema        StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))
*******__*******
Code:def explain(self, extended=False):\n        """Prints the (logical and physical) plans to the console for debugging purpose.\n\n        :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n\n        >>> df.explain()\n        == Physical Plan ==\n        *(1) Scan ExistingRDD[age#0,name#1]\n\n        >>> df.explain(True)\n        == Parsed Logical Plan ==\n        ...\n        == Analyzed Logical Plan ==\n        ...\n        == Optimized Logical Plan ==\n        ...\n        == Physical Plan ==\n        ...\n        """\n        if extended:\n            print(self._jdf.queryExecution().toString())\n        else:\n            print(self._jdf.queryExecution().simpleString())
Language: python
Code Tokens: ['def', 'explain', '(', 'self', ',', 'extended', '=', 'False', ')', ':', 'if', 'extended', ':', 'print', '(', 'self', '.', '_jdf', '.', 'queryExecution', '(', ')', '.', 'toString', '(', ')', ')', 'else', ':', 'print', '(', 'self', '.', '_jdf', '.', 'queryExecution', '(', ')', '.', 'simpleString', '(', ')', ')']
Docstring: Prints the (logical and physical) plans to the console for debugging purpose.        :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.        >>> df.explain()        == Physical Plan ==        *(1) Scan ExistingRDD[age#0,name#1]        >>> df.explain(True)        == Parsed Logical Plan ==        ...        == Analyzed Logical Plan ==        ...        == Optimized Logical Plan ==        ...        == Physical Plan ==        ...
*******__*******
Code:def exceptAll(self, other):\n        """Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n        not in another :class:`DataFrame` while preserving duplicates.\n\n        This is equivalent to `EXCEPT ALL` in SQL.\n\n        >>> df1 = spark.createDataFrame(\n        ...         [("a", 1), ("a", 1), ("a", 1), ("a", 2), ("b",  3), ("c", 4)], ["C1", "C2"])\n        >>> df2 = spark.createDataFrame([("a", 1), ("b", 3)], ["C1", "C2"])\n\n        >>> df1.exceptAll(df2).show()\n        +---+---+\n        | C1| C2|\n        +---+---+\n        |  a|  1|\n        |  a|  1|\n        |  a|  2|\n        |  c|  4|\n        +---+---+\n\n        Also as standard in SQL, this function resolves columns by position (not by name).\n        """\n        return DataFrame(self._jdf.exceptAll(other._jdf), self.sql_ctx)
Language: python
Code Tokens: ['def', 'exceptAll', '(', 'self', ',', 'other', ')', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'exceptAll', '(', 'other', '.', '_jdf', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but        not in another :class:`DataFrame` while preserving duplicates.        This is equivalent to `EXCEPT ALL` in SQL.        >>> df1 = spark.createDataFrame(        ...         [("a", 1), ("a", 1), ("a", 1), ("a", 2), ("b",  3), ("c", 4)], ["C1", "C2"])        >>> df2 = spark.createDataFrame([("a", 1), ("b", 3)], ["C1", "C2"])        >>> df1.exceptAll(df2).show()        +---+---+        | C1| C2|        +---+---+        |  a|  1|        |  a|  1|        |  a|  2|        |  c|  4|        +---+---+        Also as standard in SQL, this function resolves columns by position (not by name).
*******__*******
Code:def show(self, n=20, truncate=True, vertical=False):\n        """Prints the first ``n`` rows to the console.\n\n        :param n: Number of rows to show.\n        :param truncate: If set to True, truncate strings longer than 20 chars by default.\n            If set to a number greater than one, truncates long strings to length ``truncate``\n            and align cells right.\n        :param vertical: If set to True, print output rows vertically (one line\n            per column value).\n\n        >>> df\n        DataFrame[age: int, name: string]\n        >>> df.show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  2|Alice|\n        |  5|  Bob|\n        +---+-----+\n        >>> df.show(truncate=3)\n        +---+----+\n        |age|name|\n        +---+----+\n        |  2| Ali|\n        |  5| Bob|\n        +---+----+\n        >>> df.show(vertical=True)\n        -RECORD 0-----\n         age  | 2\n         name | Alice\n        -RECORD 1-----\n         age  | 5\n         name | Bob\n        """\n        if isinstance(truncate, bool) and truncate:\n            print(self._jdf.showString(n, 20, vertical))\n        else:\n            print(self._jdf.showString(n, int(truncate), vertical))
Language: python
Code Tokens: ['def', 'show', '(', 'self', ',', 'n', '=', '20', ',', 'truncate', '=', 'True', ',', 'vertical', '=', 'False', ')', ':', 'if', 'isinstance', '(', 'truncate', ',', 'bool', ')', 'and', 'truncate', ':', 'print', '(', 'self', '.', '_jdf', '.', 'showString', '(', 'n', ',', '20', ',', 'vertical', ')', ')', 'else', ':', 'print', '(', 'self', '.', '_jdf', '.', 'showString', '(', 'n', ',', 'int', '(', 'truncate', ')', ',', 'vertical', ')', ')']
Docstring: Prints the first ``n`` rows to the console.        :param n: Number of rows to show.        :param truncate: If set to True, truncate strings longer than 20 chars by default.            If set to a number greater than one, truncates long strings to length ``truncate``            and align cells right.        :param vertical: If set to True, print output rows vertically (one line            per column value).        >>> df        DataFrame[age: int, name: string]        >>> df.show()        +---+-----+        |age| name|        +---+-----+        |  2|Alice|        |  5|  Bob|        +---+-----+        >>> df.show(truncate=3)        +---+----+        |age|name|        +---+----+        |  2| Ali|        |  5| Bob|        +---+----+        >>> df.show(vertical=True)        -RECORD 0-----         age  | 2         name | Alice        -RECORD 1-----         age  | 5         name | Bob
*******__*******
Code:def _repr_html_(self):\n        """Returns a dataframe with html code when you enabled eager evaluation\n        by 'spark.sql.repl.eagerEval.enabled', this only called by REPL you are\n        using support eager evaluation with HTML.\n        """\n        import cgi\n        if not self._support_repr_html:\n            self._support_repr_html = True\n        if self.sql_ctx._conf.isReplEagerEvalEnabled():\n            max_num_rows = max(self.sql_ctx._conf.replEagerEvalMaxNumRows(), 0)\n            sock_info = self._jdf.getRowsToPython(\n                max_num_rows, self.sql_ctx._conf.replEagerEvalTruncate())\n            rows = list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n            head = rows[0]\n            row_data = rows[1:]\n            has_more_data = len(row_data) > max_num_rows\n            row_data = row_data[:max_num_rows]\n\n            html = "<table border='1'>\n"\n            # generate table head\n            html += "<tr><th>%s</th></tr>\n" % "</th><th>".join(map(lambda x: cgi.escape(x), head))\n            # generate table rows\n            for row in row_data:\n                html += "<tr><td>%s</td></tr>\n" % "</td><td>".join(\n                    map(lambda x: cgi.escape(x), row))\n            html += "</table>\n"\n            if has_more_data:\n                html += "only showing top %d %s\n" % (\n                    max_num_rows, "row" if max_num_rows == 1 else "rows")\n            return html\n        else:\n            return None
Language: python
Code Tokens: ['def', '_repr_html_', '(', 'self', ')', ':', 'import', 'cgi', 'if', 'not', 'self', '.', '_support_repr_html', ':', 'self', '.', '_support_repr_html', '=', 'True', 'if', 'self', '.', 'sql_ctx', '.', '_conf', '.', 'isReplEagerEvalEnabled', '(', ')', ':', 'max_num_rows', '=', 'max', '(', 'self', '.', 'sql_ctx', '.', '_conf', '.', 'replEagerEvalMaxNumRows', '(', ')', ',', '0', ')', 'sock_info', '=', 'self', '.', '_jdf', '.', 'getRowsToPython', '(', 'max_num_rows', ',', 'self', '.', 'sql_ctx', '.', '_conf', '.', 'replEagerEvalTruncate', '(', ')', ')', 'rows', '=', 'list', '(', '_load_from_socket', '(', 'sock_info', ',', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ')', ')', ')', 'head', '=', 'rows', '[', '0', ']', 'row_data', '=', 'rows', '[', '1', ':', ']', 'has_more_data', '=', 'len', '(', 'row_data', ')', '>', 'max_num_rows', 'row_data', '=', 'row_data', '[', ':', 'max_num_rows', ']', 'html', '=', '"<table border=\'1\'>\\n"', '# generate table head', 'html', '+=', '"<tr><th>%s</th></tr>\\n"', '%', '"</th><th>"', '.', 'join', '(', 'map', '(', 'lambda', 'x', ':', 'cgi', '.', 'escape', '(', 'x', ')', ',', 'head', ')', ')', '# generate table rows', 'for', 'row', 'in', 'row_data', ':', 'html', '+=', '"<tr><td>%s</td></tr>\\n"', '%', '"</td><td>"', '.', 'join', '(', 'map', '(', 'lambda', 'x', ':', 'cgi', '.', 'escape', '(', 'x', ')', ',', 'row', ')', ')', 'html', '+=', '"</table>\\n"', 'if', 'has_more_data', ':', 'html', '+=', '"only showing top %d %s\\n"', '%', '(', 'max_num_rows', ',', '"row"', 'if', 'max_num_rows', '==', '1', 'else', '"rows"', ')', 'return', 'html', 'else', ':', 'return', 'None']
Docstring: Returns a dataframe with html code when you enabled eager evaluation        by 'spark.sql.repl.eagerEval.enabled', this only called by REPL you are        using support eager evaluation with HTML.
*******__*******
Code:def checkpoint(self, eager=True):\n        """Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n        logical plan of this DataFrame, which is especially useful in iterative algorithms where the\n        plan may grow exponentially. It will be saved to files inside the checkpoint\n        directory set with L{SparkContext.setCheckpointDir()}.\n\n        :param eager: Whether to checkpoint this DataFrame immediately\n\n        .. note:: Experimental\n        """\n        jdf = self._jdf.checkpoint(eager)\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'checkpoint', '(', 'self', ',', 'eager', '=', 'True', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'checkpoint', '(', 'eager', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the        logical plan of this DataFrame, which is especially useful in iterative algorithms where the        plan may grow exponentially. It will be saved to files inside the checkpoint        directory set with L{SparkContext.setCheckpointDir()}.        :param eager: Whether to checkpoint this DataFrame immediately        .. note:: Experimental
*******__*******
Code:def localCheckpoint(self, eager=True):\n        """Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n        truncate the logical plan of this DataFrame, which is especially useful in iterative\n        algorithms where the plan may grow exponentially. Local checkpoints are stored in the\n        executors using the caching subsystem and therefore they are not reliable.\n\n        :param eager: Whether to checkpoint this DataFrame immediately\n\n        .. note:: Experimental\n        """\n        jdf = self._jdf.localCheckpoint(eager)\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'localCheckpoint', '(', 'self', ',', 'eager', '=', 'True', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'localCheckpoint', '(', 'eager', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a locally checkpointed version of this Dataset. Checkpointing can be used to        truncate the logical plan of this DataFrame, which is especially useful in iterative        algorithms where the plan may grow exponentially. Local checkpoints are stored in the        executors using the caching subsystem and therefore they are not reliable.        :param eager: Whether to checkpoint this DataFrame immediately        .. note:: Experimental
*******__*******
Code:def withWatermark(self, eventTime, delayThreshold):\n        """Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n        in time before which we assume no more late data is going to arrive.\n\n        Spark will use this watermark for several purposes:\n          - To know when a given time window aggregation can be finalized and thus can be emitted\n            when using output modes that do not allow updates.\n\n          - To minimize the amount of state that we need to keep for on-going aggregations.\n\n        The current watermark is computed by looking at the `MAX(eventTime)` seen across\n        all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n        of coordinating this value across partitions, the actual watermark used is only guaranteed\n        to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n        process records that arrive more than `delayThreshold` late.\n\n        :param eventTime: the name of the column that contains the event time of the row.\n        :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n            latest record that has been processed in the form of an interval\n            (e.g. "1 minute" or "5 hours").\n\n        .. note:: Evolving\n\n        >>> sdf.select('name', sdf.time.cast('timestamp')).withWatermark('time', '10 minutes')\n        DataFrame[name: string, time: timestamp]\n        """\n        if not eventTime or type(eventTime) is not str:\n            raise TypeError("eventTime should be provided as a string")\n        if not delayThreshold or type(delayThreshold) is not str:\n            raise TypeError("delayThreshold should be provided as a string interval")\n        jdf = self._jdf.withWatermark(eventTime, delayThreshold)\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'withWatermark', '(', 'self', ',', 'eventTime', ',', 'delayThreshold', ')', ':', 'if', 'not', 'eventTime', 'or', 'type', '(', 'eventTime', ')', 'is', 'not', 'str', ':', 'raise', 'TypeError', '(', '"eventTime should be provided as a string"', ')', 'if', 'not', 'delayThreshold', 'or', 'type', '(', 'delayThreshold', ')', 'is', 'not', 'str', ':', 'raise', 'TypeError', '(', '"delayThreshold should be provided as a string interval"', ')', 'jdf', '=', 'self', '.', '_jdf', '.', 'withWatermark', '(', 'eventTime', ',', 'delayThreshold', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point        in time before which we assume no more late data is going to arrive.        Spark will use this watermark for several purposes:          - To know when a given time window aggregation can be finalized and thus can be emitted            when using output modes that do not allow updates.          - To minimize the amount of state that we need to keep for on-going aggregations.        The current watermark is computed by looking at the `MAX(eventTime)` seen across        all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost        of coordinating this value across partitions, the actual watermark used is only guaranteed        to be at least `delayThreshold` behind the actual event time.  In some cases we may still        process records that arrive more than `delayThreshold` late.        :param eventTime: the name of the column that contains the event time of the row.        :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the            latest record that has been processed in the form of an interval            (e.g. "1 minute" or "5 hours").        .. note:: Evolving        >>> sdf.select('name', sdf.time.cast('timestamp')).withWatermark('time', '10 minutes')        DataFrame[name: string, time: timestamp]
*******__*******
Code:def hint(self, name, *parameters):\n        """Specifies some hint on the current DataFrame.\n\n        :param name: A name of the hint.\n        :param parameters: Optional parameters.\n        :return: :class:`DataFrame`\n\n        >>> df.join(df2.hint("broadcast"), "name").show()\n        +----+---+------+\n        |name|age|height|\n        +----+---+------+\n        | Bob|  5|    85|\n        +----+---+------+\n        """\n        if len(parameters) == 1 and isinstance(parameters[0], list):\n            parameters = parameters[0]\n\n        if not isinstance(name, str):\n            raise TypeError("name should be provided as str, got {0}".format(type(name)))\n\n        allowed_types = (basestring, list, float, int)\n        for p in parameters:\n            if not isinstance(p, allowed_types):\n                raise TypeError(\n                    "all parameters should be in {0}, got {1} of type {2}".format(\n                        allowed_types, p, type(p)))\n\n        jdf = self._jdf.hint(name, self._jseq(parameters))\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'hint', '(', 'self', ',', 'name', ',', '*', 'parameters', ')', ':', 'if', 'len', '(', 'parameters', ')', '==', '1', 'and', 'isinstance', '(', 'parameters', '[', '0', ']', ',', 'list', ')', ':', 'parameters', '=', 'parameters', '[', '0', ']', 'if', 'not', 'isinstance', '(', 'name', ',', 'str', ')', ':', 'raise', 'TypeError', '(', '"name should be provided as str, got {0}"', '.', 'format', '(', 'type', '(', 'name', ')', ')', ')', 'allowed_types', '=', '(', 'basestring', ',', 'list', ',', 'float', ',', 'int', ')', 'for', 'p', 'in', 'parameters', ':', 'if', 'not', 'isinstance', '(', 'p', ',', 'allowed_types', ')', ':', 'raise', 'TypeError', '(', '"all parameters should be in {0}, got {1} of type {2}"', '.', 'format', '(', 'allowed_types', ',', 'p', ',', 'type', '(', 'p', ')', ')', ')', 'jdf', '=', 'self', '.', '_jdf', '.', 'hint', '(', 'name', ',', 'self', '.', '_jseq', '(', 'parameters', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Specifies some hint on the current DataFrame.        :param name: A name of the hint.        :param parameters: Optional parameters.        :return: :class:`DataFrame`        >>> df.join(df2.hint("broadcast"), "name").show()        +----+---+------+        |name|age|height|        +----+---+------+        | Bob|  5|    85|        +----+---+------+
*******__*******
Code:def collect(self):\n        """Returns all the records as a list of :class:`Row`.\n\n        >>> df.collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        """\n        with SCCallSiteSync(self._sc) as css:\n            sock_info = self._jdf.collectToPython()\n        return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))
Language: python
Code Tokens: ['def', 'collect', '(', 'self', ')', ':', 'with', 'SCCallSiteSync', '(', 'self', '.', '_sc', ')', 'as', 'css', ':', 'sock_info', '=', 'self', '.', '_jdf', '.', 'collectToPython', '(', ')', 'return', 'list', '(', '_load_from_socket', '(', 'sock_info', ',', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ')', ')', ')']
Docstring: Returns all the records as a list of :class:`Row`.        >>> df.collect()        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
*******__*******
Code:def toLocalIterator(self):\n        """\n        Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n        The iterator will consume as much memory as the largest partition in this DataFrame.\n\n        >>> list(df.toLocalIterator())\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        """\n        with SCCallSiteSync(self._sc) as css:\n            sock_info = self._jdf.toPythonIterator()\n        return _load_from_socket(sock_info, BatchedSerializer(PickleSerializer()))
Language: python
Code Tokens: ['def', 'toLocalIterator', '(', 'self', ')', ':', 'with', 'SCCallSiteSync', '(', 'self', '.', '_sc', ')', 'as', 'css', ':', 'sock_info', '=', 'self', '.', '_jdf', '.', 'toPythonIterator', '(', ')', 'return', '_load_from_socket', '(', 'sock_info', ',', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ')', ')']
Docstring: Returns an iterator that contains all of the rows in this :class:`DataFrame`.        The iterator will consume as much memory as the largest partition in this DataFrame.        >>> list(df.toLocalIterator())        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]
*******__*******
Code:def limit(self, num):\n        """Limits the result count to the number specified.\n\n        >>> df.limit(1).collect()\n        [Row(age=2, name=u'Alice')]\n        >>> df.limit(0).collect()\n        []\n        """\n        jdf = self._jdf.limit(num)\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'limit', '(', 'self', ',', 'num', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'limit', '(', 'num', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Limits the result count to the number specified.        >>> df.limit(1).collect()        [Row(age=2, name=u'Alice')]        >>> df.limit(0).collect()        []
*******__*******
Code:def persist(self, storageLevel=StorageLevel.MEMORY_AND_DISK):\n        """Sets the storage level to persist the contents of the :class:`DataFrame` across\n        operations after the first time it is computed. This can only be used to assign\n        a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n        If no storage level is specified defaults to (C{MEMORY_AND_DISK}).\n\n        .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.\n        """\n        self.is_cached = True\n        javaStorageLevel = self._sc._getJavaStorageLevel(storageLevel)\n        self._jdf.persist(javaStorageLevel)\n        return self
Language: python
Code Tokens: ['def', 'persist', '(', 'self', ',', 'storageLevel', '=', 'StorageLevel', '.', 'MEMORY_AND_DISK', ')', ':', 'self', '.', 'is_cached', '=', 'True', 'javaStorageLevel', '=', 'self', '.', '_sc', '.', '_getJavaStorageLevel', '(', 'storageLevel', ')', 'self', '.', '_jdf', '.', 'persist', '(', 'javaStorageLevel', ')', 'return', 'self']
Docstring: Sets the storage level to persist the contents of the :class:`DataFrame` across        operations after the first time it is computed. This can only be used to assign        a new storage level if the :class:`DataFrame` does not have a storage level set yet.        If no storage level is specified defaults to (C{MEMORY_AND_DISK}).        .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.
*******__*******
Code:def storageLevel(self):\n        """Get the :class:`DataFrame`'s current storage level.\n\n        >>> df.storageLevel\n        StorageLevel(False, False, False, False, 1)\n        >>> df.cache().storageLevel\n        StorageLevel(True, True, False, True, 1)\n        >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n        StorageLevel(True, False, False, False, 2)\n        """\n        java_storage_level = self._jdf.storageLevel()\n        storage_level = StorageLevel(java_storage_level.useDisk(),\n                                     java_storage_level.useMemory(),\n                                     java_storage_level.useOffHeap(),\n                                     java_storage_level.deserialized(),\n                                     java_storage_level.replication())\n        return storage_level
Language: python
Code Tokens: ['def', 'storageLevel', '(', 'self', ')', ':', 'java_storage_level', '=', 'self', '.', '_jdf', '.', 'storageLevel', '(', ')', 'storage_level', '=', 'StorageLevel', '(', 'java_storage_level', '.', 'useDisk', '(', ')', ',', 'java_storage_level', '.', 'useMemory', '(', ')', ',', 'java_storage_level', '.', 'useOffHeap', '(', ')', ',', 'java_storage_level', '.', 'deserialized', '(', ')', ',', 'java_storage_level', '.', 'replication', '(', ')', ')', 'return', 'storage_level']
Docstring: Get the :class:`DataFrame`'s current storage level.        >>> df.storageLevel        StorageLevel(False, False, False, False, 1)        >>> df.cache().storageLevel        StorageLevel(True, True, False, True, 1)        >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel        StorageLevel(True, False, False, False, 2)
*******__*******
Code:def unpersist(self, blocking=False):\n        """Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. note:: `blocking` default has changed to False to match Scala in 2.0.\n        """\n        self.is_cached = False\n        self._jdf.unpersist(blocking)\n        return self
Language: python
Code Tokens: ['def', 'unpersist', '(', 'self', ',', 'blocking', '=', 'False', ')', ':', 'self', '.', 'is_cached', '=', 'False', 'self', '.', '_jdf', '.', 'unpersist', '(', 'blocking', ')', 'return', 'self']
Docstring: Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from        memory and disk.        .. note:: `blocking` default has changed to False to match Scala in 2.0.
*******__*******
Code:def coalesce(self, numPartitions):\n        """\n        Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n\n        :param numPartitions: int, to specify the target number of partitions\n\n        Similar to coalesce defined on an :class:`RDD`, this operation results in a\n        narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n        there will not be a shuffle, instead each of the 100 new partitions will\n        claim 10 of the current partitions. If a larger number of partitions is requested,\n        it will stay at the current number of partitions.\n\n        However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n        this may result in your computation taking place on fewer nodes than\n        you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n        you can call repartition(). This will add a shuffle step, but means the\n        current upstream partitions will be executed in parallel (per whatever\n        the current partitioning is).\n\n        >>> df.coalesce(1).rdd.getNumPartitions()\n        1\n        """\n        return DataFrame(self._jdf.coalesce(numPartitions), self.sql_ctx)
Language: python
Code Tokens: ['def', 'coalesce', '(', 'self', ',', 'numPartitions', ')', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'coalesce', '(', 'numPartitions', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.        :param numPartitions: int, to specify the target number of partitions        Similar to coalesce defined on an :class:`RDD`, this operation results in a        narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,        there will not be a shuffle, instead each of the 100 new partitions will        claim 10 of the current partitions. If a larger number of partitions is requested,        it will stay at the current number of partitions.        However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,        this may result in your computation taking place on fewer nodes than        you like (e.g. one node in the case of numPartitions = 1). To avoid this,        you can call repartition(). This will add a shuffle step, but means the        current upstream partitions will be executed in parallel (per whatever        the current partitioning is).        >>> df.coalesce(1).rdd.getNumPartitions()        1
*******__*******
Code:def repartition(self, numPartitions, *cols):\n        """\n        Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n        resulting DataFrame is hash partitioned.\n\n        :param numPartitions:\n            can be an int to specify the target number of partitions or a Column.\n            If it is a Column, it will be used as the first partitioning column. If not specified,\n            the default number of partitions is used.\n\n        .. versionchanged:: 1.6\n           Added optional arguments to specify the partitioning columns. Also made numPartitions\n           optional if partitioning columns are specified.\n\n        >>> df.repartition(10).rdd.getNumPartitions()\n        10\n        >>> data = df.union(df).repartition("age")\n        >>> data.show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  5|  Bob|\n        |  5|  Bob|\n        |  2|Alice|\n        |  2|Alice|\n        +---+-----+\n        >>> data = data.repartition(7, "age")\n        >>> data.show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  2|Alice|\n        |  5|  Bob|\n        |  2|Alice|\n        |  5|  Bob|\n        +---+-----+\n        >>> data.rdd.getNumPartitions()\n        7\n        >>> data = data.repartition("name", "age")\n        >>> data.show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  5|  Bob|\n        |  5|  Bob|\n        |  2|Alice|\n        |  2|Alice|\n        +---+-----+\n        """\n        if isinstance(numPartitions, int):\n            if len(cols) == 0:\n                return DataFrame(self._jdf.repartition(numPartitions), self.sql_ctx)\n            else:\n                return DataFrame(\n                    self._jdf.repartition(numPartitions, self._jcols(*cols)), self.sql_ctx)\n        elif isinstance(numPartitions, (basestring, Column)):\n            cols = (numPartitions, ) + cols\n            return DataFrame(self._jdf.repartition(self._jcols(*cols)), self.sql_ctx)\n        else:\n            raise TypeError("numPartitions should be an int or Column")
Language: python
Code Tokens: ['def', 'repartition', '(', 'self', ',', 'numPartitions', ',', '*', 'cols', ')', ':', 'if', 'isinstance', '(', 'numPartitions', ',', 'int', ')', ':', 'if', 'len', '(', 'cols', ')', '==', '0', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'repartition', '(', 'numPartitions', ')', ',', 'self', '.', 'sql_ctx', ')', 'else', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'repartition', '(', 'numPartitions', ',', 'self', '.', '_jcols', '(', '*', 'cols', ')', ')', ',', 'self', '.', 'sql_ctx', ')', 'elif', 'isinstance', '(', 'numPartitions', ',', '(', 'basestring', ',', 'Column', ')', ')', ':', 'cols', '=', '(', 'numPartitions', ',', ')', '+', 'cols', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'repartition', '(', 'self', '.', '_jcols', '(', '*', 'cols', ')', ')', ',', 'self', '.', 'sql_ctx', ')', 'else', ':', 'raise', 'TypeError', '(', '"numPartitions should be an int or Column"', ')']
Docstring: Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The        resulting DataFrame is hash partitioned.        :param numPartitions:            can be an int to specify the target number of partitions or a Column.            If it is a Column, it will be used as the first partitioning column. If not specified,            the default number of partitions is used.        .. versionchanged:: 1.6           Added optional arguments to specify the partitioning columns. Also made numPartitions           optional if partitioning columns are specified.        >>> df.repartition(10).rdd.getNumPartitions()        10        >>> data = df.union(df).repartition("age")        >>> data.show()        +---+-----+        |age| name|        +---+-----+        |  5|  Bob|        |  5|  Bob|        |  2|Alice|        |  2|Alice|        +---+-----+        >>> data = data.repartition(7, "age")        >>> data.show()        +---+-----+        |age| name|        +---+-----+        |  2|Alice|        |  5|  Bob|        |  2|Alice|        |  5|  Bob|        +---+-----+        >>> data.rdd.getNumPartitions()        7        >>> data = data.repartition("name", "age")        >>> data.show()        +---+-----+        |age| name|        +---+-----+        |  5|  Bob|        |  5|  Bob|        |  2|Alice|        |  2|Alice|        +---+-----+
*******__*******
Code:def sample(self, withReplacement=None, fraction=None, seed=None):\n        """Returns a sampled subset of this :class:`DataFrame`.\n\n        :param withReplacement: Sample with replacement or not (default False).\n        :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n        :param seed: Seed for sampling (default a random seed).\n\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n            count of the given :class:`DataFrame`.\n\n        .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n\n        >>> df = spark.range(10)\n        >>> df.sample(0.5, 3).count()\n        7\n        >>> df.sample(fraction=0.5, seed=3).count()\n        7\n        >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n        1\n        >>> df.sample(1.0).count()\n        10\n        >>> df.sample(fraction=1.0).count()\n        10\n        >>> df.sample(False, fraction=1.0).count()\n        10\n        """\n\n        # For the cases below:\n        #   sample(True, 0.5 [, seed])\n        #   sample(True, fraction=0.5 [, seed])\n        #   sample(withReplacement=False, fraction=0.5 [, seed])\n        is_withReplacement_set = \\n            type(withReplacement) == bool and isinstance(fraction, float)\n\n        # For the case below:\n        #   sample(faction=0.5 [, seed])\n        is_withReplacement_omitted_kwargs = \\n            withReplacement is None and isinstance(fraction, float)\n\n        # For the case below:\n        #   sample(0.5 [, seed])\n        is_withReplacement_omitted_args = isinstance(withReplacement, float)\n\n        if not (is_withReplacement_set\n                or is_withReplacement_omitted_kwargs\n                or is_withReplacement_omitted_args):\n            argtypes = [\n                str(type(arg)) for arg in [withReplacement, fraction, seed] if arg is not None]\n            raise TypeError(\n                "withReplacement (optional), fraction (required) and seed (optional)"\n                " should be a bool, float and number; however, "\n                "got [%s]." % ", ".join(argtypes))\n\n        if is_withReplacement_omitted_args:\n            if fraction is not None:\n                seed = fraction\n            fraction = withReplacement\n            withReplacement = None\n\n        seed = long(seed) if seed is not None else None\n        args = [arg for arg in [withReplacement, fraction, seed] if arg is not None]\n        jdf = self._jdf.sample(*args)\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'sample', '(', 'self', ',', 'withReplacement', '=', 'None', ',', 'fraction', '=', 'None', ',', 'seed', '=', 'None', ')', ':', '# For the cases below:', '#   sample(True, 0.5 [, seed])', '#   sample(True, fraction=0.5 [, seed])', '#   sample(withReplacement=False, fraction=0.5 [, seed])', 'is_withReplacement_set', '=', 'type', '(', 'withReplacement', ')', '==', 'bool', 'and', 'isinstance', '(', 'fraction', ',', 'float', ')', '# For the case below:', '#   sample(faction=0.5 [, seed])', 'is_withReplacement_omitted_kwargs', '=', 'withReplacement', 'is', 'None', 'and', 'isinstance', '(', 'fraction', ',', 'float', ')', '# For the case below:', '#   sample(0.5 [, seed])', 'is_withReplacement_omitted_args', '=', 'isinstance', '(', 'withReplacement', ',', 'float', ')', 'if', 'not', '(', 'is_withReplacement_set', 'or', 'is_withReplacement_omitted_kwargs', 'or', 'is_withReplacement_omitted_args', ')', ':', 'argtypes', '=', '[', 'str', '(', 'type', '(', 'arg', ')', ')', 'for', 'arg', 'in', '[', 'withReplacement', ',', 'fraction', ',', 'seed', ']', 'if', 'arg', 'is', 'not', 'None', ']', 'raise', 'TypeError', '(', '"withReplacement (optional), fraction (required) and seed (optional)"', '" should be a bool, float and number; however, "', '"got [%s]."', '%', '", "', '.', 'join', '(', 'argtypes', ')', ')', 'if', 'is_withReplacement_omitted_args', ':', 'if', 'fraction', 'is', 'not', 'None', ':', 'seed', '=', 'fraction', 'fraction', '=', 'withReplacement', 'withReplacement', '=', 'None', 'seed', '=', 'long', '(', 'seed', ')', 'if', 'seed', 'is', 'not', 'None', 'else', 'None', 'args', '=', '[', 'arg', 'for', 'arg', 'in', '[', 'withReplacement', ',', 'fraction', ',', 'seed', ']', 'if', 'arg', 'is', 'not', 'None', ']', 'jdf', '=', 'self', '.', '_jdf', '.', 'sample', '(', '*', 'args', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a sampled subset of this :class:`DataFrame`.        :param withReplacement: Sample with replacement or not (default False).        :param fraction: Fraction of rows to generate, range [0.0, 1.0].        :param seed: Seed for sampling (default a random seed).        .. note:: This is not guaranteed to provide exactly the fraction specified of the total            count of the given :class:`DataFrame`.        .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.        >>> df = spark.range(10)        >>> df.sample(0.5, 3).count()        7        >>> df.sample(fraction=0.5, seed=3).count()        7        >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()        1        >>> df.sample(1.0).count()        10        >>> df.sample(fraction=1.0).count()        10        >>> df.sample(False, fraction=1.0).count()        10
*******__*******
Code:def sampleBy(self, col, fractions, seed=None):\n        """\n        Returns a stratified sample without replacement based on the\n        fraction given on each stratum.\n\n        :param col: column that defines strata\n        :param fractions:\n            sampling fraction for each stratum. If a stratum is not\n            specified, we treat its fraction as zero.\n        :param seed: random seed\n        :return: a new DataFrame that represents the stratified sample\n\n        >>> from pyspark.sql.functions import col\n        >>> dataset = sqlContext.range(0, 100).select((col("id") % 3).alias("key"))\n        >>> sampled = dataset.sampleBy("key", fractions={0: 0.1, 1: 0.2}, seed=0)\n        >>> sampled.groupBy("key").count().orderBy("key").show()\n        +---+-----+\n        |key|count|\n        +---+-----+\n        |  0|    3|\n        |  1|    6|\n        +---+-----+\n        >>> dataset.sampleBy(col("key"), fractions={2: 1.0}, seed=0).count()\n        33\n\n        .. versionchanged:: 3.0\n           Added sampling by a column of :class:`Column`\n        """\n        if isinstance(col, basestring):\n            col = Column(col)\n        elif not isinstance(col, Column):\n            raise ValueError("col must be a string or a column, but got %r" % type(col))\n        if not isinstance(fractions, dict):\n            raise ValueError("fractions must be a dict but got %r" % type(fractions))\n        for k, v in fractions.items():\n            if not isinstance(k, (float, int, long, basestring)):\n                raise ValueError("key must be float, int, long, or string, but got %r" % type(k))\n            fractions[k] = float(v)\n        col = col._jc\n        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n        return DataFrame(self._jdf.stat().sampleBy(col, self._jmap(fractions), seed), self.sql_ctx)
Language: python
Code Tokens: ['def', 'sampleBy', '(', 'self', ',', 'col', ',', 'fractions', ',', 'seed', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'col', ',', 'basestring', ')', ':', 'col', '=', 'Column', '(', 'col', ')', 'elif', 'not', 'isinstance', '(', 'col', ',', 'Column', ')', ':', 'raise', 'ValueError', '(', '"col must be a string or a column, but got %r"', '%', 'type', '(', 'col', ')', ')', 'if', 'not', 'isinstance', '(', 'fractions', ',', 'dict', ')', ':', 'raise', 'ValueError', '(', '"fractions must be a dict but got %r"', '%', 'type', '(', 'fractions', ')', ')', 'for', 'k', ',', 'v', 'in', 'fractions', '.', 'items', '(', ')', ':', 'if', 'not', 'isinstance', '(', 'k', ',', '(', 'float', ',', 'int', ',', 'long', ',', 'basestring', ')', ')', ':', 'raise', 'ValueError', '(', '"key must be float, int, long, or string, but got %r"', '%', 'type', '(', 'k', ')', ')', 'fractions', '[', 'k', ']', '=', 'float', '(', 'v', ')', 'col', '=', 'col', '.', '_jc', 'seed', '=', 'seed', 'if', 'seed', 'is', 'not', 'None', 'else', 'random', '.', 'randint', '(', '0', ',', 'sys', '.', 'maxsize', ')', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'stat', '(', ')', '.', 'sampleBy', '(', 'col', ',', 'self', '.', '_jmap', '(', 'fractions', ')', ',', 'seed', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a stratified sample without replacement based on the        fraction given on each stratum.        :param col: column that defines strata        :param fractions:            sampling fraction for each stratum. If a stratum is not            specified, we treat its fraction as zero.        :param seed: random seed        :return: a new DataFrame that represents the stratified sample        >>> from pyspark.sql.functions import col        >>> dataset = sqlContext.range(0, 100).select((col("id") % 3).alias("key"))        >>> sampled = dataset.sampleBy("key", fractions={0: 0.1, 1: 0.2}, seed=0)        >>> sampled.groupBy("key").count().orderBy("key").show()        +---+-----+        |key|count|        +---+-----+        |  0|    3|        |  1|    6|        +---+-----+        >>> dataset.sampleBy(col("key"), fractions={2: 1.0}, seed=0).count()        33        .. versionchanged:: 3.0           Added sampling by a column of :class:`Column`
*******__*******
Code:def randomSplit(self, weights, seed=None):\n        """Randomly splits this :class:`DataFrame` with the provided weights.\n\n        :param weights: list of doubles as weights with which to split the DataFrame. Weights will\n            be normalized if they don't sum up to 1.0.\n        :param seed: The seed for sampling.\n\n        >>> splits = df4.randomSplit([1.0, 2.0], 24)\n        >>> splits[0].count()\n        2\n\n        >>> splits[1].count()\n        2\n        """\n        for w in weights:\n            if w < 0.0:\n                raise ValueError("Weights must be positive. Found weight value: %s" % w)\n        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n        rdd_array = self._jdf.randomSplit(_to_list(self.sql_ctx._sc, weights), long(seed))\n        return [DataFrame(rdd, self.sql_ctx) for rdd in rdd_array]
Language: python
Code Tokens: ['def', 'randomSplit', '(', 'self', ',', 'weights', ',', 'seed', '=', 'None', ')', ':', 'for', 'w', 'in', 'weights', ':', 'if', 'w', '<', '0.0', ':', 'raise', 'ValueError', '(', '"Weights must be positive. Found weight value: %s"', '%', 'w', ')', 'seed', '=', 'seed', 'if', 'seed', 'is', 'not', 'None', 'else', 'random', '.', 'randint', '(', '0', ',', 'sys', '.', 'maxsize', ')', 'rdd_array', '=', 'self', '.', '_jdf', '.', 'randomSplit', '(', '_to_list', '(', 'self', '.', 'sql_ctx', '.', '_sc', ',', 'weights', ')', ',', 'long', '(', 'seed', ')', ')', 'return', '[', 'DataFrame', '(', 'rdd', ',', 'self', '.', 'sql_ctx', ')', 'for', 'rdd', 'in', 'rdd_array', ']']
Docstring: Randomly splits this :class:`DataFrame` with the provided weights.        :param weights: list of doubles as weights with which to split the DataFrame. Weights will            be normalized if they don't sum up to 1.0.        :param seed: The seed for sampling.        >>> splits = df4.randomSplit([1.0, 2.0], 24)        >>> splits[0].count()        2        >>> splits[1].count()        2
*******__*******
Code:def dtypes(self):\n        """Returns all column names and their data types as a list.\n\n        >>> df.dtypes\n        [('age', 'int'), ('name', 'string')]\n        """\n        return [(str(f.name), f.dataType.simpleString()) for f in self.schema.fields]
Language: python
Code Tokens: ['def', 'dtypes', '(', 'self', ')', ':', 'return', '[', '(', 'str', '(', 'f', '.', 'name', ')', ',', 'f', '.', 'dataType', '.', 'simpleString', '(', ')', ')', 'for', 'f', 'in', 'self', '.', 'schema', '.', 'fields', ']']
Docstring: Returns all column names and their data types as a list.        >>> df.dtypes        [('age', 'int'), ('name', 'string')]
*******__*******
Code:def colRegex(self, colName):\n        """\n        Selects column based on the column name specified as a regex and returns it\n        as :class:`Column`.\n\n        :param colName: string, column name specified as a regex.\n\n        >>> df = spark.createDataFrame([("a", 1), ("b", 2), ("c",  3)], ["Col1", "Col2"])\n        >>> df.select(df.colRegex("`(Col1)?+.+`")).show()\n        +----+\n        |Col2|\n        +----+\n        |   1|\n        |   2|\n        |   3|\n        +----+\n        """\n        if not isinstance(colName, basestring):\n            raise ValueError("colName should be provided as string")\n        jc = self._jdf.colRegex(colName)\n        return Column(jc)
Language: python
Code Tokens: ['def', 'colRegex', '(', 'self', ',', 'colName', ')', ':', 'if', 'not', 'isinstance', '(', 'colName', ',', 'basestring', ')', ':', 'raise', 'ValueError', '(', '"colName should be provided as string"', ')', 'jc', '=', 'self', '.', '_jdf', '.', 'colRegex', '(', 'colName', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Selects column based on the column name specified as a regex and returns it        as :class:`Column`.        :param colName: string, column name specified as a regex.        >>> df = spark.createDataFrame([("a", 1), ("b", 2), ("c",  3)], ["Col1", "Col2"])        >>> df.select(df.colRegex("`(Col1)?+.+`")).show()        +----+        |Col2|        +----+        |   1|        |   2|        |   3|        +----+
*******__*******
Code:def alias(self, alias):\n        """Returns a new :class:`DataFrame` with an alias set.\n\n        :param alias: string, an alias name to be set for the DataFrame.\n\n        >>> from pyspark.sql.functions import *\n        >>> df_as1 = df.alias("df_as1")\n        >>> df_as2 = df.alias("df_as2")\n        >>> joined_df = df_as1.join(df_as2, col("df_as1.name") == col("df_as2.name"), 'inner')\n        >>> joined_df.select("df_as1.name", "df_as2.name", "df_as2.age").collect()\n        [Row(name=u'Bob', name=u'Bob', age=5), Row(name=u'Alice', name=u'Alice', age=2)]\n        """\n        assert isinstance(alias, basestring), "alias should be a string"\n        return DataFrame(getattr(self._jdf, "as")(alias), self.sql_ctx)
Language: python
Code Tokens: ['def', 'alias', '(', 'self', ',', 'alias', ')', ':', 'assert', 'isinstance', '(', 'alias', ',', 'basestring', ')', ',', '"alias should be a string"', 'return', 'DataFrame', '(', 'getattr', '(', 'self', '.', '_jdf', ',', '"as"', ')', '(', 'alias', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` with an alias set.        :param alias: string, an alias name to be set for the DataFrame.        >>> from pyspark.sql.functions import *        >>> df_as1 = df.alias("df_as1")        >>> df_as2 = df.alias("df_as2")        >>> joined_df = df_as1.join(df_as2, col("df_as1.name") == col("df_as2.name"), 'inner')        >>> joined_df.select("df_as1.name", "df_as2.name", "df_as2.age").collect()        [Row(name=u'Bob', name=u'Bob', age=5), Row(name=u'Alice', name=u'Alice', age=2)]
*******__*******
Code:def crossJoin(self, other):\n        """Returns the cartesian product with another :class:`DataFrame`.\n\n        :param other: Right side of the cartesian product.\n\n        >>> df.select("age", "name").collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        >>> df2.select("name", "height").collect()\n        [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85)]\n        >>> df.crossJoin(df2.select("height")).select("age", "name", "height").collect()\n        [Row(age=2, name=u'Alice', height=80), Row(age=2, name=u'Alice', height=85),\n         Row(age=5, name=u'Bob', height=80), Row(age=5, name=u'Bob', height=85)]\n        """\n\n        jdf = self._jdf.crossJoin(other._jdf)\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'crossJoin', '(', 'self', ',', 'other', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'crossJoin', '(', 'other', '.', '_jdf', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns the cartesian product with another :class:`DataFrame`.        :param other: Right side of the cartesian product.        >>> df.select("age", "name").collect()        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]        >>> df2.select("name", "height").collect()        [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85)]        >>> df.crossJoin(df2.select("height")).select("age", "name", "height").collect()        [Row(age=2, name=u'Alice', height=80), Row(age=2, name=u'Alice', height=85),         Row(age=5, name=u'Bob', height=80), Row(age=5, name=u'Bob', height=85)]
*******__*******
Code:def join(self, other, on=None, how=None):\n        """Joins with another :class:`DataFrame`, using the given join expression.\n\n        :param other: Right side of the join\n        :param on: a string for the join column name, a list of column names,\n            a join expression (Column), or a list of Columns.\n            If `on` is a string or a list of strings indicating the name of the join column(s),\n            the column(s) must exist on both sides, and this performs an equi-join.\n        :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n            ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n            ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n            ``anti``, ``leftanti`` and ``left_anti``.\n\n        The following performs a full outer join between ``df1`` and ``df2``.\n\n        >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()\n        [Row(name=None, height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]\n\n        >>> df.join(df2, 'name', 'outer').select('name', 'height').collect()\n        [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]\n\n        >>> cond = [df.name == df3.name, df.age == df3.age]\n        >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]\n\n        >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n        [Row(name=u'Bob', height=85)]\n\n        >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n        [Row(name=u'Bob', age=5)]\n        """\n\n        if on is not None and not isinstance(on, list):\n            on = [on]\n\n        if on is not None:\n            if isinstance(on[0], basestring):\n                on = self._jseq(on)\n            else:\n                assert isinstance(on[0], Column), "on should be Column or list of Column"\n                on = reduce(lambda x, y: x.__and__(y), on)\n                on = on._jc\n\n        if on is None and how is None:\n            jdf = self._jdf.join(other._jdf)\n        else:\n            if how is None:\n                how = "inner"\n            if on is None:\n                on = self._jseq([])\n            assert isinstance(how, basestring), "how should be basestring"\n            jdf = self._jdf.join(other._jdf, on, how)\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'join', '(', 'self', ',', 'other', ',', 'on', '=', 'None', ',', 'how', '=', 'None', ')', ':', 'if', 'on', 'is', 'not', 'None', 'and', 'not', 'isinstance', '(', 'on', ',', 'list', ')', ':', 'on', '=', '[', 'on', ']', 'if', 'on', 'is', 'not', 'None', ':', 'if', 'isinstance', '(', 'on', '[', '0', ']', ',', 'basestring', ')', ':', 'on', '=', 'self', '.', '_jseq', '(', 'on', ')', 'else', ':', 'assert', 'isinstance', '(', 'on', '[', '0', ']', ',', 'Column', ')', ',', '"on should be Column or list of Column"', 'on', '=', 'reduce', '(', 'lambda', 'x', ',', 'y', ':', 'x', '.', '__and__', '(', 'y', ')', ',', 'on', ')', 'on', '=', 'on', '.', '_jc', 'if', 'on', 'is', 'None', 'and', 'how', 'is', 'None', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'join', '(', 'other', '.', '_jdf', ')', 'else', ':', 'if', 'how', 'is', 'None', ':', 'how', '=', '"inner"', 'if', 'on', 'is', 'None', ':', 'on', '=', 'self', '.', '_jseq', '(', '[', ']', ')', 'assert', 'isinstance', '(', 'how', ',', 'basestring', ')', ',', '"how should be basestring"', 'jdf', '=', 'self', '.', '_jdf', '.', 'join', '(', 'other', '.', '_jdf', ',', 'on', ',', 'how', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Joins with another :class:`DataFrame`, using the given join expression.        :param other: Right side of the join        :param on: a string for the join column name, a list of column names,            a join expression (Column), or a list of Columns.            If `on` is a string or a list of strings indicating the name of the join column(s),            the column(s) must exist on both sides, and this performs an equi-join.        :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,            ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,            ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,            ``anti``, ``leftanti`` and ``left_anti``.        The following performs a full outer join between ``df1`` and ``df2``.        >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()        [Row(name=None, height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]        >>> df.join(df2, 'name', 'outer').select('name', 'height').collect()        [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]        >>> cond = [df.name == df3.name, df.age == df3.age]        >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]        >>> df.join(df2, 'name').select(df.name, df2.height).collect()        [Row(name=u'Bob', height=85)]        >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()        [Row(name=u'Bob', age=5)]
*******__*******
Code:def sortWithinPartitions(self, *cols, **kwargs):\n        """Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n\n        :param cols: list of :class:`Column` or column names to sort by.\n        :param ascending: boolean or list of boolean (default True).\n            Sort ascending vs. descending. Specify list for multiple sort orders.\n            If a list is specified, length of the list must equal length of the `cols`.\n\n        >>> df.sortWithinPartitions("age", ascending=False).show()\n        +---+-----+\n        |age| name|\n        +---+-----+\n        |  2|Alice|\n        |  5|  Bob|\n        +---+-----+\n        """\n        jdf = self._jdf.sortWithinPartitions(self._sort_cols(cols, kwargs))\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'sortWithinPartitions', '(', 'self', ',', '*', 'cols', ',', '*', '*', 'kwargs', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'sortWithinPartitions', '(', 'self', '.', '_sort_cols', '(', 'cols', ',', 'kwargs', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).        :param cols: list of :class:`Column` or column names to sort by.        :param ascending: boolean or list of boolean (default True).            Sort ascending vs. descending. Specify list for multiple sort orders.            If a list is specified, length of the list must equal length of the `cols`.        >>> df.sortWithinPartitions("age", ascending=False).show()        +---+-----+        |age| name|        +---+-----+        |  2|Alice|        |  5|  Bob|        +---+-----+
*******__*******
Code:def _jseq(self, cols, converter=None):\n        """Return a JVM Seq of Columns from a list of Column or names"""\n        return _to_seq(self.sql_ctx._sc, cols, converter)
Language: python
Code Tokens: ['def', '_jseq', '(', 'self', ',', 'cols', ',', 'converter', '=', 'None', ')', ':', 'return', '_to_seq', '(', 'self', '.', 'sql_ctx', '.', '_sc', ',', 'cols', ',', 'converter', ')']
Docstring: Return a JVM Seq of Columns from a list of Column or names
*******__*******
Code:def _jcols(self, *cols):\n        """Return a JVM Seq of Columns from a list of Column or column names\n\n        If `cols` has only one list in it, cols[0] will be used as the list.\n        """\n        if len(cols) == 1 and isinstance(cols[0], list):\n            cols = cols[0]\n        return self._jseq(cols, _to_java_column)
Language: python
Code Tokens: ['def', '_jcols', '(', 'self', ',', '*', 'cols', ')', ':', 'if', 'len', '(', 'cols', ')', '==', '1', 'and', 'isinstance', '(', 'cols', '[', '0', ']', ',', 'list', ')', ':', 'cols', '=', 'cols', '[', '0', ']', 'return', 'self', '.', '_jseq', '(', 'cols', ',', '_to_java_column', ')']
Docstring: Return a JVM Seq of Columns from a list of Column or column names        If `cols` has only one list in it, cols[0] will be used as the list.
*******__*******
Code:def _sort_cols(self, cols, kwargs):\n        """ Return a JVM Seq of Columns that describes the sort order\n        """\n        if not cols:\n            raise ValueError("should sort by at least one column")\n        if len(cols) == 1 and isinstance(cols[0], list):\n            cols = cols[0]\n        jcols = [_to_java_column(c) for c in cols]\n        ascending = kwargs.get('ascending', True)\n        if isinstance(ascending, (bool, int)):\n            if not ascending:\n                jcols = [jc.desc() for jc in jcols]\n        elif isinstance(ascending, list):\n            jcols = [jc if asc else jc.desc()\n                     for asc, jc in zip(ascending, jcols)]\n        else:\n            raise TypeError("ascending can only be boolean or list, but got %s" % type(ascending))\n        return self._jseq(jcols)
Language: python
Code Tokens: ['def', '_sort_cols', '(', 'self', ',', 'cols', ',', 'kwargs', ')', ':', 'if', 'not', 'cols', ':', 'raise', 'ValueError', '(', '"should sort by at least one column"', ')', 'if', 'len', '(', 'cols', ')', '==', '1', 'and', 'isinstance', '(', 'cols', '[', '0', ']', ',', 'list', ')', ':', 'cols', '=', 'cols', '[', '0', ']', 'jcols', '=', '[', '_to_java_column', '(', 'c', ')', 'for', 'c', 'in', 'cols', ']', 'ascending', '=', 'kwargs', '.', 'get', '(', "'ascending'", ',', 'True', ')', 'if', 'isinstance', '(', 'ascending', ',', '(', 'bool', ',', 'int', ')', ')', ':', 'if', 'not', 'ascending', ':', 'jcols', '=', '[', 'jc', '.', 'desc', '(', ')', 'for', 'jc', 'in', 'jcols', ']', 'elif', 'isinstance', '(', 'ascending', ',', 'list', ')', ':', 'jcols', '=', '[', 'jc', 'if', 'asc', 'else', 'jc', '.', 'desc', '(', ')', 'for', 'asc', ',', 'jc', 'in', 'zip', '(', 'ascending', ',', 'jcols', ')', ']', 'else', ':', 'raise', 'TypeError', '(', '"ascending can only be boolean or list, but got %s"', '%', 'type', '(', 'ascending', ')', ')', 'return', 'self', '.', '_jseq', '(', 'jcols', ')']
Docstring: Return a JVM Seq of Columns that describes the sort order
*******__*******
Code:def describe(self, *cols):\n        """Computes basic statistics for numeric and string columns.\n\n        This include count, mean, stddev, min, and max. If no columns are\n        given, this function computes statistics for all numerical or string columns.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n        >>> df.describe(['age']).show()\n        +-------+------------------+\n        |summary|               age|\n        +-------+------------------+\n        |  count|                 2|\n        |   mean|               3.5|\n        | stddev|2.1213203435596424|\n        |    min|                 2|\n        |    max|                 5|\n        +-------+------------------+\n        >>> df.describe().show()\n        +-------+------------------+-----+\n        |summary|               age| name|\n        +-------+------------------+-----+\n        |  count|                 2|    2|\n        |   mean|               3.5| null|\n        | stddev|2.1213203435596424| null|\n        |    min|                 2|Alice|\n        |    max|                 5|  Bob|\n        +-------+------------------+-----+\n\n        Use summary for expanded statistics and control over which statistics to compute.\n        """\n        if len(cols) == 1 and isinstance(cols[0], list):\n            cols = cols[0]\n        jdf = self._jdf.describe(self._jseq(cols))\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'describe', '(', 'self', ',', '*', 'cols', ')', ':', 'if', 'len', '(', 'cols', ')', '==', '1', 'and', 'isinstance', '(', 'cols', '[', '0', ']', ',', 'list', ')', ':', 'cols', '=', 'cols', '[', '0', ']', 'jdf', '=', 'self', '.', '_jdf', '.', 'describe', '(', 'self', '.', '_jseq', '(', 'cols', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Computes basic statistics for numeric and string columns.        This include count, mean, stddev, min, and max. If no columns are        given, this function computes statistics for all numerical or string columns.        .. note:: This function is meant for exploratory data analysis, as we make no            guarantee about the backward compatibility of the schema of the resulting DataFrame.        >>> df.describe(['age']).show()        +-------+------------------+        |summary|               age|        +-------+------------------+        |  count|                 2|        |   mean|               3.5|        | stddev|2.1213203435596424|        |    min|                 2|        |    max|                 5|        +-------+------------------+        >>> df.describe().show()        +-------+------------------+-----+        |summary|               age| name|        +-------+------------------+-----+        |  count|                 2|    2|        |   mean|               3.5| null|        | stddev|2.1213203435596424| null|        |    min|                 2|Alice|        |    max|                 5|  Bob|        +-------+------------------+-----+        Use summary for expanded statistics and control over which statistics to compute.
*******__*******
Code:def summary(self, *statistics):\n        """Computes specified statistics for numeric and string columns. Available statistics are:\n        - count\n        - mean\n        - stddev\n        - min\n        - max\n        - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n\n        If no statistics are given, this function computes count, mean, stddev, min,\n        approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n        >>> df.summary().show()\n        +-------+------------------+-----+\n        |summary|               age| name|\n        +-------+------------------+-----+\n        |  count|                 2|    2|\n        |   mean|               3.5| null|\n        | stddev|2.1213203435596424| null|\n        |    min|                 2|Alice|\n        |    25%|                 2| null|\n        |    50%|                 2| null|\n        |    75%|                 5| null|\n        |    max|                 5|  Bob|\n        +-------+------------------+-----+\n\n        >>> df.summary("count", "min", "25%", "75%", "max").show()\n        +-------+---+-----+\n        |summary|age| name|\n        +-------+---+-----+\n        |  count|  2|    2|\n        |    min|  2|Alice|\n        |    25%|  2| null|\n        |    75%|  5| null|\n        |    max|  5|  Bob|\n        +-------+---+-----+\n\n        To do a summary for specific columns first select them:\n\n        >>> df.select("age", "name").summary("count").show()\n        +-------+---+----+\n        |summary|age|name|\n        +-------+---+----+\n        |  count|  2|   2|\n        +-------+---+----+\n\n        See also describe for basic statistics.\n        """\n        if len(statistics) == 1 and isinstance(statistics[0], list):\n            statistics = statistics[0]\n        jdf = self._jdf.summary(self._jseq(statistics))\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'summary', '(', 'self', ',', '*', 'statistics', ')', ':', 'if', 'len', '(', 'statistics', ')', '==', '1', 'and', 'isinstance', '(', 'statistics', '[', '0', ']', ',', 'list', ')', ':', 'statistics', '=', 'statistics', '[', '0', ']', 'jdf', '=', 'self', '.', '_jdf', '.', 'summary', '(', 'self', '.', '_jseq', '(', 'statistics', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Computes specified statistics for numeric and string columns. Available statistics are:        - count        - mean        - stddev        - min        - max        - arbitrary approximate percentiles specified as a percentage (eg, 75%)        If no statistics are given, this function computes count, mean, stddev, min,        approximate quartiles (percentiles at 25%, 50%, and 75%), and max.        .. note:: This function is meant for exploratory data analysis, as we make no            guarantee about the backward compatibility of the schema of the resulting DataFrame.        >>> df.summary().show()        +-------+------------------+-----+        |summary|               age| name|        +-------+------------------+-----+        |  count|                 2|    2|        |   mean|               3.5| null|        | stddev|2.1213203435596424| null|        |    min|                 2|Alice|        |    25%|                 2| null|        |    50%|                 2| null|        |    75%|                 5| null|        |    max|                 5|  Bob|        +-------+------------------+-----+        >>> df.summary("count", "min", "25%", "75%", "max").show()        +-------+---+-----+        |summary|age| name|        +-------+---+-----+        |  count|  2|    2|        |    min|  2|Alice|        |    25%|  2| null|        |    75%|  5| null|        |    max|  5|  Bob|        +-------+---+-----+        To do a summary for specific columns first select them:        >>> df.select("age", "name").summary("count").show()        +-------+---+----+        |summary|age|name|        +-------+---+----+        |  count|  2|   2|        +-------+---+----+        See also describe for basic statistics.
*******__*******
Code:def head(self, n=None):\n        """Returns the first ``n`` rows.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        :param n: int, default 1. Number of rows to return.\n        :return: If n is greater than 1, return a list of :class:`Row`.\n            If n is 1, return a single Row.\n\n        >>> df.head()\n        Row(age=2, name=u'Alice')\n        >>> df.head(1)\n        [Row(age=2, name=u'Alice')]\n        """\n        if n is None:\n            rs = self.head(1)\n            return rs[0] if rs else None\n        return self.take(n)
Language: python
Code Tokens: ['def', 'head', '(', 'self', ',', 'n', '=', 'None', ')', ':', 'if', 'n', 'is', 'None', ':', 'rs', '=', 'self', '.', 'head', '(', '1', ')', 'return', 'rs', '[', '0', ']', 'if', 'rs', 'else', 'None', 'return', 'self', '.', 'take', '(', 'n', ')']
Docstring: Returns the first ``n`` rows.        .. note:: This method should only be used if the resulting array is expected            to be small, as all the data is loaded into the driver's memory.        :param n: int, default 1. Number of rows to return.        :return: If n is greater than 1, return a list of :class:`Row`.            If n is 1, return a single Row.        >>> df.head()        Row(age=2, name=u'Alice')        >>> df.head(1)        [Row(age=2, name=u'Alice')]
*******__*******
Code:def select(self, *cols):\n        """Projects a set of expressions and returns a new :class:`DataFrame`.\n\n        :param cols: list of column names (string) or expressions (:class:`Column`).\n            If one of the column names is '*', that column is expanded to include all columns\n            in the current DataFrame.\n\n        >>> df.select('*').collect()\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\n        >>> df.select('name', 'age').collect()\n        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]\n        >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n        [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\n        """\n        jdf = self._jdf.select(self._jcols(*cols))\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'select', '(', 'self', ',', '*', 'cols', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'select', '(', 'self', '.', '_jcols', '(', '*', 'cols', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Projects a set of expressions and returns a new :class:`DataFrame`.        :param cols: list of column names (string) or expressions (:class:`Column`).            If one of the column names is '*', that column is expanded to include all columns            in the current DataFrame.        >>> df.select('*').collect()        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]        >>> df.select('name', 'age').collect()        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]        >>> df.select(df.name, (df.age + 10).alias('age')).collect()        [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]
*******__*******
Code:def selectExpr(self, *expr):\n        """Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n\n        This is a variant of :func:`select` that accepts SQL expressions.\n\n        >>> df.selectExpr("age * 2", "abs(age)").collect()\n        [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n        """\n        if len(expr) == 1 and isinstance(expr[0], list):\n            expr = expr[0]\n        jdf = self._jdf.selectExpr(self._jseq(expr))\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'selectExpr', '(', 'self', ',', '*', 'expr', ')', ':', 'if', 'len', '(', 'expr', ')', '==', '1', 'and', 'isinstance', '(', 'expr', '[', '0', ']', ',', 'list', ')', ':', 'expr', '=', 'expr', '[', '0', ']', 'jdf', '=', 'self', '.', '_jdf', '.', 'selectExpr', '(', 'self', '.', '_jseq', '(', 'expr', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Projects a set of SQL expressions and returns a new :class:`DataFrame`.        This is a variant of :func:`select` that accepts SQL expressions.        >>> df.selectExpr("age * 2", "abs(age)").collect()        [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]
*******__*******
Code:def filter(self, condition):\n        """Filters rows using the given condition.\n\n        :func:`where` is an alias for :func:`filter`.\n\n        :param condition: a :class:`Column` of :class:`types.BooleanType`\n            or a string of SQL expression.\n\n        >>> df.filter(df.age > 3).collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df.where(df.age == 2).collect()\n        [Row(age=2, name=u'Alice')]\n\n        >>> df.filter("age > 3").collect()\n        [Row(age=5, name=u'Bob')]\n        >>> df.where("age = 2").collect()\n        [Row(age=2, name=u'Alice')]\n        """\n        if isinstance(condition, basestring):\n            jdf = self._jdf.filter(condition)\n        elif isinstance(condition, Column):\n            jdf = self._jdf.filter(condition._jc)\n        else:\n            raise TypeError("condition should be string or Column")\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'filter', '(', 'self', ',', 'condition', ')', ':', 'if', 'isinstance', '(', 'condition', ',', 'basestring', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'filter', '(', 'condition', ')', 'elif', 'isinstance', '(', 'condition', ',', 'Column', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'filter', '(', 'condition', '.', '_jc', ')', 'else', ':', 'raise', 'TypeError', '(', '"condition should be string or Column"', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Filters rows using the given condition.        :func:`where` is an alias for :func:`filter`.        :param condition: a :class:`Column` of :class:`types.BooleanType`            or a string of SQL expression.        >>> df.filter(df.age > 3).collect()        [Row(age=5, name=u'Bob')]        >>> df.where(df.age == 2).collect()        [Row(age=2, name=u'Alice')]        >>> df.filter("age > 3").collect()        [Row(age=5, name=u'Bob')]        >>> df.where("age = 2").collect()        [Row(age=2, name=u'Alice')]
*******__*******
Code:def groupBy(self, *cols):\n        """Groups the :class:`DataFrame` using the specified columns,\n        so we can run aggregation on them. See :class:`GroupedData`\n        for all the available aggregate functions.\n\n        :func:`groupby` is an alias for :func:`groupBy`.\n\n        :param cols: list of columns to group by.\n            Each element should be a column name (string) or an expression (:class:`Column`).\n\n        >>> df.groupBy().avg().collect()\n        [Row(avg(age)=3.5)]\n        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]\n        >>> sorted(df.groupBy(df.name).avg().collect())\n        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]\n        >>> sorted(df.groupBy(['name', df.age]).count().collect())\n        [Row(name=u'Alice', age=2, count=1), Row(name=u'Bob', age=5, count=1)]\n        """\n        jgd = self._jdf.groupBy(self._jcols(*cols))\n        from pyspark.sql.group import GroupedData\n        return GroupedData(jgd, self)
Language: python
Code Tokens: ['def', 'groupBy', '(', 'self', ',', '*', 'cols', ')', ':', 'jgd', '=', 'self', '.', '_jdf', '.', 'groupBy', '(', 'self', '.', '_jcols', '(', '*', 'cols', ')', ')', 'from', 'pyspark', '.', 'sql', '.', 'group', 'import', 'GroupedData', 'return', 'GroupedData', '(', 'jgd', ',', 'self', ')']
Docstring: Groups the :class:`DataFrame` using the specified columns,        so we can run aggregation on them. See :class:`GroupedData`        for all the available aggregate functions.        :func:`groupby` is an alias for :func:`groupBy`.        :param cols: list of columns to group by.            Each element should be a column name (string) or an expression (:class:`Column`).        >>> df.groupBy().avg().collect()        [Row(avg(age)=3.5)]        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]        >>> sorted(df.groupBy(df.name).avg().collect())        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]        >>> sorted(df.groupBy(['name', df.age]).count().collect())        [Row(name=u'Alice', age=2, count=1), Row(name=u'Bob', age=5, count=1)]
*******__*******
Code:def union(self, other):\n        """ Return a new :class:`DataFrame` containing union of rows in this and another frame.\n\n        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n        (that does deduplication of elements), use this function followed by :func:`distinct`.\n\n        Also as standard in SQL, this function resolves columns by position (not by name).\n        """\n        return DataFrame(self._jdf.union(other._jdf), self.sql_ctx)
Language: python
Code Tokens: ['def', 'union', '(', 'self', ',', 'other', ')', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'union', '(', 'other', '.', '_jdf', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Return a new :class:`DataFrame` containing union of rows in this and another frame.        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union        (that does deduplication of elements), use this function followed by :func:`distinct`.        Also as standard in SQL, this function resolves columns by position (not by name).
*******__*******
Code:def unionByName(self, other):\n        """ Returns a new :class:`DataFrame` containing union of rows in this and another frame.\n\n        This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n        union (that does deduplication of elements), use this function followed by :func:`distinct`.\n\n        The difference between this function and :func:`union` is that this function\n        resolves columns by name (not by position):\n\n        >>> df1 = spark.createDataFrame([[1, 2, 3]], ["col0", "col1", "col2"])\n        >>> df2 = spark.createDataFrame([[4, 5, 6]], ["col1", "col2", "col0"])\n        >>> df1.unionByName(df2).show()\n        +----+----+----+\n        |col0|col1|col2|\n        +----+----+----+\n        |   1|   2|   3|\n        |   6|   4|   5|\n        +----+----+----+\n        """\n        return DataFrame(self._jdf.unionByName(other._jdf), self.sql_ctx)
Language: python
Code Tokens: ['def', 'unionByName', '(', 'self', ',', 'other', ')', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'unionByName', '(', 'other', '.', '_jdf', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` containing union of rows in this and another frame.        This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set        union (that does deduplication of elements), use this function followed by :func:`distinct`.        The difference between this function and :func:`union` is that this function        resolves columns by name (not by position):        >>> df1 = spark.createDataFrame([[1, 2, 3]], ["col0", "col1", "col2"])        >>> df2 = spark.createDataFrame([[4, 5, 6]], ["col1", "col2", "col0"])        >>> df1.unionByName(df2).show()        +----+----+----+        |col0|col1|col2|        +----+----+----+        |   1|   2|   3|        |   6|   4|   5|        +----+----+----+
*******__*******
Code:def intersect(self, other):\n        """ Return a new :class:`DataFrame` containing rows only in\n        both this frame and another frame.\n\n        This is equivalent to `INTERSECT` in SQL.\n        """\n        return DataFrame(self._jdf.intersect(other._jdf), self.sql_ctx)
Language: python
Code Tokens: ['def', 'intersect', '(', 'self', ',', 'other', ')', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'intersect', '(', 'other', '.', '_jdf', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Return a new :class:`DataFrame` containing rows only in        both this frame and another frame.        This is equivalent to `INTERSECT` in SQL.
*******__*******
Code:def intersectAll(self, other):\n        """ Return a new :class:`DataFrame` containing rows in both this dataframe and other\n        dataframe while preserving duplicates.\n\n        This is equivalent to `INTERSECT ALL` in SQL.\n        >>> df1 = spark.createDataFrame([("a", 1), ("a", 1), ("b", 3), ("c", 4)], ["C1", "C2"])\n        >>> df2 = spark.createDataFrame([("a", 1), ("a", 1), ("b", 3)], ["C1", "C2"])\n\n        >>> df1.intersectAll(df2).sort("C1", "C2").show()\n        +---+---+\n        | C1| C2|\n        +---+---+\n        |  a|  1|\n        |  a|  1|\n        |  b|  3|\n        +---+---+\n\n        Also as standard in SQL, this function resolves columns by position (not by name).\n        """\n        return DataFrame(self._jdf.intersectAll(other._jdf), self.sql_ctx)
Language: python
Code Tokens: ['def', 'intersectAll', '(', 'self', ',', 'other', ')', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'intersectAll', '(', 'other', '.', '_jdf', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Return a new :class:`DataFrame` containing rows in both this dataframe and other        dataframe while preserving duplicates.        This is equivalent to `INTERSECT ALL` in SQL.        >>> df1 = spark.createDataFrame([("a", 1), ("a", 1), ("b", 3), ("c", 4)], ["C1", "C2"])        >>> df2 = spark.createDataFrame([("a", 1), ("a", 1), ("b", 3)], ["C1", "C2"])        >>> df1.intersectAll(df2).sort("C1", "C2").show()        +---+---+        | C1| C2|        +---+---+        |  a|  1|        |  a|  1|        |  b|  3|        +---+---+        Also as standard in SQL, this function resolves columns by position (not by name).
*******__*******
Code:def subtract(self, other):\n        """ Return a new :class:`DataFrame` containing rows in this frame\n        but not in another frame.\n\n        This is equivalent to `EXCEPT DISTINCT` in SQL.\n\n        """\n        return DataFrame(getattr(self._jdf, "except")(other._jdf), self.sql_ctx)
Language: python
Code Tokens: ['def', 'subtract', '(', 'self', ',', 'other', ')', ':', 'return', 'DataFrame', '(', 'getattr', '(', 'self', '.', '_jdf', ',', '"except"', ')', '(', 'other', '.', '_jdf', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Return a new :class:`DataFrame` containing rows in this frame        but not in another frame.        This is equivalent to `EXCEPT DISTINCT` in SQL.
*******__*******
Code:def dropDuplicates(self, subset=None):\n        """Return a new :class:`DataFrame` with duplicate rows removed,\n        optionally only considering certain columns.\n\n        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n        be and system will accordingly limit the state. In addition, too late data older than\n        watermark will be dropped to avoid any possibility of duplicates.\n\n        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n\n        >>> from pyspark.sql import Row\n        >>> df = sc.parallelize([ \\\n        ...     Row(name='Alice', age=5, height=80), \\\n        ...     Row(name='Alice', age=5, height=80), \\\n        ...     Row(name='Alice', age=10, height=80)]).toDF()\n        >>> df.dropDuplicates().show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        |  5|    80|Alice|\n        | 10|    80|Alice|\n        +---+------+-----+\n\n        >>> df.dropDuplicates(['name', 'height']).show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        |  5|    80|Alice|\n        +---+------+-----+\n        """\n        if subset is None:\n            jdf = self._jdf.dropDuplicates()\n        else:\n            jdf = self._jdf.dropDuplicates(self._jseq(subset))\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'dropDuplicates', '(', 'self', ',', 'subset', '=', 'None', ')', ':', 'if', 'subset', 'is', 'None', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'dropDuplicates', '(', ')', 'else', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'dropDuplicates', '(', 'self', '.', '_jseq', '(', 'subset', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Return a new :class:`DataFrame` with duplicate rows removed,        optionally only considering certain columns.        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can        be and system will accordingly limit the state. In addition, too late data older than        watermark will be dropped to avoid any possibility of duplicates.        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.        >>> from pyspark.sql import Row        >>> df = sc.parallelize([ \\        ...     Row(name='Alice', age=5, height=80), \\        ...     Row(name='Alice', age=5, height=80), \\        ...     Row(name='Alice', age=10, height=80)]).toDF()        >>> df.dropDuplicates().show()        +---+------+-----+        |age|height| name|        +---+------+-----+        |  5|    80|Alice|        | 10|    80|Alice|        +---+------+-----+        >>> df.dropDuplicates(['name', 'height']).show()        +---+------+-----+        |age|height| name|        +---+------+-----+        |  5|    80|Alice|        +---+------+-----+
*******__*******
Code:def dropna(self, how='any', thresh=None, subset=None):\n        """Returns a new :class:`DataFrame` omitting rows with null values.\n        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n\n        :param how: 'any' or 'all'.\n            If 'any', drop a row if it contains any nulls.\n            If 'all', drop a row only if all its values are null.\n        :param thresh: int, default None\n            If specified, drop rows that have less than `thresh` non-null values.\n            This overwrites the `how` parameter.\n        :param subset: optional list of column names to consider.\n\n        >>> df4.na.drop().show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        | 10|    80|Alice|\n        +---+------+-----+\n        """\n        if how is not None and how not in ['any', 'all']:\n            raise ValueError("how ('" + how + "') should be 'any' or 'all'")\n\n        if subset is None:\n            subset = self.columns\n        elif isinstance(subset, basestring):\n            subset = [subset]\n        elif not isinstance(subset, (list, tuple)):\n            raise ValueError("subset should be a list or tuple of column names")\n\n        if thresh is None:\n            thresh = len(subset) if how == 'any' else 1\n\n        return DataFrame(self._jdf.na().drop(thresh, self._jseq(subset)), self.sql_ctx)
Language: python
Code Tokens: ['def', 'dropna', '(', 'self', ',', 'how', '=', "'any'", ',', 'thresh', '=', 'None', ',', 'subset', '=', 'None', ')', ':', 'if', 'how', 'is', 'not', 'None', 'and', 'how', 'not', 'in', '[', "'any'", ',', "'all'", ']', ':', 'raise', 'ValueError', '(', '"how (\'"', '+', 'how', '+', '"\') should be \'any\' or \'all\'"', ')', 'if', 'subset', 'is', 'None', ':', 'subset', '=', 'self', '.', 'columns', 'elif', 'isinstance', '(', 'subset', ',', 'basestring', ')', ':', 'subset', '=', '[', 'subset', ']', 'elif', 'not', 'isinstance', '(', 'subset', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'raise', 'ValueError', '(', '"subset should be a list or tuple of column names"', ')', 'if', 'thresh', 'is', 'None', ':', 'thresh', '=', 'len', '(', 'subset', ')', 'if', 'how', '==', "'any'", 'else', '1', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'na', '(', ')', '.', 'drop', '(', 'thresh', ',', 'self', '.', '_jseq', '(', 'subset', ')', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` omitting rows with null values.        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.        :param how: 'any' or 'all'.            If 'any', drop a row if it contains any nulls.            If 'all', drop a row only if all its values are null.        :param thresh: int, default None            If specified, drop rows that have less than `thresh` non-null values.            This overwrites the `how` parameter.        :param subset: optional list of column names to consider.        >>> df4.na.drop().show()        +---+------+-----+        |age|height| name|        +---+------+-----+        | 10|    80|Alice|        +---+------+-----+
*******__*******
Code:def fillna(self, value, subset=None):\n        """Replace null values, alias for ``na.fill()``.\n        :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n\n        :param value: int, long, float, string, bool or dict.\n            Value to replace null values with.\n            If the value is a dict, then `subset` is ignored and `value` must be a mapping\n            from column name (string) to replacement value. The replacement value must be\n            an int, long, float, boolean, or string.\n        :param subset: optional list of column names to consider.\n            Columns specified in subset that do not have matching data type are ignored.\n            For example, if `value` is a string, and subset contains a non-string column,\n            then the non-string column is simply ignored.\n\n        >>> df4.na.fill(50).show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        | 10|    80|Alice|\n        |  5|    50|  Bob|\n        | 50|    50|  Tom|\n        | 50|    50| null|\n        +---+------+-----+\n\n        >>> df5.na.fill(False).show()\n        +----+-------+-----+\n        | age|   name|  spy|\n        +----+-------+-----+\n        |  10|  Alice|false|\n        |   5|    Bob|false|\n        |null|Mallory| true|\n        +----+-------+-----+\n\n        >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n        +---+------+-------+\n        |age|height|   name|\n        +---+------+-------+\n        | 10|    80|  Alice|\n        |  5|  null|    Bob|\n        | 50|  null|    Tom|\n        | 50|  null|unknown|\n        +---+------+-------+\n        """\n        if not isinstance(value, (float, int, long, basestring, bool, dict)):\n            raise ValueError("value should be a float, int, long, string, bool or dict")\n\n        # Note that bool validates isinstance(int), but we don't want to\n        # convert bools to floats\n\n        if not isinstance(value, bool) and isinstance(value, (int, long)):\n            value = float(value)\n\n        if isinstance(value, dict):\n            return DataFrame(self._jdf.na().fill(value), self.sql_ctx)\n        elif subset is None:\n            return DataFrame(self._jdf.na().fill(value), self.sql_ctx)\n        else:\n            if isinstance(subset, basestring):\n                subset = [subset]\n            elif not isinstance(subset, (list, tuple)):\n                raise ValueError("subset should be a list or tuple of column names")\n\n            return DataFrame(self._jdf.na().fill(value, self._jseq(subset)), self.sql_ctx)
Language: python
Code Tokens: ['def', 'fillna', '(', 'self', ',', 'value', ',', 'subset', '=', 'None', ')', ':', 'if', 'not', 'isinstance', '(', 'value', ',', '(', 'float', ',', 'int', ',', 'long', ',', 'basestring', ',', 'bool', ',', 'dict', ')', ')', ':', 'raise', 'ValueError', '(', '"value should be a float, int, long, string, bool or dict"', ')', "# Note that bool validates isinstance(int), but we don't want to", '# convert bools to floats', 'if', 'not', 'isinstance', '(', 'value', ',', 'bool', ')', 'and', 'isinstance', '(', 'value', ',', '(', 'int', ',', 'long', ')', ')', ':', 'value', '=', 'float', '(', 'value', ')', 'if', 'isinstance', '(', 'value', ',', 'dict', ')', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'na', '(', ')', '.', 'fill', '(', 'value', ')', ',', 'self', '.', 'sql_ctx', ')', 'elif', 'subset', 'is', 'None', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'na', '(', ')', '.', 'fill', '(', 'value', ')', ',', 'self', '.', 'sql_ctx', ')', 'else', ':', 'if', 'isinstance', '(', 'subset', ',', 'basestring', ')', ':', 'subset', '=', '[', 'subset', ']', 'elif', 'not', 'isinstance', '(', 'subset', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'raise', 'ValueError', '(', '"subset should be a list or tuple of column names"', ')', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'na', '(', ')', '.', 'fill', '(', 'value', ',', 'self', '.', '_jseq', '(', 'subset', ')', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Replace null values, alias for ``na.fill()``.        :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.        :param value: int, long, float, string, bool or dict.            Value to replace null values with.            If the value is a dict, then `subset` is ignored and `value` must be a mapping            from column name (string) to replacement value. The replacement value must be            an int, long, float, boolean, or string.        :param subset: optional list of column names to consider.            Columns specified in subset that do not have matching data type are ignored.            For example, if `value` is a string, and subset contains a non-string column,            then the non-string column is simply ignored.        >>> df4.na.fill(50).show()        +---+------+-----+        |age|height| name|        +---+------+-----+        | 10|    80|Alice|        |  5|    50|  Bob|        | 50|    50|  Tom|        | 50|    50| null|        +---+------+-----+        >>> df5.na.fill(False).show()        +----+-------+-----+        | age|   name|  spy|        +----+-------+-----+        |  10|  Alice|false|        |   5|    Bob|false|        |null|Mallory| true|        +----+-------+-----+        >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()        +---+------+-------+        |age|height|   name|        +---+------+-------+        | 10|    80|  Alice|        |  5|  null|    Bob|        | 50|  null|    Tom|        | 50|  null|unknown|        +---+------+-------+
*******__*******
Code:def replace(self, to_replace, value=_NoValue, subset=None):\n        """Returns a new :class:`DataFrame` replacing a value with another value.\n        :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n        aliases of each other.\n        Values to_replace and value must have the same type and can only be numerics, booleans,\n        or strings. Value can have None. When replacing, the new value will be cast\n        to the type of the existing column.\n        For numeric replacements all values to be replaced should have unique\n        floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n        and arbitrary replacement will be used.\n\n        :param to_replace: bool, int, long, float, string, list or dict.\n            Value to be replaced.\n            If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n            must be a mapping between a value and a replacement.\n        :param value: bool, int, long, float, string, list or None.\n            The replacement value must be a bool, int, long, float, string or None. If `value` is a\n            list, `value` should be of the same length and type as `to_replace`.\n            If `value` is a scalar and `to_replace` is a sequence, then `value` is\n            used as a replacement for each item in `to_replace`.\n        :param subset: optional list of column names to consider.\n            Columns specified in subset that do not have matching data type are ignored.\n            For example, if `value` is a string, and subset contains a non-string column,\n            then the non-string column is simply ignored.\n\n        >>> df4.na.replace(10, 20).show()\n        +----+------+-----+\n        | age|height| name|\n        +----+------+-----+\n        |  20|    80|Alice|\n        |   5|  null|  Bob|\n        |null|  null|  Tom|\n        |null|  null| null|\n        +----+------+-----+\n\n        >>> df4.na.replace('Alice', None).show()\n        +----+------+----+\n        | age|height|name|\n        +----+------+----+\n        |  10|    80|null|\n        |   5|  null| Bob|\n        |null|  null| Tom|\n        |null|  null|null|\n        +----+------+----+\n\n        >>> df4.na.replace({'Alice': None}).show()\n        +----+------+----+\n        | age|height|name|\n        +----+------+----+\n        |  10|    80|null|\n        |   5|  null| Bob|\n        |null|  null| Tom|\n        |null|  null|null|\n        +----+------+----+\n\n        >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n        +----+------+----+\n        | age|height|name|\n        +----+------+----+\n        |  10|    80|   A|\n        |   5|  null|   B|\n        |null|  null| Tom|\n        |null|  null|null|\n        +----+------+----+\n        """\n        if value is _NoValue:\n            if isinstance(to_replace, dict):\n                value = None\n            else:\n                raise TypeError("value argument is required when to_replace is not a dictionary.")\n\n        # Helper functions\n        def all_of(types):\n            """Given a type or tuple of types and a sequence of xs\n            check if each x is instance of type(s)\n\n            >>> all_of(bool)([True, False])\n            True\n            >>> all_of(basestring)(["a", 1])\n            False\n            """\n            def all_of_(xs):\n                return all(isinstance(x, types) for x in xs)\n            return all_of_\n\n        all_of_bool = all_of(bool)\n        all_of_str = all_of(basestring)\n        all_of_numeric = all_of((float, int, long))\n\n        # Validate input types\n        valid_types = (bool, float, int, long, basestring, list, tuple)\n        if not isinstance(to_replace, valid_types + (dict, )):\n            raise ValueError(\n                "to_replace should be a bool, float, int, long, string, list, tuple, or dict. "\n                "Got {0}".format(type(to_replace)))\n\n        if not isinstance(value, valid_types) and value is not None \\n                and not isinstance(to_replace, dict):\n            raise ValueError("If to_replace is not a dict, value should be "\n                             "a bool, float, int, long, string, list, tuple or None. "\n                             "Got {0}".format(type(value)))\n\n        if isinstance(to_replace, (list, tuple)) and isinstance(value, (list, tuple)):\n            if len(to_replace) != len(value):\n                raise ValueError("to_replace and value lists should be of the same length. "\n                                 "Got {0} and {1}".format(len(to_replace), len(value)))\n\n        if not (subset is None or isinstance(subset, (list, tuple, basestring))):\n            raise ValueError("subset should be a list or tuple of column names, "\n                             "column name or None. Got {0}".format(type(subset)))\n\n        # Reshape input arguments if necessary\n        if isinstance(to_replace, (float, int, long, basestring)):\n            to_replace = [to_replace]\n\n        if isinstance(to_replace, dict):\n            rep_dict = to_replace\n            if value is not None:\n                warnings.warn("to_replace is a dict and value is not None. value will be ignored.")\n        else:\n            if isinstance(value, (float, int, long, basestring)) or value is None:\n                value = [value for _ in range(len(to_replace))]\n            rep_dict = dict(zip(to_replace, value))\n\n        if isinstance(subset, basestring):\n            subset = [subset]\n\n        # Verify we were not passed in mixed type generics.\n        if not any(all_of_type(rep_dict.keys())\n                   and all_of_type(x for x in rep_dict.values() if x is not None)\n                   for all_of_type in [all_of_bool, all_of_str, all_of_numeric]):\n            raise ValueError("Mixed type replacements are not supported")\n\n        if subset is None:\n            return DataFrame(self._jdf.na().replace('*', rep_dict), self.sql_ctx)\n        else:\n            return DataFrame(\n                self._jdf.na().replace(self._jseq(subset), self._jmap(rep_dict)), self.sql_ctx)
Language: python
Code Tokens: ['def', 'replace', '(', 'self', ',', 'to_replace', ',', 'value', '=', '_NoValue', ',', 'subset', '=', 'None', ')', ':', 'if', 'value', 'is', '_NoValue', ':', 'if', 'isinstance', '(', 'to_replace', ',', 'dict', ')', ':', 'value', '=', 'None', 'else', ':', 'raise', 'TypeError', '(', '"value argument is required when to_replace is not a dictionary."', ')', '# Helper functions', 'def', 'all_of', '(', 'types', ')', ':', '"""Given a type or tuple of types and a sequence of xs\n            check if each x is instance of type(s)\n\n            >>> all_of(bool)([True, False])\n            True\n            >>> all_of(basestring)(["a", 1])\n            False\n            """', 'def', 'all_of_', '(', 'xs', ')', ':', 'return', 'all', '(', 'isinstance', '(', 'x', ',', 'types', ')', 'for', 'x', 'in', 'xs', ')', 'return', 'all_of_', 'all_of_bool', '=', 'all_of', '(', 'bool', ')', 'all_of_str', '=', 'all_of', '(', 'basestring', ')', 'all_of_numeric', '=', 'all_of', '(', '(', 'float', ',', 'int', ',', 'long', ')', ')', '# Validate input types', 'valid_types', '=', '(', 'bool', ',', 'float', ',', 'int', ',', 'long', ',', 'basestring', ',', 'list', ',', 'tuple', ')', 'if', 'not', 'isinstance', '(', 'to_replace', ',', 'valid_types', '+', '(', 'dict', ',', ')', ')', ':', 'raise', 'ValueError', '(', '"to_replace should be a bool, float, int, long, string, list, tuple, or dict. "', '"Got {0}"', '.', 'format', '(', 'type', '(', 'to_replace', ')', ')', ')', 'if', 'not', 'isinstance', '(', 'value', ',', 'valid_types', ')', 'and', 'value', 'is', 'not', 'None', 'and', 'not', 'isinstance', '(', 'to_replace', ',', 'dict', ')', ':', 'raise', 'ValueError', '(', '"If to_replace is not a dict, value should be "', '"a bool, float, int, long, string, list, tuple or None. "', '"Got {0}"', '.', 'format', '(', 'type', '(', 'value', ')', ')', ')', 'if', 'isinstance', '(', 'to_replace', ',', '(', 'list', ',', 'tuple', ')', ')', 'and', 'isinstance', '(', 'value', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'if', 'len', '(', 'to_replace', ')', '!=', 'len', '(', 'value', ')', ':', 'raise', 'ValueError', '(', '"to_replace and value lists should be of the same length. "', '"Got {0} and {1}"', '.', 'format', '(', 'len', '(', 'to_replace', ')', ',', 'len', '(', 'value', ')', ')', ')', 'if', 'not', '(', 'subset', 'is', 'None', 'or', 'isinstance', '(', 'subset', ',', '(', 'list', ',', 'tuple', ',', 'basestring', ')', ')', ')', ':', 'raise', 'ValueError', '(', '"subset should be a list or tuple of column names, "', '"column name or None. Got {0}"', '.', 'format', '(', 'type', '(', 'subset', ')', ')', ')', '# Reshape input arguments if necessary', 'if', 'isinstance', '(', 'to_replace', ',', '(', 'float', ',', 'int', ',', 'long', ',', 'basestring', ')', ')', ':', 'to_replace', '=', '[', 'to_replace', ']', 'if', 'isinstance', '(', 'to_replace', ',', 'dict', ')', ':', 'rep_dict', '=', 'to_replace', 'if', 'value', 'is', 'not', 'None', ':', 'warnings', '.', 'warn', '(', '"to_replace is a dict and value is not None. value will be ignored."', ')', 'else', ':', 'if', 'isinstance', '(', 'value', ',', '(', 'float', ',', 'int', ',', 'long', ',', 'basestring', ')', ')', 'or', 'value', 'is', 'None', ':', 'value', '=', '[', 'value', 'for', '_', 'in', 'range', '(', 'len', '(', 'to_replace', ')', ')', ']', 'rep_dict', '=', 'dict', '(', 'zip', '(', 'to_replace', ',', 'value', ')', ')', 'if', 'isinstance', '(', 'subset', ',', 'basestring', ')', ':', 'subset', '=', '[', 'subset', ']', '# Verify we were not passed in mixed type generics.', 'if', 'not', 'any', '(', 'all_of_type', '(', 'rep_dict', '.', 'keys', '(', ')', ')', 'and', 'all_of_type', '(', 'x', 'for', 'x', 'in', 'rep_dict', '.', 'values', '(', ')', 'if', 'x', 'is', 'not', 'None', ')', 'for', 'all_of_type', 'in', '[', 'all_of_bool', ',', 'all_of_str', ',', 'all_of_numeric', ']', ')', ':', 'raise', 'ValueError', '(', '"Mixed type replacements are not supported"', ')', 'if', 'subset', 'is', 'None', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'na', '(', ')', '.', 'replace', '(', "'*'", ',', 'rep_dict', ')', ',', 'self', '.', 'sql_ctx', ')', 'else', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'na', '(', ')', '.', 'replace', '(', 'self', '.', '_jseq', '(', 'subset', ')', ',', 'self', '.', '_jmap', '(', 'rep_dict', ')', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` replacing a value with another value.        :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are        aliases of each other.        Values to_replace and value must have the same type and can only be numerics, booleans,        or strings. Value can have None. When replacing, the new value will be cast        to the type of the existing column.        For numeric replacements all values to be replaced should have unique        floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)        and arbitrary replacement will be used.        :param to_replace: bool, int, long, float, string, list or dict.            Value to be replaced.            If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`            must be a mapping between a value and a replacement.        :param value: bool, int, long, float, string, list or None.            The replacement value must be a bool, int, long, float, string or None. If `value` is a            list, `value` should be of the same length and type as `to_replace`.            If `value` is a scalar and `to_replace` is a sequence, then `value` is            used as a replacement for each item in `to_replace`.        :param subset: optional list of column names to consider.            Columns specified in subset that do not have matching data type are ignored.            For example, if `value` is a string, and subset contains a non-string column,            then the non-string column is simply ignored.        >>> df4.na.replace(10, 20).show()        +----+------+-----+        | age|height| name|        +----+------+-----+        |  20|    80|Alice|        |   5|  null|  Bob|        |null|  null|  Tom|        |null|  null| null|        +----+------+-----+        >>> df4.na.replace('Alice', None).show()        +----+------+----+        | age|height|name|        +----+------+----+        |  10|    80|null|        |   5|  null| Bob|        |null|  null| Tom|        |null|  null|null|        +----+------+----+        >>> df4.na.replace({'Alice': None}).show()        +----+------+----+        | age|height|name|        +----+------+----+        |  10|    80|null|        |   5|  null| Bob|        |null|  null| Tom|        |null|  null|null|        +----+------+----+        >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()        +----+------+----+        | age|height|name|        +----+------+----+        |  10|    80|   A|        |   5|  null|   B|        |null|  null| Tom|        |null|  null|null|        +----+------+----+
*******__*******
Code:def approxQuantile(self, col, probabilities, relativeError):\n        """\n        Calculates the approximate quantiles of numerical columns of a\n        DataFrame.\n\n        The result of this algorithm has the following deterministic bound:\n        If the DataFrame has N elements and if we request the quantile at\n        probability `p` up to error `err`, then the algorithm will return\n        a sample `x` from the DataFrame so that the *exact* rank of `x` is\n        close to (p * N). More precisely,\n\n          floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n\n        This method implements a variation of the Greenwald-Khanna\n        algorithm (with some speed optimizations). The algorithm was first\n        present in [[https://doi.org/10.1145/375663.375670\n        Space-efficient Online Computation of Quantile Summaries]]\n        by Greenwald and Khanna.\n\n        Note that null values will be ignored in numerical columns before calculation.\n        For columns only containing null values, an empty list is returned.\n\n        :param col: str, list.\n          Can be a single column name, or a list of names for multiple columns.\n        :param probabilities: a list of quantile probabilities\n          Each number must belong to [0, 1].\n          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n        :param relativeError:  The relative target precision to achieve\n          (>= 0). If set to zero, the exact quantiles are computed, which\n          could be very expensive. Note that values greater than 1 are\n          accepted but give the same result as 1.\n        :return:  the approximate quantiles at the given probabilities. If\n          the input `col` is a string, the output is a list of floats. If the\n          input `col` is a list or tuple of strings, the output is also a\n          list, but each element in it is a list of floats, i.e., the output\n          is a list of list of floats.\n\n        .. versionchanged:: 2.2\n           Added support for multiple columns.\n        """\n\n        if not isinstance(col, (basestring, list, tuple)):\n            raise ValueError("col should be a string, list or tuple, but got %r" % type(col))\n\n        isStr = isinstance(col, basestring)\n\n        if isinstance(col, tuple):\n            col = list(col)\n        elif isStr:\n            col = [col]\n\n        for c in col:\n            if not isinstance(c, basestring):\n                raise ValueError("columns should be strings, but got %r" % type(c))\n        col = _to_list(self._sc, col)\n\n        if not isinstance(probabilities, (list, tuple)):\n            raise ValueError("probabilities should be a list or tuple")\n        if isinstance(probabilities, tuple):\n            probabilities = list(probabilities)\n        for p in probabilities:\n            if not isinstance(p, (float, int, long)) or p < 0 or p > 1:\n                raise ValueError("probabilities should be numerical (float, int, long) in [0,1].")\n        probabilities = _to_list(self._sc, probabilities)\n\n        if not isinstance(relativeError, (float, int, long)) or relativeError < 0:\n            raise ValueError("relativeError should be numerical (float, int, long) >= 0.")\n        relativeError = float(relativeError)\n\n        jaq = self._jdf.stat().approxQuantile(col, probabilities, relativeError)\n        jaq_list = [list(j) for j in jaq]\n        return jaq_list[0] if isStr else jaq_list
Language: python
Code Tokens: ['def', 'approxQuantile', '(', 'self', ',', 'col', ',', 'probabilities', ',', 'relativeError', ')', ':', 'if', 'not', 'isinstance', '(', 'col', ',', '(', 'basestring', ',', 'list', ',', 'tuple', ')', ')', ':', 'raise', 'ValueError', '(', '"col should be a string, list or tuple, but got %r"', '%', 'type', '(', 'col', ')', ')', 'isStr', '=', 'isinstance', '(', 'col', ',', 'basestring', ')', 'if', 'isinstance', '(', 'col', ',', 'tuple', ')', ':', 'col', '=', 'list', '(', 'col', ')', 'elif', 'isStr', ':', 'col', '=', '[', 'col', ']', 'for', 'c', 'in', 'col', ':', 'if', 'not', 'isinstance', '(', 'c', ',', 'basestring', ')', ':', 'raise', 'ValueError', '(', '"columns should be strings, but got %r"', '%', 'type', '(', 'c', ')', ')', 'col', '=', '_to_list', '(', 'self', '.', '_sc', ',', 'col', ')', 'if', 'not', 'isinstance', '(', 'probabilities', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'raise', 'ValueError', '(', '"probabilities should be a list or tuple"', ')', 'if', 'isinstance', '(', 'probabilities', ',', 'tuple', ')', ':', 'probabilities', '=', 'list', '(', 'probabilities', ')', 'for', 'p', 'in', 'probabilities', ':', 'if', 'not', 'isinstance', '(', 'p', ',', '(', 'float', ',', 'int', ',', 'long', ')', ')', 'or', 'p', '<', '0', 'or', 'p', '>', '1', ':', 'raise', 'ValueError', '(', '"probabilities should be numerical (float, int, long) in [0,1]."', ')', 'probabilities', '=', '_to_list', '(', 'self', '.', '_sc', ',', 'probabilities', ')', 'if', 'not', 'isinstance', '(', 'relativeError', ',', '(', 'float', ',', 'int', ',', 'long', ')', ')', 'or', 'relativeError', '<', '0', ':', 'raise', 'ValueError', '(', '"relativeError should be numerical (float, int, long) >= 0."', ')', 'relativeError', '=', 'float', '(', 'relativeError', ')', 'jaq', '=', 'self', '.', '_jdf', '.', 'stat', '(', ')', '.', 'approxQuantile', '(', 'col', ',', 'probabilities', ',', 'relativeError', ')', 'jaq_list', '=', '[', 'list', '(', 'j', ')', 'for', 'j', 'in', 'jaq', ']', 'return', 'jaq_list', '[', '0', ']', 'if', 'isStr', 'else', 'jaq_list']
Docstring: Calculates the approximate quantiles of numerical columns of a        DataFrame.        The result of this algorithm has the following deterministic bound:        If the DataFrame has N elements and if we request the quantile at        probability `p` up to error `err`, then the algorithm will return        a sample `x` from the DataFrame so that the *exact* rank of `x` is        close to (p * N). More precisely,          floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).        This method implements a variation of the Greenwald-Khanna        algorithm (with some speed optimizations). The algorithm was first        present in [[https://doi.org/10.1145/375663.375670        Space-efficient Online Computation of Quantile Summaries]]        by Greenwald and Khanna.        Note that null values will be ignored in numerical columns before calculation.        For columns only containing null values, an empty list is returned.        :param col: str, list.          Can be a single column name, or a list of names for multiple columns.        :param probabilities: a list of quantile probabilities          Each number must belong to [0, 1].          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.        :param relativeError:  The relative target precision to achieve          (>= 0). If set to zero, the exact quantiles are computed, which          could be very expensive. Note that values greater than 1 are          accepted but give the same result as 1.        :return:  the approximate quantiles at the given probabilities. If          the input `col` is a string, the output is a list of floats. If the          input `col` is a list or tuple of strings, the output is also a          list, but each element in it is a list of floats, i.e., the output          is a list of list of floats.        .. versionchanged:: 2.2           Added support for multiple columns.
*******__*******
Code:def corr(self, col1, col2, method=None):\n        """\n        Calculates the correlation of two columns of a DataFrame as a double value.\n        Currently only supports the Pearson Correlation Coefficient.\n        :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n\n        :param col1: The name of the first column\n        :param col2: The name of the second column\n        :param method: The correlation method. Currently only supports "pearson"\n        """\n        if not isinstance(col1, basestring):\n            raise ValueError("col1 should be a string.")\n        if not isinstance(col2, basestring):\n            raise ValueError("col2 should be a string.")\n        if not method:\n            method = "pearson"\n        if not method == "pearson":\n            raise ValueError("Currently only the calculation of the Pearson Correlation " +\n                             "coefficient is supported.")\n        return self._jdf.stat().corr(col1, col2, method)
Language: python
Code Tokens: ['def', 'corr', '(', 'self', ',', 'col1', ',', 'col2', ',', 'method', '=', 'None', ')', ':', 'if', 'not', 'isinstance', '(', 'col1', ',', 'basestring', ')', ':', 'raise', 'ValueError', '(', '"col1 should be a string."', ')', 'if', 'not', 'isinstance', '(', 'col2', ',', 'basestring', ')', ':', 'raise', 'ValueError', '(', '"col2 should be a string."', ')', 'if', 'not', 'method', ':', 'method', '=', '"pearson"', 'if', 'not', 'method', '==', '"pearson"', ':', 'raise', 'ValueError', '(', '"Currently only the calculation of the Pearson Correlation "', '+', '"coefficient is supported."', ')', 'return', 'self', '.', '_jdf', '.', 'stat', '(', ')', '.', 'corr', '(', 'col1', ',', 'col2', ',', 'method', ')']
Docstring: Calculates the correlation of two columns of a DataFrame as a double value.        Currently only supports the Pearson Correlation Coefficient.        :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.        :param col1: The name of the first column        :param col2: The name of the second column        :param method: The correlation method. Currently only supports "pearson"
*******__*******
Code:def cov(self, col1, col2):\n        """\n        Calculate the sample covariance for the given columns, specified by their names, as a\n        double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n\n        :param col1: The name of the first column\n        :param col2: The name of the second column\n        """\n        if not isinstance(col1, basestring):\n            raise ValueError("col1 should be a string.")\n        if not isinstance(col2, basestring):\n            raise ValueError("col2 should be a string.")\n        return self._jdf.stat().cov(col1, col2)
Language: python
Code Tokens: ['def', 'cov', '(', 'self', ',', 'col1', ',', 'col2', ')', ':', 'if', 'not', 'isinstance', '(', 'col1', ',', 'basestring', ')', ':', 'raise', 'ValueError', '(', '"col1 should be a string."', ')', 'if', 'not', 'isinstance', '(', 'col2', ',', 'basestring', ')', ':', 'raise', 'ValueError', '(', '"col2 should be a string."', ')', 'return', 'self', '.', '_jdf', '.', 'stat', '(', ')', '.', 'cov', '(', 'col1', ',', 'col2', ')']
Docstring: Calculate the sample covariance for the given columns, specified by their names, as a        double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.        :param col1: The name of the first column        :param col2: The name of the second column
*******__*******
Code:def crosstab(self, col1, col2):\n        """\n        Computes a pair-wise frequency table of the given columns. Also known as a contingency\n        table. The number of distinct values for each column should be less than 1e4. At most 1e6\n        non-zero pair frequencies will be returned.\n        The first column of each row will be the distinct values of `col1` and the column names\n        will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n        Pairs that have no occurrences will have zero as their counts.\n        :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n\n        :param col1: The name of the first column. Distinct items will make the first item of\n            each row.\n        :param col2: The name of the second column. Distinct items will make the column names\n            of the DataFrame.\n        """\n        if not isinstance(col1, basestring):\n            raise ValueError("col1 should be a string.")\n        if not isinstance(col2, basestring):\n            raise ValueError("col2 should be a string.")\n        return DataFrame(self._jdf.stat().crosstab(col1, col2), self.sql_ctx)
Language: python
Code Tokens: ['def', 'crosstab', '(', 'self', ',', 'col1', ',', 'col2', ')', ':', 'if', 'not', 'isinstance', '(', 'col1', ',', 'basestring', ')', ':', 'raise', 'ValueError', '(', '"col1 should be a string."', ')', 'if', 'not', 'isinstance', '(', 'col2', ',', 'basestring', ')', ':', 'raise', 'ValueError', '(', '"col2 should be a string."', ')', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'stat', '(', ')', '.', 'crosstab', '(', 'col1', ',', 'col2', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Computes a pair-wise frequency table of the given columns. Also known as a contingency        table. The number of distinct values for each column should be less than 1e4. At most 1e6        non-zero pair frequencies will be returned.        The first column of each row will be the distinct values of `col1` and the column names        will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.        Pairs that have no occurrences will have zero as their counts.        :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.        :param col1: The name of the first column. Distinct items will make the first item of            each row.        :param col2: The name of the second column. Distinct items will make the column names            of the DataFrame.
*******__*******
Code:def freqItems(self, cols, support=None):\n        """\n        Finding frequent items for columns, possibly with false positives. Using the\n        frequent element count algorithm described in\n        "https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou".\n        :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n\n        .. note:: This function is meant for exploratory data analysis, as we make no\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\n\n        :param cols: Names of the columns to calculate frequent items for as a list or tuple of\n            strings.\n        :param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n            The support must be greater than 1e-4.\n        """\n        if isinstance(cols, tuple):\n            cols = list(cols)\n        if not isinstance(cols, list):\n            raise ValueError("cols must be a list or tuple of column names as strings.")\n        if not support:\n            support = 0.01\n        return DataFrame(self._jdf.stat().freqItems(_to_seq(self._sc, cols), support), self.sql_ctx)
Language: python
Code Tokens: ['def', 'freqItems', '(', 'self', ',', 'cols', ',', 'support', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'cols', ',', 'tuple', ')', ':', 'cols', '=', 'list', '(', 'cols', ')', 'if', 'not', 'isinstance', '(', 'cols', ',', 'list', ')', ':', 'raise', 'ValueError', '(', '"cols must be a list or tuple of column names as strings."', ')', 'if', 'not', 'support', ':', 'support', '=', '0.01', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'stat', '(', ')', '.', 'freqItems', '(', '_to_seq', '(', 'self', '.', '_sc', ',', 'cols', ')', ',', 'support', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Finding frequent items for columns, possibly with false positives. Using the        frequent element count algorithm described in        "https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou".        :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.        .. note:: This function is meant for exploratory data analysis, as we make no            guarantee about the backward compatibility of the schema of the resulting DataFrame.        :param cols: Names of the columns to calculate frequent items for as a list or tuple of            strings.        :param support: The frequency with which to consider an item 'frequent'. Default is 1%.            The support must be greater than 1e-4.
*******__*******
Code:def withColumn(self, colName, col):\n        """\n        Returns a new :class:`DataFrame` by adding a column or replacing the\n        existing column that has the same name.\n\n        The column expression must be an expression over this DataFrame; attempting to add\n        a column from some other dataframe will raise an error.\n\n        :param colName: string, name of the new column.\n        :param col: a :class:`Column` expression for the new column.\n\n        .. note:: This method introduces a projection internally. Therefore, calling it multiple\n            times, for instance, via loops in order to add multiple columns can generate big\n            plans which can cause performance issues and even `StackOverflowException`.\n            To avoid this, use :func:`select` with the multiple columns at once.\n\n        >>> df.withColumn('age2', df.age + 2).collect()\n        [Row(age=2, name=u'Alice', age2=4), Row(age=5, name=u'Bob', age2=7)]\n\n        """\n        assert isinstance(col, Column), "col should be Column"\n        return DataFrame(self._jdf.withColumn(colName, col._jc), self.sql_ctx)
Language: python
Code Tokens: ['def', 'withColumn', '(', 'self', ',', 'colName', ',', 'col', ')', ':', 'assert', 'isinstance', '(', 'col', ',', 'Column', ')', ',', '"col should be Column"', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'withColumn', '(', 'colName', ',', 'col', '.', '_jc', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` by adding a column or replacing the        existing column that has the same name.        The column expression must be an expression over this DataFrame; attempting to add        a column from some other dataframe will raise an error.        :param colName: string, name of the new column.        :param col: a :class:`Column` expression for the new column.        .. note:: This method introduces a projection internally. Therefore, calling it multiple            times, for instance, via loops in order to add multiple columns can generate big            plans which can cause performance issues and even `StackOverflowException`.            To avoid this, use :func:`select` with the multiple columns at once.        >>> df.withColumn('age2', df.age + 2).collect()        [Row(age=2, name=u'Alice', age2=4), Row(age=5, name=u'Bob', age2=7)]
*******__*******
Code:def withColumnRenamed(self, existing, new):\n        """Returns a new :class:`DataFrame` by renaming an existing column.\n        This is a no-op if schema doesn't contain the given column name.\n\n        :param existing: string, name of the existing column to rename.\n        :param new: string, new name of the column.\n\n        >>> df.withColumnRenamed('age', 'age2').collect()\n        [Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]\n        """\n        return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)
Language: python
Code Tokens: ['def', 'withColumnRenamed', '(', 'self', ',', 'existing', ',', 'new', ')', ':', 'return', 'DataFrame', '(', 'self', '.', '_jdf', '.', 'withColumnRenamed', '(', 'existing', ',', 'new', ')', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` by renaming an existing column.        This is a no-op if schema doesn't contain the given column name.        :param existing: string, name of the existing column to rename.        :param new: string, new name of the column.        >>> df.withColumnRenamed('age', 'age2').collect()        [Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]
*******__*******
Code:def drop(self, *cols):\n        """Returns a new :class:`DataFrame` that drops the specified column.\n        This is a no-op if schema doesn't contain the given column name(s).\n\n        :param cols: a string name of the column to drop, or a\n            :class:`Column` to drop, or a list of string name of the columns to drop.\n\n        >>> df.drop('age').collect()\n        [Row(name=u'Alice'), Row(name=u'Bob')]\n\n        >>> df.drop(df.age).collect()\n        [Row(name=u'Alice'), Row(name=u'Bob')]\n\n        >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n        [Row(age=5, height=85, name=u'Bob')]\n\n        >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n        [Row(age=5, name=u'Bob', height=85)]\n\n        >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n        [Row(name=u'Bob')]\n        """\n        if len(cols) == 1:\n            col = cols[0]\n            if isinstance(col, basestring):\n                jdf = self._jdf.drop(col)\n            elif isinstance(col, Column):\n                jdf = self._jdf.drop(col._jc)\n            else:\n                raise TypeError("col should be a string or a Column")\n        else:\n            for col in cols:\n                if not isinstance(col, basestring):\n                    raise TypeError("each col in the param list should be a string")\n            jdf = self._jdf.drop(self._jseq(cols))\n\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'drop', '(', 'self', ',', '*', 'cols', ')', ':', 'if', 'len', '(', 'cols', ')', '==', '1', ':', 'col', '=', 'cols', '[', '0', ']', 'if', 'isinstance', '(', 'col', ',', 'basestring', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'drop', '(', 'col', ')', 'elif', 'isinstance', '(', 'col', ',', 'Column', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'drop', '(', 'col', '.', '_jc', ')', 'else', ':', 'raise', 'TypeError', '(', '"col should be a string or a Column"', ')', 'else', ':', 'for', 'col', 'in', 'cols', ':', 'if', 'not', 'isinstance', '(', 'col', ',', 'basestring', ')', ':', 'raise', 'TypeError', '(', '"each col in the param list should be a string"', ')', 'jdf', '=', 'self', '.', '_jdf', '.', 'drop', '(', 'self', '.', '_jseq', '(', 'cols', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new :class:`DataFrame` that drops the specified column.        This is a no-op if schema doesn't contain the given column name(s).        :param cols: a string name of the column to drop, or a            :class:`Column` to drop, or a list of string name of the columns to drop.        >>> df.drop('age').collect()        [Row(name=u'Alice'), Row(name=u'Bob')]        >>> df.drop(df.age).collect()        [Row(name=u'Alice'), Row(name=u'Bob')]        >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()        [Row(age=5, height=85, name=u'Bob')]        >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()        [Row(age=5, name=u'Bob', height=85)]        >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()        [Row(name=u'Bob')]
*******__*******
Code:def toDF(self, *cols):\n        """Returns a new class:`DataFrame` that with new specified column names\n\n        :param cols: list of new column names (string)\n\n        >>> df.toDF('f1', 'f2').collect()\n        [Row(f1=2, f2=u'Alice'), Row(f1=5, f2=u'Bob')]\n        """\n        jdf = self._jdf.toDF(self._jseq(cols))\n        return DataFrame(jdf, self.sql_ctx)
Language: python
Code Tokens: ['def', 'toDF', '(', 'self', ',', '*', 'cols', ')', ':', 'jdf', '=', 'self', '.', '_jdf', '.', 'toDF', '(', 'self', '.', '_jseq', '(', 'cols', ')', ')', 'return', 'DataFrame', '(', 'jdf', ',', 'self', '.', 'sql_ctx', ')']
Docstring: Returns a new class:`DataFrame` that with new specified column names        :param cols: list of new column names (string)        >>> df.toDF('f1', 'f2').collect()        [Row(f1=2, f2=u'Alice'), Row(f1=5, f2=u'Bob')]
*******__*******
Code:def transform(self, func):\n        """Returns a new class:`DataFrame`. Concise syntax for chaining custom transformations.\n\n        :param func: a function that takes and returns a class:`DataFrame`.\n\n        >>> from pyspark.sql.functions import col\n        >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], ["int", "float"])\n        >>> def cast_all_to_int(input_df):\n        ...     return input_df.select([col(col_name).cast("int") for col_name in input_df.columns])\n        >>> def sort_columns_asc(input_df):\n        ...     return input_df.select(*sorted(input_df.columns))\n        >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n        +-----+---+\n        |float|int|\n        +-----+---+\n        |    1|  1|\n        |    2|  2|\n        +-----+---+\n        """\n        result = func(self)\n        assert isinstance(result, DataFrame), "Func returned an instance of type [%s], " \\n                                              "should have been DataFrame." % type(result)\n        return result
Language: python
Code Tokens: ['def', 'transform', '(', 'self', ',', 'func', ')', ':', 'result', '=', 'func', '(', 'self', ')', 'assert', 'isinstance', '(', 'result', ',', 'DataFrame', ')', ',', '"Func returned an instance of type [%s], "', '"should have been DataFrame."', '%', 'type', '(', 'result', ')', 'return', 'result']
Docstring: Returns a new class:`DataFrame`. Concise syntax for chaining custom transformations.        :param func: a function that takes and returns a class:`DataFrame`.        >>> from pyspark.sql.functions import col        >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], ["int", "float"])        >>> def cast_all_to_int(input_df):        ...     return input_df.select([col(col_name).cast("int") for col_name in input_df.columns])        >>> def sort_columns_asc(input_df):        ...     return input_df.select(*sorted(input_df.columns))        >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()        +-----+---+        |float|int|        +-----+---+        |    1|  1|        |    2|  2|        +-----+---+
*******__*******
Code:def toPandas(self):\n        """\n        Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n\n        This is only available if Pandas is installed and available.\n\n        .. note:: This method should only be used if the resulting Pandas's DataFrame is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n\n        >>> df.toPandas()  # doctest: +SKIP\n           age   name\n        0    2  Alice\n        1    5    Bob\n        """\n        from pyspark.sql.utils import require_minimum_pandas_version\n        require_minimum_pandas_version()\n\n        import pandas as pd\n\n        if self.sql_ctx._conf.pandasRespectSessionTimeZone():\n            timezone = self.sql_ctx._conf.sessionLocalTimeZone()\n        else:\n            timezone = None\n\n        if self.sql_ctx._conf.arrowEnabled():\n            use_arrow = True\n            try:\n                from pyspark.sql.types import to_arrow_schema\n                from pyspark.sql.utils import require_minimum_pyarrow_version\n\n                require_minimum_pyarrow_version()\n                to_arrow_schema(self.schema)\n            except Exception as e:\n\n                if self.sql_ctx._conf.arrowFallbackEnabled():\n                    msg = (\n                        "toPandas attempted Arrow optimization because "\n                        "'spark.sql.execution.arrow.enabled' is set to true; however, "\n                        "failed by the reason below:\n  %s\n"\n                        "Attempting non-optimization as "\n                        "'spark.sql.execution.arrow.fallback.enabled' is set to "\n                        "true." % _exception_message(e))\n                    warnings.warn(msg)\n                    use_arrow = False\n                else:\n                    msg = (\n                        "toPandas attempted Arrow optimization because "\n                        "'spark.sql.execution.arrow.enabled' is set to true, but has reached "\n                        "the error below and will not continue because automatic fallback "\n                        "with 'spark.sql.execution.arrow.fallback.enabled' has been set to "\n                        "false.\n  %s" % _exception_message(e))\n                    warnings.warn(msg)\n                    raise\n\n            # Try to use Arrow optimization when the schema is supported and the required version\n            # of PyArrow is found, if 'spark.sql.execution.arrow.enabled' is enabled.\n            if use_arrow:\n                try:\n                    from pyspark.sql.types import _check_dataframe_localize_timestamps\n                    import pyarrow\n                    batches = self._collectAsArrow()\n                    if len(batches) > 0:\n                        table = pyarrow.Table.from_batches(batches)\n                        # Pandas DataFrame created from PyArrow uses datetime64[ns] for date type\n                        # values, but we should use datetime.date to match the behavior with when\n                        # Arrow optimization is disabled.\n                        pdf = table.to_pandas(date_as_object=True)\n                        return _check_dataframe_localize_timestamps(pdf, timezone)\n                    else:\n                        return pd.DataFrame.from_records([], columns=self.columns)\n                except Exception as e:\n                    # We might have to allow fallback here as well but multiple Spark jobs can\n                    # be executed. So, simply fail in this case for now.\n                    msg = (\n                        "toPandas attempted Arrow optimization because "\n                        "'spark.sql.execution.arrow.enabled' is set to true, but has reached "\n                        "the error below and can not continue. Note that "\n                        "'spark.sql.execution.arrow.fallback.enabled' does not have an effect "\n                        "on failures in the middle of computation.\n  %s" % _exception_message(e))\n                    warnings.warn(msg)\n                    raise\n\n        # Below is toPandas without Arrow optimization.\n        pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n\n        dtype = {}\n        for field in self.schema:\n            pandas_type = _to_corrected_pandas_type(field.dataType)\n            # SPARK-21766: if an integer field is nullable and has null values, it can be\n            # inferred by pandas as float column. Once we convert the column with NaN back\n            # to integer type e.g., np.int16, we will hit exception. So we use the inferred\n            # float type, not the corrected type from the schema in this case.\n            if pandas_type is not None and \\n                not(isinstance(field.dataType, IntegralType) and field.nullable and\n                    pdf[field.name].isnull().any()):\n                dtype[field.name] = pandas_type\n\n        for f, t in dtype.items():\n            pdf[f] = pdf[f].astype(t, copy=False)\n\n        if timezone is None:\n            return pdf\n        else:\n            from pyspark.sql.types import _check_series_convert_timestamps_local_tz\n            for field in self.schema:\n                # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n                if isinstance(field.dataType, TimestampType):\n                    pdf[field.name] = \\n                        _check_series_convert_timestamps_local_tz(pdf[field.name], timezone)\n            return pdf
Language: python
Code Tokens: ['def', 'toPandas', '(', 'self', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'import', 'pandas', 'as', 'pd', 'if', 'self', '.', 'sql_ctx', '.', '_conf', '.', 'pandasRespectSessionTimeZone', '(', ')', ':', 'timezone', '=', 'self', '.', 'sql_ctx', '.', '_conf', '.', 'sessionLocalTimeZone', '(', ')', 'else', ':', 'timezone', '=', 'None', 'if', 'self', '.', 'sql_ctx', '.', '_conf', '.', 'arrowEnabled', '(', ')', ':', 'use_arrow', '=', 'True', 'try', ':', 'from', 'pyspark', '.', 'sql', '.', 'types', 'import', 'to_arrow_schema', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pyarrow_version', 'require_minimum_pyarrow_version', '(', ')', 'to_arrow_schema', '(', 'self', '.', 'schema', ')', 'except', 'Exception', 'as', 'e', ':', 'if', 'self', '.', 'sql_ctx', '.', '_conf', '.', 'arrowFallbackEnabled', '(', ')', ':', 'msg', '=', '(', '"toPandas attempted Arrow optimization because "', '"\'spark.sql.execution.arrow.enabled\' is set to true; however, "', '"failed by the reason below:\\n  %s\\n"', '"Attempting non-optimization as "', '"\'spark.sql.execution.arrow.fallback.enabled\' is set to "', '"true."', '%', '_exception_message', '(', 'e', ')', ')', 'warnings', '.', 'warn', '(', 'msg', ')', 'use_arrow', '=', 'False', 'else', ':', 'msg', '=', '(', '"toPandas attempted Arrow optimization because "', '"\'spark.sql.execution.arrow.enabled\' is set to true, but has reached "', '"the error below and will not continue because automatic fallback "', '"with \'spark.sql.execution.arrow.fallback.enabled\' has been set to "', '"false.\\n  %s"', '%', '_exception_message', '(', 'e', ')', ')', 'warnings', '.', 'warn', '(', 'msg', ')', 'raise', '# Try to use Arrow optimization when the schema is supported and the required version', "# of PyArrow is found, if 'spark.sql.execution.arrow.enabled' is enabled.", 'if', 'use_arrow', ':', 'try', ':', 'from', 'pyspark', '.', 'sql', '.', 'types', 'import', '_check_dataframe_localize_timestamps', 'import', 'pyarrow', 'batches', '=', 'self', '.', '_collectAsArrow', '(', ')', 'if', 'len', '(', 'batches', ')', '>', '0', ':', 'table', '=', 'pyarrow', '.', 'Table', '.', 'from_batches', '(', 'batches', ')', '# Pandas DataFrame created from PyArrow uses datetime64[ns] for date type', '# values, but we should use datetime.date to match the behavior with when', '# Arrow optimization is disabled.', 'pdf', '=', 'table', '.', 'to_pandas', '(', 'date_as_object', '=', 'True', ')', 'return', '_check_dataframe_localize_timestamps', '(', 'pdf', ',', 'timezone', ')', 'else', ':', 'return', 'pd', '.', 'DataFrame', '.', 'from_records', '(', '[', ']', ',', 'columns', '=', 'self', '.', 'columns', ')', 'except', 'Exception', 'as', 'e', ':', '# We might have to allow fallback here as well but multiple Spark jobs can', '# be executed. So, simply fail in this case for now.', 'msg', '=', '(', '"toPandas attempted Arrow optimization because "', '"\'spark.sql.execution.arrow.enabled\' is set to true, but has reached "', '"the error below and can not continue. Note that "', '"\'spark.sql.execution.arrow.fallback.enabled\' does not have an effect "', '"on failures in the middle of computation.\\n  %s"', '%', '_exception_message', '(', 'e', ')', ')', 'warnings', '.', 'warn', '(', 'msg', ')', 'raise', '# Below is toPandas without Arrow optimization.', 'pdf', '=', 'pd', '.', 'DataFrame', '.', 'from_records', '(', 'self', '.', 'collect', '(', ')', ',', 'columns', '=', 'self', '.', 'columns', ')', 'dtype', '=', '{', '}', 'for', 'field', 'in', 'self', '.', 'schema', ':', 'pandas_type', '=', '_to_corrected_pandas_type', '(', 'field', '.', 'dataType', ')', '# SPARK-21766: if an integer field is nullable and has null values, it can be', '# inferred by pandas as float column. Once we convert the column with NaN back', '# to integer type e.g., np.int16, we will hit exception. So we use the inferred', '# float type, not the corrected type from the schema in this case.', 'if', 'pandas_type', 'is', 'not', 'None', 'and', 'not', '(', 'isinstance', '(', 'field', '.', 'dataType', ',', 'IntegralType', ')', 'and', 'field', '.', 'nullable', 'and', 'pdf', '[', 'field', '.', 'name', ']', '.', 'isnull', '(', ')', '.', 'any', '(', ')', ')', ':', 'dtype', '[', 'field', '.', 'name', ']', '=', 'pandas_type', 'for', 'f', ',', 't', 'in', 'dtype', '.', 'items', '(', ')', ':', 'pdf', '[', 'f', ']', '=', 'pdf', '[', 'f', ']', '.', 'astype', '(', 't', ',', 'copy', '=', 'False', ')', 'if', 'timezone', 'is', 'None', ':', 'return', 'pdf', 'else', ':', 'from', 'pyspark', '.', 'sql', '.', 'types', 'import', '_check_series_convert_timestamps_local_tz', 'for', 'field', 'in', 'self', '.', 'schema', ':', '# TODO: handle nested timestamps, such as ArrayType(TimestampType())?', 'if', 'isinstance', '(', 'field', '.', 'dataType', ',', 'TimestampType', ')', ':', 'pdf', '[', 'field', '.', 'name', ']', '=', '_check_series_convert_timestamps_local_tz', '(', 'pdf', '[', 'field', '.', 'name', ']', ',', 'timezone', ')', 'return', 'pdf']
Docstring: Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.        This is only available if Pandas is installed and available.        .. note:: This method should only be used if the resulting Pandas's DataFrame is expected            to be small, as all the data is loaded into the driver's memory.        .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.        >>> df.toPandas()  # doctest: +SKIP           age   name        0    2  Alice        1    5    Bob
*******__*******
Code:def _collectAsArrow(self):\n        """\n        Returns all records as a list of ArrowRecordBatches, pyarrow must be installed\n        and available on driver and worker Python environments.\n\n        .. note:: Experimental.\n        """\n        with SCCallSiteSync(self._sc) as css:\n            sock_info = self._jdf.collectAsArrowToPython()\n\n        # Collect list of un-ordered batches where last element is a list of correct order indices\n        results = list(_load_from_socket(sock_info, ArrowCollectSerializer()))\n        batches = results[:-1]\n        batch_order = results[-1]\n\n        # Re-order the batch list using the correct order\n        return [batches[i] for i in batch_order]
Language: python
Code Tokens: ['def', '_collectAsArrow', '(', 'self', ')', ':', 'with', 'SCCallSiteSync', '(', 'self', '.', '_sc', ')', 'as', 'css', ':', 'sock_info', '=', 'self', '.', '_jdf', '.', 'collectAsArrowToPython', '(', ')', '# Collect list of un-ordered batches where last element is a list of correct order indices', 'results', '=', 'list', '(', '_load_from_socket', '(', 'sock_info', ',', 'ArrowCollectSerializer', '(', ')', ')', ')', 'batches', '=', 'results', '[', ':', '-', '1', ']', 'batch_order', '=', 'results', '[', '-', '1', ']', '# Re-order the batch list using the correct order', 'return', '[', 'batches', '[', 'i', ']', 'for', 'i', 'in', 'batch_order', ']']
Docstring: Returns all records as a list of ArrowRecordBatches, pyarrow must be installed        and available on driver and worker Python environments.        .. note:: Experimental.
*******__*******
Code:def asDict(self, sample=False):\n        """Returns the :class:`StatCounter` members as a ``dict``.\n\n        >>> sc.parallelize([1., 2., 3., 4.]).stats().asDict()\n        {'count': 4L,\n         'max': 4.0,\n         'mean': 2.5,\n         'min': 1.0,\n         'stdev': 1.2909944487358056,\n         'sum': 10.0,\n         'variance': 1.6666666666666667}\n        """\n        return {\n            'count': self.count(),\n            'mean': self.mean(),\n            'sum': self.sum(),\n            'min': self.min(),\n            'max': self.max(),\n            'stdev': self.stdev() if sample else self.sampleStdev(),\n            'variance': self.variance() if sample else self.sampleVariance()\n        }
Language: python
Code Tokens: ['def', 'asDict', '(', 'self', ',', 'sample', '=', 'False', ')', ':', 'return', '{', "'count'", ':', 'self', '.', 'count', '(', ')', ',', "'mean'", ':', 'self', '.', 'mean', '(', ')', ',', "'sum'", ':', 'self', '.', 'sum', '(', ')', ',', "'min'", ':', 'self', '.', 'min', '(', ')', ',', "'max'", ':', 'self', '.', 'max', '(', ')', ',', "'stdev'", ':', 'self', '.', 'stdev', '(', ')', 'if', 'sample', 'else', 'self', '.', 'sampleStdev', '(', ')', ',', "'variance'", ':', 'self', '.', 'variance', '(', ')', 'if', 'sample', 'else', 'self', '.', 'sampleVariance', '(', ')', '}']
Docstring: Returns the :class:`StatCounter` members as a ``dict``.        >>> sc.parallelize([1., 2., 3., 4.]).stats().asDict()        {'count': 4L,         'max': 4.0,         'mean': 2.5,         'min': 1.0,         'stdev': 1.2909944487358056,         'sum': 10.0,         'variance': 1.6666666666666667}
*******__*******
Code:def _list_function_infos(jvm):\n    """\n    Returns a list of function information via JVM. Sorts wrapped expression infos by name\n    and returns them.\n    """\n\n    jinfos = jvm.org.apache.spark.sql.api.python.PythonSQLUtils.listBuiltinFunctionInfos()\n    infos = []\n    for jinfo in jinfos:\n        name = jinfo.getName()\n        usage = jinfo.getUsage()\n        usage = usage.replace("_FUNC_", name) if usage is not None else usage\n        infos.append(ExpressionInfo(\n            className=jinfo.getClassName(),\n            name=name,\n            usage=usage,\n            arguments=jinfo.getArguments().replace("_FUNC_", name),\n            examples=jinfo.getExamples().replace("_FUNC_", name),\n            note=jinfo.getNote(),\n            since=jinfo.getSince(),\n            deprecated=jinfo.getDeprecated()))\n    return sorted(infos, key=lambda i: i.name)
Language: python
Code Tokens: ['def', '_list_function_infos', '(', 'jvm', ')', ':', 'jinfos', '=', 'jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'sql', '.', 'api', '.', 'python', '.', 'PythonSQLUtils', '.', 'listBuiltinFunctionInfos', '(', ')', 'infos', '=', '[', ']', 'for', 'jinfo', 'in', 'jinfos', ':', 'name', '=', 'jinfo', '.', 'getName', '(', ')', 'usage', '=', 'jinfo', '.', 'getUsage', '(', ')', 'usage', '=', 'usage', '.', 'replace', '(', '"_FUNC_"', ',', 'name', ')', 'if', 'usage', 'is', 'not', 'None', 'else', 'usage', 'infos', '.', 'append', '(', 'ExpressionInfo', '(', 'className', '=', 'jinfo', '.', 'getClassName', '(', ')', ',', 'name', '=', 'name', ',', 'usage', '=', 'usage', ',', 'arguments', '=', 'jinfo', '.', 'getArguments', '(', ')', '.', 'replace', '(', '"_FUNC_"', ',', 'name', ')', ',', 'examples', '=', 'jinfo', '.', 'getExamples', '(', ')', '.', 'replace', '(', '"_FUNC_"', ',', 'name', ')', ',', 'note', '=', 'jinfo', '.', 'getNote', '(', ')', ',', 'since', '=', 'jinfo', '.', 'getSince', '(', ')', ',', 'deprecated', '=', 'jinfo', '.', 'getDeprecated', '(', ')', ')', ')', 'return', 'sorted', '(', 'infos', ',', 'key', '=', 'lambda', 'i', ':', 'i', '.', 'name', ')']
Docstring: Returns a list of function information via JVM. Sorts wrapped expression infos by name    and returns them.
*******__*******
Code:def _make_pretty_usage(usage):\n    """\n    Makes the usage description pretty and returns a formatted string if `usage`\n    is not an empty string. Otherwise, returns None.\n    """\n\n    if usage is not None and usage.strip() != "":\n        usage = "\n".join(map(lambda u: u.strip(), usage.split("\n")))\n        return "%s\n\n" % usage
Language: python
Code Tokens: ['def', '_make_pretty_usage', '(', 'usage', ')', ':', 'if', 'usage', 'is', 'not', 'None', 'and', 'usage', '.', 'strip', '(', ')', '!=', '""', ':', 'usage', '=', '"\\n"', '.', 'join', '(', 'map', '(', 'lambda', 'u', ':', 'u', '.', 'strip', '(', ')', ',', 'usage', '.', 'split', '(', '"\\n"', ')', ')', ')', 'return', '"%s\\n\\n"', '%', 'usage']
Docstring: Makes the usage description pretty and returns a formatted string if `usage`    is not an empty string. Otherwise, returns None.
*******__*******
Code:def _make_pretty_arguments(arguments):\n    """\n    Makes the arguments description pretty and returns a formatted string if `arguments`\n    starts with the argument prefix. Otherwise, returns None.\n\n    Expected input:\n\n        Arguments:\n          * arg0 - ...\n              ...\n          * arg0 - ...\n              ...\n\n    Expected output:\n    **Arguments:**\n\n    * arg0 - ...\n        ...\n    * arg0 - ...\n        ...\n\n    """\n\n    if arguments.startswith("\n    Arguments:"):\n        arguments = "\n".join(map(lambda u: u[6:], arguments.strip().split("\n")[1:]))\n        return "**Arguments:**\n\n%s\n\n" % arguments
Language: python
Code Tokens: ['def', '_make_pretty_arguments', '(', 'arguments', ')', ':', 'if', 'arguments', '.', 'startswith', '(', '"\\n    Arguments:"', ')', ':', 'arguments', '=', '"\\n"', '.', 'join', '(', 'map', '(', 'lambda', 'u', ':', 'u', '[', '6', ':', ']', ',', 'arguments', '.', 'strip', '(', ')', '.', 'split', '(', '"\\n"', ')', '[', '1', ':', ']', ')', ')', 'return', '"**Arguments:**\\n\\n%s\\n\\n"', '%', 'arguments']
Docstring: Makes the arguments description pretty and returns a formatted string if `arguments`    starts with the argument prefix. Otherwise, returns None.    Expected input:        Arguments:          * arg0 - ...              ...          * arg0 - ...              ...    Expected output:    **Arguments:**    * arg0 - ...        ...    * arg0 - ...        ...
*******__*******
Code:def _make_pretty_examples(examples):\n    """\n    Makes the examples description pretty and returns a formatted string if `examples`\n    starts with the example prefix. Otherwise, returns None.\n\n    Expected input:\n\n        Examples:\n          > SELECT ...;\n           ...\n          > SELECT ...;\n           ...\n\n    Expected output:\n    **Examples:**\n\n    ```\n    > SELECT ...;\n     ...\n    > SELECT ...;\n     ...\n    ```\n\n    """\n\n    if examples.startswith("\n    Examples:"):\n        examples = "\n".join(map(lambda u: u[6:], examples.strip().split("\n")[1:]))\n        return "**Examples:**\n\n```\n%s\n```\n\n" % examples
Language: python
Code Tokens: ['def', '_make_pretty_examples', '(', 'examples', ')', ':', 'if', 'examples', '.', 'startswith', '(', '"\\n    Examples:"', ')', ':', 'examples', '=', '"\\n"', '.', 'join', '(', 'map', '(', 'lambda', 'u', ':', 'u', '[', '6', ':', ']', ',', 'examples', '.', 'strip', '(', ')', '.', 'split', '(', '"\\n"', ')', '[', '1', ':', ']', ')', ')', 'return', '"**Examples:**\\n\\n```\\n%s\\n```\\n\\n"', '%', 'examples']
Docstring: Makes the examples description pretty and returns a formatted string if `examples`    starts with the example prefix. Otherwise, returns None.    Expected input:        Examples:          > SELECT ...;           ...          > SELECT ...;           ...    Expected output:    **Examples:**    ```    > SELECT ...;     ...    > SELECT ...;     ...    ```
*******__*******
Code:def _make_pretty_note(note):\n    """\n    Makes the note description pretty and returns a formatted string if `note` is not\n    an empty string. Otherwise, returns None.\n\n    Expected input:\n\n        ...\n\n    Expected output:\n    **Note:**\n\n    ...\n\n    """\n\n    if note != "":\n        note = "\n".join(map(lambda n: n[4:], note.split("\n")))\n        return "**Note:**\n%s\n" % note
Language: python
Code Tokens: ['def', '_make_pretty_note', '(', 'note', ')', ':', 'if', 'note', '!=', '""', ':', 'note', '=', '"\\n"', '.', 'join', '(', 'map', '(', 'lambda', 'n', ':', 'n', '[', '4', ':', ']', ',', 'note', '.', 'split', '(', '"\\n"', ')', ')', ')', 'return', '"**Note:**\\n%s\\n"', '%', 'note']
Docstring: Makes the note description pretty and returns a formatted string if `note` is not    an empty string. Otherwise, returns None.    Expected input:        ...    Expected output:    **Note:**    ...
*******__*******
Code:def _make_pretty_deprecated(deprecated):\n    """\n    Makes the deprecated description pretty and returns a formatted string if `deprecated`\n    is not an empty string. Otherwise, returns None.\n\n    Expected input:\n\n        ...\n\n    Expected output:\n    **Deprecated:**\n\n    ...\n\n    """\n\n    if deprecated != "":\n        deprecated = "\n".join(map(lambda n: n[4:], deprecated.split("\n")))\n        return "**Deprecated:**\n%s\n" % deprecated
Language: python
Code Tokens: ['def', '_make_pretty_deprecated', '(', 'deprecated', ')', ':', 'if', 'deprecated', '!=', '""', ':', 'deprecated', '=', '"\\n"', '.', 'join', '(', 'map', '(', 'lambda', 'n', ':', 'n', '[', '4', ':', ']', ',', 'deprecated', '.', 'split', '(', '"\\n"', ')', ')', ')', 'return', '"**Deprecated:**\\n%s\\n"', '%', 'deprecated']
Docstring: Makes the deprecated description pretty and returns a formatted string if `deprecated`    is not an empty string. Otherwise, returns None.    Expected input:        ...    Expected output:    **Deprecated:**    ...
*******__*******
Code:def generate_sql_markdown(jvm, path):\n    """\n    Generates a markdown file after listing the function information. The output file\n    is created in `path`.\n\n    Expected output:\n    ### NAME\n\n    USAGE\n\n    **Arguments:**\n\n    ARGUMENTS\n\n    **Examples:**\n\n    ```\n    EXAMPLES\n    ```\n\n    **Note:**\n\n    NOTE\n\n    **Since:** SINCE\n\n    **Deprecated:**\n\n    DEPRECATED\n\n    <br/>\n\n    """\n\n    with open(path, 'w') as mdfile:\n        for info in _list_function_infos(jvm):\n            name = info.name\n            usage = _make_pretty_usage(info.usage)\n            arguments = _make_pretty_arguments(info.arguments)\n            examples = _make_pretty_examples(info.examples)\n            note = _make_pretty_note(info.note)\n            since = info.since\n            deprecated = _make_pretty_deprecated(info.deprecated)\n\n            mdfile.write("### %s\n\n" % name)\n            if usage is not None:\n                mdfile.write("%s\n\n" % usage.strip())\n            if arguments is not None:\n                mdfile.write(arguments)\n            if examples is not None:\n                mdfile.write(examples)\n            if note is not None:\n                mdfile.write(note)\n            if since is not None and since != "":\n                mdfile.write("**Since:** %s\n\n" % since.strip())\n            if deprecated is not None:\n                mdfile.write(deprecated)\n            mdfile.write("<br/>\n\n")
Language: python
Code Tokens: ['def', 'generate_sql_markdown', '(', 'jvm', ',', 'path', ')', ':', 'with', 'open', '(', 'path', ',', "'w'", ')', 'as', 'mdfile', ':', 'for', 'info', 'in', '_list_function_infos', '(', 'jvm', ')', ':', 'name', '=', 'info', '.', 'name', 'usage', '=', '_make_pretty_usage', '(', 'info', '.', 'usage', ')', 'arguments', '=', '_make_pretty_arguments', '(', 'info', '.', 'arguments', ')', 'examples', '=', '_make_pretty_examples', '(', 'info', '.', 'examples', ')', 'note', '=', '_make_pretty_note', '(', 'info', '.', 'note', ')', 'since', '=', 'info', '.', 'since', 'deprecated', '=', '_make_pretty_deprecated', '(', 'info', '.', 'deprecated', ')', 'mdfile', '.', 'write', '(', '"### %s\\n\\n"', '%', 'name', ')', 'if', 'usage', 'is', 'not', 'None', ':', 'mdfile', '.', 'write', '(', '"%s\\n\\n"', '%', 'usage', '.', 'strip', '(', ')', ')', 'if', 'arguments', 'is', 'not', 'None', ':', 'mdfile', '.', 'write', '(', 'arguments', ')', 'if', 'examples', 'is', 'not', 'None', ':', 'mdfile', '.', 'write', '(', 'examples', ')', 'if', 'note', 'is', 'not', 'None', ':', 'mdfile', '.', 'write', '(', 'note', ')', 'if', 'since', 'is', 'not', 'None', 'and', 'since', '!=', '""', ':', 'mdfile', '.', 'write', '(', '"**Since:** %s\\n\\n"', '%', 'since', '.', 'strip', '(', ')', ')', 'if', 'deprecated', 'is', 'not', 'None', ':', 'mdfile', '.', 'write', '(', 'deprecated', ')', 'mdfile', '.', 'write', '(', '"<br/>\\n\\n"', ')']
Docstring: Generates a markdown file after listing the function information. The output file    is created in `path`.    Expected output:    ### NAME    USAGE    **Arguments:**    ARGUMENTS    **Examples:**    ```    EXAMPLES    ```    **Note:**    NOTE    **Since:** SINCE    **Deprecated:**    DEPRECATED    <br/>
*******__*******
Code:def predict(self, x):\n        """\n        Predict values for a single data point or an RDD of points\n        using the model trained.\n        """\n        if isinstance(x, RDD):\n            return x.map(lambda v: self.predict(v))\n\n        x = _convert_to_vector(x)\n        if self.numClasses == 2:\n            margin = self.weights.dot(x) + self._intercept\n            if margin > 0:\n                prob = 1 / (1 + exp(-margin))\n            else:\n                exp_margin = exp(margin)\n                prob = exp_margin / (1 + exp_margin)\n            if self._threshold is None:\n                return prob\n            else:\n                return 1 if prob > self._threshold else 0\n        else:\n            best_class = 0\n            max_margin = 0.0\n            if x.size + 1 == self._dataWithBiasSize:\n                for i in range(0, self._numClasses - 1):\n                    margin = x.dot(self._weightsMatrix[i][0:x.size]) + \\n                        self._weightsMatrix[i][x.size]\n                    if margin > max_margin:\n                        max_margin = margin\n                        best_class = i + 1\n            else:\n                for i in range(0, self._numClasses - 1):\n                    margin = x.dot(self._weightsMatrix[i])\n                    if margin > max_margin:\n                        max_margin = margin\n                        best_class = i + 1\n            return best_class
Language: python
Code Tokens: ['def', 'predict', '(', 'self', ',', 'x', ')', ':', 'if', 'isinstance', '(', 'x', ',', 'RDD', ')', ':', 'return', 'x', '.', 'map', '(', 'lambda', 'v', ':', 'self', '.', 'predict', '(', 'v', ')', ')', 'x', '=', '_convert_to_vector', '(', 'x', ')', 'if', 'self', '.', 'numClasses', '==', '2', ':', 'margin', '=', 'self', '.', 'weights', '.', 'dot', '(', 'x', ')', '+', 'self', '.', '_intercept', 'if', 'margin', '>', '0', ':', 'prob', '=', '1', '/', '(', '1', '+', 'exp', '(', '-', 'margin', ')', ')', 'else', ':', 'exp_margin', '=', 'exp', '(', 'margin', ')', 'prob', '=', 'exp_margin', '/', '(', '1', '+', 'exp_margin', ')', 'if', 'self', '.', '_threshold', 'is', 'None', ':', 'return', 'prob', 'else', ':', 'return', '1', 'if', 'prob', '>', 'self', '.', '_threshold', 'else', '0', 'else', ':', 'best_class', '=', '0', 'max_margin', '=', '0.0', 'if', 'x', '.', 'size', '+', '1', '==', 'self', '.', '_dataWithBiasSize', ':', 'for', 'i', 'in', 'range', '(', '0', ',', 'self', '.', '_numClasses', '-', '1', ')', ':', 'margin', '=', 'x', '.', 'dot', '(', 'self', '.', '_weightsMatrix', '[', 'i', ']', '[', '0', ':', 'x', '.', 'size', ']', ')', '+', 'self', '.', '_weightsMatrix', '[', 'i', ']', '[', 'x', '.', 'size', ']', 'if', 'margin', '>', 'max_margin', ':', 'max_margin', '=', 'margin', 'best_class', '=', 'i', '+', '1', 'else', ':', 'for', 'i', 'in', 'range', '(', '0', ',', 'self', '.', '_numClasses', '-', '1', ')', ':', 'margin', '=', 'x', '.', 'dot', '(', 'self', '.', '_weightsMatrix', '[', 'i', ']', ')', 'if', 'margin', '>', 'max_margin', ':', 'max_margin', '=', 'margin', 'best_class', '=', 'i', '+', '1', 'return', 'best_class']
Docstring: Predict values for a single data point or an RDD of points        using the model trained.
*******__*******
Code:def save(self, sc, path):\n        """\n        Save this model to the given path.\n        """\n        java_model = sc._jvm.org.apache.spark.mllib.classification.LogisticRegressionModel(\n            _py2java(sc, self._coeff), self.intercept, self.numFeatures, self.numClasses)\n        java_model.save(sc._jsc.sc(), path)
Language: python
Code Tokens: ['def', 'save', '(', 'self', ',', 'sc', ',', 'path', ')', ':', 'java_model', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'classification', '.', 'LogisticRegressionModel', '(', '_py2java', '(', 'sc', ',', 'self', '.', '_coeff', ')', ',', 'self', '.', 'intercept', ',', 'self', '.', 'numFeatures', ',', 'self', '.', 'numClasses', ')', 'java_model', '.', 'save', '(', 'sc', '.', '_jsc', '.', 'sc', '(', ')', ',', 'path', ')']
Docstring: Save this model to the given path.
*******__*******
Code:def train(cls, data, iterations=100, initialWeights=None, regParam=0.0, regType="l2",\n              intercept=False, corrections=10, tolerance=1e-6, validateData=True, numClasses=2):\n        """\n        Train a logistic regression model on the given data.\n\n        :param data:\n          The training data, an RDD of LabeledPoint.\n        :param iterations:\n          The number of iterations.\n          (default: 100)\n        :param initialWeights:\n          The initial weights.\n          (default: None)\n        :param regParam:\n          The regularizer parameter.\n          (default: 0.0)\n        :param regType:\n          The type of regularizer used for training our model.\n          Supported values:\n\n            - "l1" for using L1 regularization\n            - "l2" for using L2 regularization (default)\n            - None for no regularization\n        :param intercept:\n          Boolean parameter which indicates the use or not of the\n          augmented representation for training data (i.e., whether bias\n          features are activated or not).\n          (default: False)\n        :param corrections:\n          The number of corrections used in the LBFGS update.\n          If a known updater is used for binary classification,\n          it calls the ml implementation and this parameter will\n          have no effect. (default: 10)\n        :param tolerance:\n          The convergence tolerance of iterations for L-BFGS.\n          (default: 1e-6)\n        :param validateData:\n          Boolean parameter which indicates if the algorithm should\n          validate data before training.\n          (default: True)\n        :param numClasses:\n          The number of classes (i.e., outcomes) a label can take in\n          Multinomial Logistic Regression.\n          (default: 2)\n\n        >>> data = [\n        ...     LabeledPoint(0.0, [0.0, 1.0]),\n        ...     LabeledPoint(1.0, [1.0, 0.0]),\n        ... ]\n        >>> lrm = LogisticRegressionWithLBFGS.train(sc.parallelize(data), iterations=10)\n        >>> lrm.predict([1.0, 0.0])\n        1\n        >>> lrm.predict([0.0, 1.0])\n        0\n        """\n        def train(rdd, i):\n            return callMLlibFunc("trainLogisticRegressionModelWithLBFGS", rdd, int(iterations), i,\n                                 float(regParam), regType, bool(intercept), int(corrections),\n                                 float(tolerance), bool(validateData), int(numClasses))\n\n        if initialWeights is None:\n            if numClasses == 2:\n                initialWeights = [0.0] * len(data.first().features)\n            else:\n                if intercept:\n                    initialWeights = [0.0] * (len(data.first().features) + 1) * (numClasses - 1)\n                else:\n                    initialWeights = [0.0] * len(data.first().features) * (numClasses - 1)\n        return _regression_train_wrapper(train, LogisticRegressionModel, data, initialWeights)
Language: python
Code Tokens: ['def', 'train', '(', 'cls', ',', 'data', ',', 'iterations', '=', '100', ',', 'initialWeights', '=', 'None', ',', 'regParam', '=', '0.0', ',', 'regType', '=', '"l2"', ',', 'intercept', '=', 'False', ',', 'corrections', '=', '10', ',', 'tolerance', '=', '1e-6', ',', 'validateData', '=', 'True', ',', 'numClasses', '=', '2', ')', ':', 'def', 'train', '(', 'rdd', ',', 'i', ')', ':', 'return', 'callMLlibFunc', '(', '"trainLogisticRegressionModelWithLBFGS"', ',', 'rdd', ',', 'int', '(', 'iterations', ')', ',', 'i', ',', 'float', '(', 'regParam', ')', ',', 'regType', ',', 'bool', '(', 'intercept', ')', ',', 'int', '(', 'corrections', ')', ',', 'float', '(', 'tolerance', ')', ',', 'bool', '(', 'validateData', ')', ',', 'int', '(', 'numClasses', ')', ')', 'if', 'initialWeights', 'is', 'None', ':', 'if', 'numClasses', '==', '2', ':', 'initialWeights', '=', '[', '0.0', ']', '*', 'len', '(', 'data', '.', 'first', '(', ')', '.', 'features', ')', 'else', ':', 'if', 'intercept', ':', 'initialWeights', '=', '[', '0.0', ']', '*', '(', 'len', '(', 'data', '.', 'first', '(', ')', '.', 'features', ')', '+', '1', ')', '*', '(', 'numClasses', '-', '1', ')', 'else', ':', 'initialWeights', '=', '[', '0.0', ']', '*', 'len', '(', 'data', '.', 'first', '(', ')', '.', 'features', ')', '*', '(', 'numClasses', '-', '1', ')', 'return', '_regression_train_wrapper', '(', 'train', ',', 'LogisticRegressionModel', ',', 'data', ',', 'initialWeights', ')']
Docstring: Train a logistic regression model on the given data.        :param data:          The training data, an RDD of LabeledPoint.        :param iterations:          The number of iterations.          (default: 100)        :param initialWeights:          The initial weights.          (default: None)        :param regParam:          The regularizer parameter.          (default: 0.0)        :param regType:          The type of regularizer used for training our model.          Supported values:            - "l1" for using L1 regularization            - "l2" for using L2 regularization (default)            - None for no regularization        :param intercept:          Boolean parameter which indicates the use or not of the          augmented representation for training data (i.e., whether bias          features are activated or not).          (default: False)        :param corrections:          The number of corrections used in the LBFGS update.          If a known updater is used for binary classification,          it calls the ml implementation and this parameter will          have no effect. (default: 10)        :param tolerance:          The convergence tolerance of iterations for L-BFGS.          (default: 1e-6)        :param validateData:          Boolean parameter which indicates if the algorithm should          validate data before training.          (default: True)        :param numClasses:          The number of classes (i.e., outcomes) a label can take in          Multinomial Logistic Regression.          (default: 2)        >>> data = [        ...     LabeledPoint(0.0, [0.0, 1.0]),        ...     LabeledPoint(1.0, [1.0, 0.0]),        ... ]        >>> lrm = LogisticRegressionWithLBFGS.train(sc.parallelize(data), iterations=10)        >>> lrm.predict([1.0, 0.0])        1        >>> lrm.predict([0.0, 1.0])        0
*******__*******
Code:def predict(self, x):\n        """\n        Predict values for a single data point or an RDD of points\n        using the model trained.\n        """\n        if isinstance(x, RDD):\n            return x.map(lambda v: self.predict(v))\n\n        x = _convert_to_vector(x)\n        margin = self.weights.dot(x) + self.intercept\n        if self._threshold is None:\n            return margin\n        else:\n            return 1 if margin > self._threshold else 0
Language: python
Code Tokens: ['def', 'predict', '(', 'self', ',', 'x', ')', ':', 'if', 'isinstance', '(', 'x', ',', 'RDD', ')', ':', 'return', 'x', '.', 'map', '(', 'lambda', 'v', ':', 'self', '.', 'predict', '(', 'v', ')', ')', 'x', '=', '_convert_to_vector', '(', 'x', ')', 'margin', '=', 'self', '.', 'weights', '.', 'dot', '(', 'x', ')', '+', 'self', '.', 'intercept', 'if', 'self', '.', '_threshold', 'is', 'None', ':', 'return', 'margin', 'else', ':', 'return', '1', 'if', 'margin', '>', 'self', '.', '_threshold', 'else', '0']
Docstring: Predict values for a single data point or an RDD of points        using the model trained.
*******__*******
Code:def save(self, sc, path):\n        """\n        Save this model to the given path.\n        """\n        java_model = sc._jvm.org.apache.spark.mllib.classification.SVMModel(\n            _py2java(sc, self._coeff), self.intercept)\n        java_model.save(sc._jsc.sc(), path)
Language: python
Code Tokens: ['def', 'save', '(', 'self', ',', 'sc', ',', 'path', ')', ':', 'java_model', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'classification', '.', 'SVMModel', '(', '_py2java', '(', 'sc', ',', 'self', '.', '_coeff', ')', ',', 'self', '.', 'intercept', ')', 'java_model', '.', 'save', '(', 'sc', '.', '_jsc', '.', 'sc', '(', ')', ',', 'path', ')']
Docstring: Save this model to the given path.
*******__*******
Code:def load(cls, sc, path):\n        """\n        Load a model from the given path.\n        """\n        java_model = sc._jvm.org.apache.spark.mllib.classification.SVMModel.load(\n            sc._jsc.sc(), path)\n        weights = _java2py(sc, java_model.weights())\n        intercept = java_model.intercept()\n        threshold = java_model.getThreshold().get()\n        model = SVMModel(weights, intercept)\n        model.setThreshold(threshold)\n        return model
Language: python
Code Tokens: ['def', 'load', '(', 'cls', ',', 'sc', ',', 'path', ')', ':', 'java_model', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'classification', '.', 'SVMModel', '.', 'load', '(', 'sc', '.', '_jsc', '.', 'sc', '(', ')', ',', 'path', ')', 'weights', '=', '_java2py', '(', 'sc', ',', 'java_model', '.', 'weights', '(', ')', ')', 'intercept', '=', 'java_model', '.', 'intercept', '(', ')', 'threshold', '=', 'java_model', '.', 'getThreshold', '(', ')', '.', 'get', '(', ')', 'model', '=', 'SVMModel', '(', 'weights', ',', 'intercept', ')', 'model', '.', 'setThreshold', '(', 'threshold', ')', 'return', 'model']
Docstring: Load a model from the given path.
*******__*******
Code:def train(cls, data, lambda_=1.0):\n        """\n        Train a Naive Bayes model given an RDD of (label, features)\n        vectors.\n\n        This is the Multinomial NB (U{http://tinyurl.com/lsdw6p}) which\n        can handle all kinds of discrete data.  For example, by\n        converting documents into TF-IDF vectors, it can be used for\n        document classification. By making every vector a 0-1 vector,\n        it can also be used as Bernoulli NB (U{http://tinyurl.com/p7c96j6}).\n        The input feature values must be nonnegative.\n\n        :param data:\n          RDD of LabeledPoint.\n        :param lambda_:\n          The smoothing parameter.\n          (default: 1.0)\n        """\n        first = data.first()\n        if not isinstance(first, LabeledPoint):\n            raise ValueError("`data` should be an RDD of LabeledPoint")\n        labels, pi, theta = callMLlibFunc("trainNaiveBayesModel", data, lambda_)\n        return NaiveBayesModel(labels.toArray(), pi.toArray(), numpy.array(theta))
Language: python
Code Tokens: ['def', 'train', '(', 'cls', ',', 'data', ',', 'lambda_', '=', '1.0', ')', ':', 'first', '=', 'data', '.', 'first', '(', ')', 'if', 'not', 'isinstance', '(', 'first', ',', 'LabeledPoint', ')', ':', 'raise', 'ValueError', '(', '"`data` should be an RDD of LabeledPoint"', ')', 'labels', ',', 'pi', ',', 'theta', '=', 'callMLlibFunc', '(', '"trainNaiveBayesModel"', ',', 'data', ',', 'lambda_', ')', 'return', 'NaiveBayesModel', '(', 'labels', '.', 'toArray', '(', ')', ',', 'pi', '.', 'toArray', '(', ')', ',', 'numpy', '.', 'array', '(', 'theta', ')', ')']
Docstring: Train a Naive Bayes model given an RDD of (label, features)        vectors.        This is the Multinomial NB (U{http://tinyurl.com/lsdw6p}) which        can handle all kinds of discrete data.  For example, by        converting documents into TF-IDF vectors, it can be used for        document classification. By making every vector a 0-1 vector,        it can also be used as Bernoulli NB (U{http://tinyurl.com/p7c96j6}).        The input feature values must be nonnegative.        :param data:          RDD of LabeledPoint.        :param lambda_:          The smoothing parameter.          (default: 1.0)
*******__*******
Code:def heappush(heap, item):\n    """Push item onto heap, maintaining the heap invariant."""\n    heap.append(item)\n    _siftdown(heap, 0, len(heap)-1)
Language: python
Code Tokens: ['def', 'heappush', '(', 'heap', ',', 'item', ')', ':', 'heap', '.', 'append', '(', 'item', ')', '_siftdown', '(', 'heap', ',', '0', ',', 'len', '(', 'heap', ')', '-', '1', ')']
Docstring: Push item onto heap, maintaining the heap invariant.
*******__*******
Code:def heappop(heap):\n    """Pop the smallest item off the heap, maintaining the heap invariant."""\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup(heap, 0)\n        return returnitem\n    return lastelt
Language: python
Code Tokens: ['def', 'heappop', '(', 'heap', ')', ':', 'lastelt', '=', 'heap', '.', 'pop', '(', ')', '# raises appropriate IndexError if heap is empty', 'if', 'heap', ':', 'returnitem', '=', 'heap', '[', '0', ']', 'heap', '[', '0', ']', '=', 'lastelt', '_siftup', '(', 'heap', ',', '0', ')', 'return', 'returnitem', 'return', 'lastelt']
Docstring: Pop the smallest item off the heap, maintaining the heap invariant.
*******__*******
Code:def heapreplace(heap, item):\n    """Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)\n    """\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup(heap, 0)\n    return returnitem
Language: python
Code Tokens: ['def', 'heapreplace', '(', 'heap', ',', 'item', ')', ':', 'returnitem', '=', 'heap', '[', '0', ']', '# raises appropriate IndexError if heap is empty', 'heap', '[', '0', ']', '=', 'item', '_siftup', '(', 'heap', ',', '0', ')', 'return', 'returnitem']
Docstring: Pop and return the current smallest value, and add the new item.    This is more efficient than heappop() followed by heappush(), and can be    more appropriate when using a fixed-size heap.  Note that the value    returned may be larger than item!  That constrains reasonable uses of    this routine unless written as part of a conditional replacement:        if item > heap[0]:            item = heapreplace(heap, item)
*******__*******
Code:def heappushpop(heap, item):\n    """Fast version of a heappush followed by a heappop."""\n    if heap and heap[0] < item:\n        item, heap[0] = heap[0], item\n        _siftup(heap, 0)\n    return item
Language: python
Code Tokens: ['def', 'heappushpop', '(', 'heap', ',', 'item', ')', ':', 'if', 'heap', 'and', 'heap', '[', '0', ']', '<', 'item', ':', 'item', ',', 'heap', '[', '0', ']', '=', 'heap', '[', '0', ']', ',', 'item', '_siftup', '(', 'heap', ',', '0', ')', 'return', 'item']
Docstring: Fast version of a heappush followed by a heappop.
*******__*******
Code:def heapify(x):\n    """Transform list into a heap, in-place, in O(len(x)) time."""\n    n = len(x)\n    # Transform bottom-up.  The largest index there's any point to looking at\n    # is the largest with a child index in-range, so must have 2*i + 1 < n,\n    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so\n    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is\n    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.\n    for i in reversed(range(n//2)):\n        _siftup(x, i)
Language: python
Code Tokens: ['def', 'heapify', '(', 'x', ')', ':', 'n', '=', 'len', '(', 'x', ')', "# Transform bottom-up.  The largest index there's any point to looking at", '# is the largest with a child index in-range, so must have 2*i + 1 < n,', '# or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so', '# j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is', "# (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.", 'for', 'i', 'in', 'reversed', '(', 'range', '(', 'n', '//', '2', ')', ')', ':', '_siftup', '(', 'x', ',', 'i', ')']
Docstring: Transform list into a heap, in-place, in O(len(x)) time.
*******__*******
Code:def _heappop_max(heap):\n    """Maxheap version of a heappop."""\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup_max(heap, 0)\n        return returnitem\n    return lastelt
Language: python
Code Tokens: ['def', '_heappop_max', '(', 'heap', ')', ':', 'lastelt', '=', 'heap', '.', 'pop', '(', ')', '# raises appropriate IndexError if heap is empty', 'if', 'heap', ':', 'returnitem', '=', 'heap', '[', '0', ']', 'heap', '[', '0', ']', '=', 'lastelt', '_siftup_max', '(', 'heap', ',', '0', ')', 'return', 'returnitem', 'return', 'lastelt']
Docstring: Maxheap version of a heappop.
*******__*******
Code:def _heapreplace_max(heap, item):\n    """Maxheap version of a heappop followed by a heappush."""\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup_max(heap, 0)\n    return returnitem
Language: python
Code Tokens: ['def', '_heapreplace_max', '(', 'heap', ',', 'item', ')', ':', 'returnitem', '=', 'heap', '[', '0', ']', '# raises appropriate IndexError if heap is empty', 'heap', '[', '0', ']', '=', 'item', '_siftup_max', '(', 'heap', ',', '0', ')', 'return', 'returnitem']
Docstring: Maxheap version of a heappop followed by a heappush.
*******__*******
Code:def _heapify_max(x):\n    """Transform list into a maxheap, in-place, in O(len(x)) time."""\n    n = len(x)\n    for i in reversed(range(n//2)):\n        _siftup_max(x, i)
Language: python
Code Tokens: ['def', '_heapify_max', '(', 'x', ')', ':', 'n', '=', 'len', '(', 'x', ')', 'for', 'i', 'in', 'reversed', '(', 'range', '(', 'n', '//', '2', ')', ')', ':', '_siftup_max', '(', 'x', ',', 'i', ')']
Docstring: Transform list into a maxheap, in-place, in O(len(x)) time.
*******__*******
Code:def _siftdown_max(heap, startpos, pos):\n    'Maxheap variant of _siftdown'\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if parent < newitem:\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem
Language: python
Code Tokens: ['def', '_siftdown_max', '(', 'heap', ',', 'startpos', ',', 'pos', ')', ':', 'newitem', '=', 'heap', '[', 'pos', ']', '# Follow the path to the root, moving parents down until finding a place', '# newitem fits.', 'while', 'pos', '>', 'startpos', ':', 'parentpos', '=', '(', 'pos', '-', '1', ')', '>>', '1', 'parent', '=', 'heap', '[', 'parentpos', ']', 'if', 'parent', '<', 'newitem', ':', 'heap', '[', 'pos', ']', '=', 'parent', 'pos', '=', 'parentpos', 'continue', 'break', 'heap', '[', 'pos', ']', '=', 'newitem']
Docstring: Maxheap variant of _siftdown
*******__*******
Code:def _siftup_max(heap, pos):\n    'Maxheap variant of _siftup'\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the larger child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of larger child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not heap[rightpos] < heap[childpos]:\n            childpos = rightpos\n        # Move the larger child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown_max(heap, startpos, pos)
Language: python
Code Tokens: ['def', '_siftup_max', '(', 'heap', ',', 'pos', ')', ':', 'endpos', '=', 'len', '(', 'heap', ')', 'startpos', '=', 'pos', 'newitem', '=', 'heap', '[', 'pos', ']', '# Bubble up the larger child until hitting a leaf.', 'childpos', '=', '2', '*', 'pos', '+', '1', '# leftmost child position', 'while', 'childpos', '<', 'endpos', ':', '# Set childpos to index of larger child.', 'rightpos', '=', 'childpos', '+', '1', 'if', 'rightpos', '<', 'endpos', 'and', 'not', 'heap', '[', 'rightpos', ']', '<', 'heap', '[', 'childpos', ']', ':', 'childpos', '=', 'rightpos', '# Move the larger child up.', 'heap', '[', 'pos', ']', '=', 'heap', '[', 'childpos', ']', 'pos', '=', 'childpos', 'childpos', '=', '2', '*', 'pos', '+', '1', '# The leaf at pos is empty now.  Put newitem there, and bubble it up', '# to its final resting place (by sifting its parents down).', 'heap', '[', 'pos', ']', '=', 'newitem', '_siftdown_max', '(', 'heap', ',', 'startpos', ',', 'pos', ')']
Docstring: Maxheap variant of _siftup
*******__*******
Code:def merge(iterables, key=None, reverse=False):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    If *key* is not None, applies a key function to each element to determine\n    its sort order.\n\n    >>> list(merge(['dog', 'horse'], ['cat', 'fish', 'kangaroo'], key=len))\n    ['dog', 'cat', 'fish', 'horse', 'kangaroo']\n\n    '''\n\n    h = []\n    h_append = h.append\n\n    if reverse:\n        _heapify = _heapify_max\n        _heappop = _heappop_max\n        _heapreplace = _heapreplace_max\n        direction = -1\n    else:\n        _heapify = heapify\n        _heappop = heappop\n        _heapreplace = heapreplace\n        direction = 1\n\n    if key is None:\n        for order, it in enumerate(map(iter, iterables)):\n            try:\n                h_append([next(it), order * direction, it])\n            except StopIteration:\n                pass\n        _heapify(h)\n        while len(h) > 1:\n            try:\n                while True:\n                    value, order, it = s = h[0]\n                    yield value\n                    s[0] = next(it)           # raises StopIteration when exhausted\n                    _heapreplace(h, s)      # restore heap condition\n            except StopIteration:\n                _heappop(h)                 # remove empty iterator\n        if h:\n            # fast case when only a single iterator remains\n            value, order, it = h[0]\n            yield value\n            for value in it:\n                yield value\n        return\n\n    for order, it in enumerate(map(iter, iterables)):\n        try:\n            value = next(it)\n            h_append([key(value), order * direction, value, it])\n        except StopIteration:\n            pass\n    _heapify(h)\n    while len(h) > 1:\n        try:\n            while True:\n                key_value, order, value, it = s = h[0]\n                yield value\n                value = next(it)\n                s[0] = key(value)\n                s[2] = value\n                _heapreplace(h, s)\n        except StopIteration:\n            _heappop(h)\n    if h:\n        key_value, order, value, it = h[0]\n        yield value\n        for value in it:\n            yield value
Language: python
Code Tokens: ['def', 'merge', '(', 'iterables', ',', 'key', '=', 'None', ',', 'reverse', '=', 'False', ')', ':', 'h', '=', '[', ']', 'h_append', '=', 'h', '.', 'append', 'if', 'reverse', ':', '_heapify', '=', '_heapify_max', '_heappop', '=', '_heappop_max', '_heapreplace', '=', '_heapreplace_max', 'direction', '=', '-', '1', 'else', ':', '_heapify', '=', 'heapify', '_heappop', '=', 'heappop', '_heapreplace', '=', 'heapreplace', 'direction', '=', '1', 'if', 'key', 'is', 'None', ':', 'for', 'order', ',', 'it', 'in', 'enumerate', '(', 'map', '(', 'iter', ',', 'iterables', ')', ')', ':', 'try', ':', 'h_append', '(', '[', 'next', '(', 'it', ')', ',', 'order', '*', 'direction', ',', 'it', ']', ')', 'except', 'StopIteration', ':', 'pass', '_heapify', '(', 'h', ')', 'while', 'len', '(', 'h', ')', '>', '1', ':', 'try', ':', 'while', 'True', ':', 'value', ',', 'order', ',', 'it', '=', 's', '=', 'h', '[', '0', ']', 'yield', 'value', 's', '[', '0', ']', '=', 'next', '(', 'it', ')', '# raises StopIteration when exhausted', '_heapreplace', '(', 'h', ',', 's', ')', '# restore heap condition', 'except', 'StopIteration', ':', '_heappop', '(', 'h', ')', '# remove empty iterator', 'if', 'h', ':', '# fast case when only a single iterator remains', 'value', ',', 'order', ',', 'it', '=', 'h', '[', '0', ']', 'yield', 'value', 'for', 'value', 'in', 'it', ':', 'yield', 'value', 'return', 'for', 'order', ',', 'it', 'in', 'enumerate', '(', 'map', '(', 'iter', ',', 'iterables', ')', ')', ':', 'try', ':', 'value', '=', 'next', '(', 'it', ')', 'h_append', '(', '[', 'key', '(', 'value', ')', ',', 'order', '*', 'direction', ',', 'value', ',', 'it', ']', ')', 'except', 'StopIteration', ':', 'pass', '_heapify', '(', 'h', ')', 'while', 'len', '(', 'h', ')', '>', '1', ':', 'try', ':', 'while', 'True', ':', 'key_value', ',', 'order', ',', 'value', ',', 'it', '=', 's', '=', 'h', '[', '0', ']', 'yield', 'value', 'value', '=', 'next', '(', 'it', ')', 's', '[', '0', ']', '=', 'key', '(', 'value', ')', 's', '[', '2', ']', '=', 'value', '_heapreplace', '(', 'h', ',', 's', ')', 'except', 'StopIteration', ':', '_heappop', '(', 'h', ')', 'if', 'h', ':', 'key_value', ',', 'order', ',', 'value', ',', 'it', '=', 'h', '[', '0', ']', 'yield', 'value', 'for', 'value', 'in', 'it', ':', 'yield', 'value']
Docstring: Merge multiple sorted inputs into a single sorted output.    Similar to sorted(itertools.chain(*iterables)) but returns a generator,    does not pull the data into memory all at once, and assumes that each of    the input streams is already sorted (smallest to largest).    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]    If *key* is not None, applies a key function to each element to determine    its sort order.    >>> list(merge(['dog', 'horse'], ['cat', 'fish', 'kangaroo'], key=len))    ['dog', 'cat', 'fish', 'horse', 'kangaroo']
*******__*******
Code:def nsmallest(n, iterable, key=None):\n    """Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    """\n\n    # Short-cut for n==1 is to use min()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = min(it, default=sentinel)\n        else:\n            result = min(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        # put the range(n) first so that zip() doesn't\n        # consume one too many elements from the iterator\n        result = [(elem, i) for i, elem in zip(range(n), it)]\n        if not result:\n            return result\n        _heapify_max(result)\n        top = result[0][0]\n        order = n\n        _heapreplace = _heapreplace_max\n        for elem in it:\n            if elem < top:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order += 1\n        result.sort()\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(n), it)]\n    if not result:\n        return result\n    _heapify_max(result)\n    top = result[0][0]\n    order = n\n    _heapreplace = _heapreplace_max\n    for elem in it:\n        k = key(elem)\n        if k < top:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order += 1\n    result.sort()\n    return [r[2] for r in result]
Language: python
Code Tokens: ['def', 'nsmallest', '(', 'n', ',', 'iterable', ',', 'key', '=', 'None', ')', ':', '# Short-cut for n==1 is to use min()', 'if', 'n', '==', '1', ':', 'it', '=', 'iter', '(', 'iterable', ')', 'sentinel', '=', 'object', '(', ')', 'if', 'key', 'is', 'None', ':', 'result', '=', 'min', '(', 'it', ',', 'default', '=', 'sentinel', ')', 'else', ':', 'result', '=', 'min', '(', 'it', ',', 'default', '=', 'sentinel', ',', 'key', '=', 'key', ')', 'return', '[', ']', 'if', 'result', 'is', 'sentinel', 'else', '[', 'result', ']', "# When n>=size, it's faster to use sorted()", 'try', ':', 'size', '=', 'len', '(', 'iterable', ')', 'except', '(', 'TypeError', ',', 'AttributeError', ')', ':', 'pass', 'else', ':', 'if', 'n', '>=', 'size', ':', 'return', 'sorted', '(', 'iterable', ',', 'key', '=', 'key', ')', '[', ':', 'n', ']', '# When key is none, use simpler decoration', 'if', 'key', 'is', 'None', ':', 'it', '=', 'iter', '(', 'iterable', ')', "# put the range(n) first so that zip() doesn't", '# consume one too many elements from the iterator', 'result', '=', '[', '(', 'elem', ',', 'i', ')', 'for', 'i', ',', 'elem', 'in', 'zip', '(', 'range', '(', 'n', ')', ',', 'it', ')', ']', 'if', 'not', 'result', ':', 'return', 'result', '_heapify_max', '(', 'result', ')', 'top', '=', 'result', '[', '0', ']', '[', '0', ']', 'order', '=', 'n', '_heapreplace', '=', '_heapreplace_max', 'for', 'elem', 'in', 'it', ':', 'if', 'elem', '<', 'top', ':', '_heapreplace', '(', 'result', ',', '(', 'elem', ',', 'order', ')', ')', 'top', '=', 'result', '[', '0', ']', '[', '0', ']', 'order', '+=', '1', 'result', '.', 'sort', '(', ')', 'return', '[', 'r', '[', '0', ']', 'for', 'r', 'in', 'result', ']', '# General case, slowest method', 'it', '=', 'iter', '(', 'iterable', ')', 'result', '=', '[', '(', 'key', '(', 'elem', ')', ',', 'i', ',', 'elem', ')', 'for', 'i', ',', 'elem', 'in', 'zip', '(', 'range', '(', 'n', ')', ',', 'it', ')', ']', 'if', 'not', 'result', ':', 'return', 'result', '_heapify_max', '(', 'result', ')', 'top', '=', 'result', '[', '0', ']', '[', '0', ']', 'order', '=', 'n', '_heapreplace', '=', '_heapreplace_max', 'for', 'elem', 'in', 'it', ':', 'k', '=', 'key', '(', 'elem', ')', 'if', 'k', '<', 'top', ':', '_heapreplace', '(', 'result', ',', '(', 'k', ',', 'order', ',', 'elem', ')', ')', 'top', '=', 'result', '[', '0', ']', '[', '0', ']', 'order', '+=', '1', 'result', '.', 'sort', '(', ')', 'return', '[', 'r', '[', '2', ']', 'for', 'r', 'in', 'result', ']']
Docstring: Find the n smallest elements in a dataset.    Equivalent to:  sorted(iterable, key=key)[:n]
*******__*******
Code:def nlargest(n, iterable, key=None):\n    """Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    """\n\n    # Short-cut for n==1 is to use max()\n    if n == 1:\n        it = iter(iterable)\n        sentinel = object()\n        if key is None:\n            result = max(it, default=sentinel)\n        else:\n            result = max(it, default=sentinel, key=key)\n        return [] if result is sentinel else [result]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = iter(iterable)\n        result = [(elem, i) for i, elem in zip(range(0, -n, -1), it)]\n        if not result:\n            return result\n        heapify(result)\n        top = result[0][0]\n        order = -n\n        _heapreplace = heapreplace\n        for elem in it:\n            if top < elem:\n                _heapreplace(result, (elem, order))\n                top = result[0][0]\n                order -= 1\n        result.sort(reverse=True)\n        return [r[0] for r in result]\n\n    # General case, slowest method\n    it = iter(iterable)\n    result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]\n    if not result:\n        return result\n    heapify(result)\n    top = result[0][0]\n    order = -n\n    _heapreplace = heapreplace\n    for elem in it:\n        k = key(elem)\n        if top < k:\n            _heapreplace(result, (k, order, elem))\n            top = result[0][0]\n            order -= 1\n    result.sort(reverse=True)\n    return [r[2] for r in result]
Language: python
Code Tokens: ['def', 'nlargest', '(', 'n', ',', 'iterable', ',', 'key', '=', 'None', ')', ':', '# Short-cut for n==1 is to use max()', 'if', 'n', '==', '1', ':', 'it', '=', 'iter', '(', 'iterable', ')', 'sentinel', '=', 'object', '(', ')', 'if', 'key', 'is', 'None', ':', 'result', '=', 'max', '(', 'it', ',', 'default', '=', 'sentinel', ')', 'else', ':', 'result', '=', 'max', '(', 'it', ',', 'default', '=', 'sentinel', ',', 'key', '=', 'key', ')', 'return', '[', ']', 'if', 'result', 'is', 'sentinel', 'else', '[', 'result', ']', "# When n>=size, it's faster to use sorted()", 'try', ':', 'size', '=', 'len', '(', 'iterable', ')', 'except', '(', 'TypeError', ',', 'AttributeError', ')', ':', 'pass', 'else', ':', 'if', 'n', '>=', 'size', ':', 'return', 'sorted', '(', 'iterable', ',', 'key', '=', 'key', ',', 'reverse', '=', 'True', ')', '[', ':', 'n', ']', '# When key is none, use simpler decoration', 'if', 'key', 'is', 'None', ':', 'it', '=', 'iter', '(', 'iterable', ')', 'result', '=', '[', '(', 'elem', ',', 'i', ')', 'for', 'i', ',', 'elem', 'in', 'zip', '(', 'range', '(', '0', ',', '-', 'n', ',', '-', '1', ')', ',', 'it', ')', ']', 'if', 'not', 'result', ':', 'return', 'result', 'heapify', '(', 'result', ')', 'top', '=', 'result', '[', '0', ']', '[', '0', ']', 'order', '=', '-', 'n', '_heapreplace', '=', 'heapreplace', 'for', 'elem', 'in', 'it', ':', 'if', 'top', '<', 'elem', ':', '_heapreplace', '(', 'result', ',', '(', 'elem', ',', 'order', ')', ')', 'top', '=', 'result', '[', '0', ']', '[', '0', ']', 'order', '-=', '1', 'result', '.', 'sort', '(', 'reverse', '=', 'True', ')', 'return', '[', 'r', '[', '0', ']', 'for', 'r', 'in', 'result', ']', '# General case, slowest method', 'it', '=', 'iter', '(', 'iterable', ')', 'result', '=', '[', '(', 'key', '(', 'elem', ')', ',', 'i', ',', 'elem', ')', 'for', 'i', ',', 'elem', 'in', 'zip', '(', 'range', '(', '0', ',', '-', 'n', ',', '-', '1', ')', ',', 'it', ')', ']', 'if', 'not', 'result', ':', 'return', 'result', 'heapify', '(', 'result', ')', 'top', '=', 'result', '[', '0', ']', '[', '0', ']', 'order', '=', '-', 'n', '_heapreplace', '=', 'heapreplace', 'for', 'elem', 'in', 'it', ':', 'k', '=', 'key', '(', 'elem', ')', 'if', 'top', '<', 'k', ':', '_heapreplace', '(', 'result', ',', '(', 'k', ',', 'order', ',', 'elem', ')', ')', 'top', '=', 'result', '[', '0', ']', '[', '0', ']', 'order', '-=', '1', 'result', '.', 'sort', '(', 'reverse', '=', 'True', ')', 'return', '[', 'r', '[', '2', ']', 'for', 'r', 'in', 'result', ']']
Docstring: Find the n largest elements in a dataset.    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]
*******__*******
Code:def corr(dataset, column, method="pearson"):\n        """\n        Compute the correlation matrix with specified method using dataset.\n\n        :param dataset:\n          A Dataset or a DataFrame.\n        :param column:\n          The name of the column of vectors for which the correlation coefficient needs\n          to be computed. This must be a column of the dataset, and it must contain\n          Vector objects.\n        :param method:\n          String specifying the method to use for computing correlation.\n          Supported: `pearson` (default), `spearman`.\n        :return:\n          A DataFrame that contains the correlation matrix of the column of vectors. This\n          DataFrame contains a single row and a single column of name\n          '$METHODNAME($COLUMN)'.\n\n        >>> from pyspark.ml.linalg import Vectors\n        >>> from pyspark.ml.stat import Correlation\n        >>> dataset = [[Vectors.dense([1, 0, 0, -2])],\n        ...            [Vectors.dense([4, 5, 0, 3])],\n        ...            [Vectors.dense([6, 7, 0, 8])],\n        ...            [Vectors.dense([9, 0, 0, 1])]]\n        >>> dataset = spark.createDataFrame(dataset, ['features'])\n        >>> pearsonCorr = Correlation.corr(dataset, 'features', 'pearson').collect()[0][0]\n        >>> print(str(pearsonCorr).replace('nan', 'NaN'))\n        DenseMatrix([[ 1.        ,  0.0556...,         NaN,  0.4004...],\n                     [ 0.0556...,  1.        ,         NaN,  0.9135...],\n                     [        NaN,         NaN,  1.        ,         NaN],\n                     [ 0.4004...,  0.9135...,         NaN,  1.        ]])\n        >>> spearmanCorr = Correlation.corr(dataset, 'features', method='spearman').collect()[0][0]\n        >>> print(str(spearmanCorr).replace('nan', 'NaN'))\n        DenseMatrix([[ 1.        ,  0.1054...,         NaN,  0.4       ],\n                     [ 0.1054...,  1.        ,         NaN,  0.9486... ],\n                     [        NaN,         NaN,  1.        ,         NaN],\n                     [ 0.4       ,  0.9486... ,         NaN,  1.        ]])\n        """\n        sc = SparkContext._active_spark_context\n        javaCorrObj = _jvm().org.apache.spark.ml.stat.Correlation\n        args = [_py2java(sc, arg) for arg in (dataset, column, method)]\n        return _java2py(sc, javaCorrObj.corr(*args))
Language: python
Code Tokens: ['def', 'corr', '(', 'dataset', ',', 'column', ',', 'method', '=', '"pearson"', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'javaCorrObj', '=', '_jvm', '(', ')', '.', 'org', '.', 'apache', '.', 'spark', '.', 'ml', '.', 'stat', '.', 'Correlation', 'args', '=', '[', '_py2java', '(', 'sc', ',', 'arg', ')', 'for', 'arg', 'in', '(', 'dataset', ',', 'column', ',', 'method', ')', ']', 'return', '_java2py', '(', 'sc', ',', 'javaCorrObj', '.', 'corr', '(', '*', 'args', ')', ')']
Docstring: Compute the correlation matrix with specified method using dataset.        :param dataset:          A Dataset or a DataFrame.        :param column:          The name of the column of vectors for which the correlation coefficient needs          to be computed. This must be a column of the dataset, and it must contain          Vector objects.        :param method:          String specifying the method to use for computing correlation.          Supported: `pearson` (default), `spearman`.        :return:          A DataFrame that contains the correlation matrix of the column of vectors. This          DataFrame contains a single row and a single column of name          '$METHODNAME($COLUMN)'.        >>> from pyspark.ml.linalg import Vectors        >>> from pyspark.ml.stat import Correlation        >>> dataset = [[Vectors.dense([1, 0, 0, -2])],        ...            [Vectors.dense([4, 5, 0, 3])],        ...            [Vectors.dense([6, 7, 0, 8])],        ...            [Vectors.dense([9, 0, 0, 1])]]        >>> dataset = spark.createDataFrame(dataset, ['features'])        >>> pearsonCorr = Correlation.corr(dataset, 'features', 'pearson').collect()[0][0]        >>> print(str(pearsonCorr).replace('nan', 'NaN'))        DenseMatrix([[ 1.        ,  0.0556...,         NaN,  0.4004...],                     [ 0.0556...,  1.        ,         NaN,  0.9135...],                     [        NaN,         NaN,  1.        ,         NaN],                     [ 0.4004...,  0.9135...,         NaN,  1.        ]])        >>> spearmanCorr = Correlation.corr(dataset, 'features', method='spearman').collect()[0][0]        >>> print(str(spearmanCorr).replace('nan', 'NaN'))        DenseMatrix([[ 1.        ,  0.1054...,         NaN,  0.4       ],                     [ 0.1054...,  1.        ,         NaN,  0.9486... ],                     [        NaN,         NaN,  1.        ,         NaN],                     [ 0.4       ,  0.9486... ,         NaN,  1.        ]])
*******__*******
Code:def metrics(*metrics):\n        """\n        Given a list of metrics, provides a builder that it turns computes metrics from a column.\n\n        See the documentation of [[Summarizer]] for an example.\n\n        The following metrics are accepted (case sensitive):\n         - mean: a vector that contains the coefficient-wise mean.\n         - variance: a vector tha contains the coefficient-wise variance.\n         - count: the count of all vectors seen.\n         - numNonzeros: a vector with the number of non-zeros for each coefficients\n         - max: the maximum for each coefficient.\n         - min: the minimum for each coefficient.\n         - normL2: the Euclidean norm for each coefficient.\n         - normL1: the L1 norm of each coefficient (sum of the absolute values).\n\n        :param metrics:\n         metrics that can be provided.\n        :return:\n         an object of :py:class:`pyspark.ml.stat.SummaryBuilder`\n\n        Note: Currently, the performance of this interface is about 2x~3x slower then using the RDD\n        interface.\n        """\n        sc = SparkContext._active_spark_context\n        js = JavaWrapper._new_java_obj("org.apache.spark.ml.stat.Summarizer.metrics",\n                                       _to_seq(sc, metrics))\n        return SummaryBuilder(js)
Language: python
Code Tokens: ['def', 'metrics', '(', '*', 'metrics', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'js', '=', 'JavaWrapper', '.', '_new_java_obj', '(', '"org.apache.spark.ml.stat.Summarizer.metrics"', ',', '_to_seq', '(', 'sc', ',', 'metrics', ')', ')', 'return', 'SummaryBuilder', '(', 'js', ')']
Docstring: Given a list of metrics, provides a builder that it turns computes metrics from a column.        See the documentation of [[Summarizer]] for an example.        The following metrics are accepted (case sensitive):         - mean: a vector that contains the coefficient-wise mean.         - variance: a vector tha contains the coefficient-wise variance.         - count: the count of all vectors seen.         - numNonzeros: a vector with the number of non-zeros for each coefficients         - max: the maximum for each coefficient.         - min: the minimum for each coefficient.         - normL2: the Euclidean norm for each coefficient.         - normL1: the L1 norm of each coefficient (sum of the absolute values).        :param metrics:         metrics that can be provided.        :return:         an object of :py:class:`pyspark.ml.stat.SummaryBuilder`        Note: Currently, the performance of this interface is about 2x~3x slower then using the RDD        interface.
*******__*******
Code:def summary(self, featuresCol, weightCol=None):\n        """\n        Returns an aggregate object that contains the summary of the column with the requested\n        metrics.\n\n        :param featuresCol:\n         a column that contains features Vector object.\n        :param weightCol:\n         a column that contains weight value. Default weight is 1.0.\n        :return:\n         an aggregate column that contains the statistics. The exact content of this\n         structure is determined during the creation of the builder.\n        """\n        featuresCol, weightCol = Summarizer._check_param(featuresCol, weightCol)\n        return Column(self._java_obj.summary(featuresCol._jc, weightCol._jc))
Language: python
Code Tokens: ['def', 'summary', '(', 'self', ',', 'featuresCol', ',', 'weightCol', '=', 'None', ')', ':', 'featuresCol', ',', 'weightCol', '=', 'Summarizer', '.', '_check_param', '(', 'featuresCol', ',', 'weightCol', ')', 'return', 'Column', '(', 'self', '.', '_java_obj', '.', 'summary', '(', 'featuresCol', '.', '_jc', ',', 'weightCol', '.', '_jc', ')', ')']
Docstring: Returns an aggregate object that contains the summary of the column with the requested        metrics.        :param featuresCol:         a column that contains features Vector object.        :param weightCol:         a column that contains weight value. Default weight is 1.0.        :return:         an aggregate column that contains the statistics. The exact content of this         structure is determined during the creation of the builder.
*******__*******
Code:def corr(x, y=None, method=None):\n        """\n        Compute the correlation (matrix) for the input RDD(s) using the\n        specified method.\n        Methods currently supported: I{pearson (default), spearman}.\n\n        If a single RDD of Vectors is passed in, a correlation matrix\n        comparing the columns in the input RDD is returned. Use C{method=}\n        to specify the method to be used for single RDD inout.\n        If two RDDs of floats are passed in, a single float is returned.\n\n        :param x: an RDD of vector for which the correlation matrix is to be computed,\n                  or an RDD of float of the same cardinality as y when y is specified.\n        :param y: an RDD of float of the same cardinality as x.\n        :param method: String specifying the method to use for computing correlation.\n                       Supported: `pearson` (default), `spearman`\n        :return: Correlation matrix comparing columns in x.\n\n        >>> x = sc.parallelize([1.0, 0.0, -2.0], 2)\n        >>> y = sc.parallelize([4.0, 5.0, 3.0], 2)\n        >>> zeros = sc.parallelize([0.0, 0.0, 0.0], 2)\n        >>> abs(Statistics.corr(x, y) - 0.6546537) < 1e-7\n        True\n        >>> Statistics.corr(x, y) == Statistics.corr(x, y, "pearson")\n        True\n        >>> Statistics.corr(x, y, "spearman")\n        0.5\n        >>> from math import isnan\n        >>> isnan(Statistics.corr(x, zeros))\n        True\n        >>> from pyspark.mllib.linalg import Vectors\n        >>> rdd = sc.parallelize([Vectors.dense([1, 0, 0, -2]), Vectors.dense([4, 5, 0, 3]),\n        ...                       Vectors.dense([6, 7, 0,  8]), Vectors.dense([9, 0, 0, 1])])\n        >>> pearsonCorr = Statistics.corr(rdd)\n        >>> print(str(pearsonCorr).replace('nan', 'NaN'))\n        [[ 1.          0.05564149         NaN  0.40047142]\n         [ 0.05564149  1.                 NaN  0.91359586]\n         [        NaN         NaN  1.                 NaN]\n         [ 0.40047142  0.91359586         NaN  1.        ]]\n        >>> spearmanCorr = Statistics.corr(rdd, method="spearman")\n        >>> print(str(spearmanCorr).replace('nan', 'NaN'))\n        [[ 1.          0.10540926         NaN  0.4       ]\n         [ 0.10540926  1.                 NaN  0.9486833 ]\n         [        NaN         NaN  1.                 NaN]\n         [ 0.4         0.9486833          NaN  1.        ]]\n        >>> try:\n        ...     Statistics.corr(rdd, "spearman")\n        ...     print("Method name as second argument without 'method=' shouldn't be allowed.")\n        ... except TypeError:\n        ...     pass\n        """\n        # Check inputs to determine whether a single value or a matrix is needed for output.\n        # Since it's legal for users to use the method name as the second argument, we need to\n        # check if y is used to specify the method name instead.\n        if type(y) == str:\n            raise TypeError("Use 'method=' to specify method name.")\n\n        if not y:\n            return callMLlibFunc("corr", x.map(_convert_to_vector), method).toArray()\n        else:\n            return callMLlibFunc("corr", x.map(float), y.map(float), method)
Language: python
Code Tokens: ['def', 'corr', '(', 'x', ',', 'y', '=', 'None', ',', 'method', '=', 'None', ')', ':', '# Check inputs to determine whether a single value or a matrix is needed for output.', "# Since it's legal for users to use the method name as the second argument, we need to", '# check if y is used to specify the method name instead.', 'if', 'type', '(', 'y', ')', '==', 'str', ':', 'raise', 'TypeError', '(', '"Use \'method=\' to specify method name."', ')', 'if', 'not', 'y', ':', 'return', 'callMLlibFunc', '(', '"corr"', ',', 'x', '.', 'map', '(', '_convert_to_vector', ')', ',', 'method', ')', '.', 'toArray', '(', ')', 'else', ':', 'return', 'callMLlibFunc', '(', '"corr"', ',', 'x', '.', 'map', '(', 'float', ')', ',', 'y', '.', 'map', '(', 'float', ')', ',', 'method', ')']
Docstring: Compute the correlation (matrix) for the input RDD(s) using the        specified method.        Methods currently supported: I{pearson (default), spearman}.        If a single RDD of Vectors is passed in, a correlation matrix        comparing the columns in the input RDD is returned. Use C{method=}        to specify the method to be used for single RDD inout.        If two RDDs of floats are passed in, a single float is returned.        :param x: an RDD of vector for which the correlation matrix is to be computed,                  or an RDD of float of the same cardinality as y when y is specified.        :param y: an RDD of float of the same cardinality as x.        :param method: String specifying the method to use for computing correlation.                       Supported: `pearson` (default), `spearman`        :return: Correlation matrix comparing columns in x.        >>> x = sc.parallelize([1.0, 0.0, -2.0], 2)        >>> y = sc.parallelize([4.0, 5.0, 3.0], 2)        >>> zeros = sc.parallelize([0.0, 0.0, 0.0], 2)        >>> abs(Statistics.corr(x, y) - 0.6546537) < 1e-7        True        >>> Statistics.corr(x, y) == Statistics.corr(x, y, "pearson")        True        >>> Statistics.corr(x, y, "spearman")        0.5        >>> from math import isnan        >>> isnan(Statistics.corr(x, zeros))        True        >>> from pyspark.mllib.linalg import Vectors        >>> rdd = sc.parallelize([Vectors.dense([1, 0, 0, -2]), Vectors.dense([4, 5, 0, 3]),        ...                       Vectors.dense([6, 7, 0,  8]), Vectors.dense([9, 0, 0, 1])])        >>> pearsonCorr = Statistics.corr(rdd)        >>> print(str(pearsonCorr).replace('nan', 'NaN'))        [[ 1.          0.05564149         NaN  0.40047142]         [ 0.05564149  1.                 NaN  0.91359586]         [        NaN         NaN  1.                 NaN]         [ 0.40047142  0.91359586         NaN  1.        ]]        >>> spearmanCorr = Statistics.corr(rdd, method="spearman")        >>> print(str(spearmanCorr).replace('nan', 'NaN'))        [[ 1.          0.10540926         NaN  0.4       ]         [ 0.10540926  1.                 NaN  0.9486833 ]         [        NaN         NaN  1.                 NaN]         [ 0.4         0.9486833          NaN  1.        ]]        >>> try:        ...     Statistics.corr(rdd, "spearman")        ...     print("Method name as second argument without 'method=' shouldn't be allowed.")        ... except TypeError:        ...     pass
*******__*******
Code:def _parallelFitTasks(est, train, eva, validation, epm, collectSubModel):\n    """\n    Creates a list of callables which can be called from different threads to fit and evaluate\n    an estimator in parallel. Each callable returns an `(index, metric)` pair.\n\n    :param est: Estimator, the estimator to be fit.\n    :param train: DataFrame, training data set, used for fitting.\n    :param eva: Evaluator, used to compute `metric`\n    :param validation: DataFrame, validation data set, used for evaluation.\n    :param epm: Sequence of ParamMap, params maps to be used during fitting & evaluation.\n    :param collectSubModel: Whether to collect sub model.\n    :return: (int, float, subModel), an index into `epm` and the associated metric value.\n    """\n    modelIter = est.fitMultiple(train, epm)\n\n    def singleTask():\n        index, model = next(modelIter)\n        metric = eva.evaluate(model.transform(validation, epm[index]))\n        return index, metric, model if collectSubModel else None\n\n    return [singleTask] * len(epm)
Language: python
Code Tokens: ['def', '_parallelFitTasks', '(', 'est', ',', 'train', ',', 'eva', ',', 'validation', ',', 'epm', ',', 'collectSubModel', ')', ':', 'modelIter', '=', 'est', '.', 'fitMultiple', '(', 'train', ',', 'epm', ')', 'def', 'singleTask', '(', ')', ':', 'index', ',', 'model', '=', 'next', '(', 'modelIter', ')', 'metric', '=', 'eva', '.', 'evaluate', '(', 'model', '.', 'transform', '(', 'validation', ',', 'epm', '[', 'index', ']', ')', ')', 'return', 'index', ',', 'metric', ',', 'model', 'if', 'collectSubModel', 'else', 'None', 'return', '[', 'singleTask', ']', '*', 'len', '(', 'epm', ')']
Docstring: Creates a list of callables which can be called from different threads to fit and evaluate    an estimator in parallel. Each callable returns an `(index, metric)` pair.    :param est: Estimator, the estimator to be fit.    :param train: DataFrame, training data set, used for fitting.    :param eva: Evaluator, used to compute `metric`    :param validation: DataFrame, validation data set, used for evaluation.    :param epm: Sequence of ParamMap, params maps to be used during fitting & evaluation.    :param collectSubModel: Whether to collect sub model.    :return: (int, float, subModel), an index into `epm` and the associated metric value.
*******__*******
Code:def baseOn(self, *args):\n        """\n        Sets the given parameters in this grid to fixed values.\n        Accepts either a parameter dictionary or a list of (parameter, value) pairs.\n        """\n        if isinstance(args[0], dict):\n            self.baseOn(*args[0].items())\n        else:\n            for (param, value) in args:\n                self.addGrid(param, [value])\n\n        return self
Language: python
Code Tokens: ['def', 'baseOn', '(', 'self', ',', '*', 'args', ')', ':', 'if', 'isinstance', '(', 'args', '[', '0', ']', ',', 'dict', ')', ':', 'self', '.', 'baseOn', '(', '*', 'args', '[', '0', ']', '.', 'items', '(', ')', ')', 'else', ':', 'for', '(', 'param', ',', 'value', ')', 'in', 'args', ':', 'self', '.', 'addGrid', '(', 'param', ',', '[', 'value', ']', ')', 'return', 'self']
Docstring: Sets the given parameters in this grid to fixed values.        Accepts either a parameter dictionary or a list of (parameter, value) pairs.
*******__*******
Code:def build(self):\n        """\n        Builds and returns all combinations of parameters specified\n        by the param grid.\n        """\n        keys = self._param_grid.keys()\n        grid_values = self._param_grid.values()\n\n        def to_key_value_pairs(keys, values):\n            return [(key, key.typeConverter(value)) for key, value in zip(keys, values)]\n\n        return [dict(to_key_value_pairs(keys, prod)) for prod in itertools.product(*grid_values)]
Language: python
Code Tokens: ['def', 'build', '(', 'self', ')', ':', 'keys', '=', 'self', '.', '_param_grid', '.', 'keys', '(', ')', 'grid_values', '=', 'self', '.', '_param_grid', '.', 'values', '(', ')', 'def', 'to_key_value_pairs', '(', 'keys', ',', 'values', ')', ':', 'return', '[', '(', 'key', ',', 'key', '.', 'typeConverter', '(', 'value', ')', ')', 'for', 'key', ',', 'value', 'in', 'zip', '(', 'keys', ',', 'values', ')', ']', 'return', '[', 'dict', '(', 'to_key_value_pairs', '(', 'keys', ',', 'prod', ')', ')', 'for', 'prod', 'in', 'itertools', '.', 'product', '(', '*', 'grid_values', ')', ']']
Docstring: Builds and returns all combinations of parameters specified        by the param grid.
*******__*******
Code:def _from_java_impl(cls, java_stage):\n        """\n        Return Python estimator, estimatorParamMaps, and evaluator from a Java ValidatorParams.\n        """\n\n        # Load information from java_stage to the instance.\n        estimator = JavaParams._from_java(java_stage.getEstimator())\n        evaluator = JavaParams._from_java(java_stage.getEvaluator())\n        epms = [estimator._transfer_param_map_from_java(epm)\n                for epm in java_stage.getEstimatorParamMaps()]\n        return estimator, epms, evaluator
Language: python
Code Tokens: ['def', '_from_java_impl', '(', 'cls', ',', 'java_stage', ')', ':', '# Load information from java_stage to the instance.', 'estimator', '=', 'JavaParams', '.', '_from_java', '(', 'java_stage', '.', 'getEstimator', '(', ')', ')', 'evaluator', '=', 'JavaParams', '.', '_from_java', '(', 'java_stage', '.', 'getEvaluator', '(', ')', ')', 'epms', '=', '[', 'estimator', '.', '_transfer_param_map_from_java', '(', 'epm', ')', 'for', 'epm', 'in', 'java_stage', '.', 'getEstimatorParamMaps', '(', ')', ']', 'return', 'estimator', ',', 'epms', ',', 'evaluator']
Docstring: Return Python estimator, estimatorParamMaps, and evaluator from a Java ValidatorParams.
*******__*******
Code:def _to_java_impl(self):\n        """\n        Return Java estimator, estimatorParamMaps, and evaluator from this Python instance.\n        """\n\n        gateway = SparkContext._gateway\n        cls = SparkContext._jvm.org.apache.spark.ml.param.ParamMap\n\n        java_epms = gateway.new_array(cls, len(self.getEstimatorParamMaps()))\n        for idx, epm in enumerate(self.getEstimatorParamMaps()):\n            java_epms[idx] = self.getEstimator()._transfer_param_map_to_java(epm)\n\n        java_estimator = self.getEstimator()._to_java()\n        java_evaluator = self.getEvaluator()._to_java()\n        return java_estimator, java_epms, java_evaluator
Language: python
Code Tokens: ['def', '_to_java_impl', '(', 'self', ')', ':', 'gateway', '=', 'SparkContext', '.', '_gateway', 'cls', '=', 'SparkContext', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'ml', '.', 'param', '.', 'ParamMap', 'java_epms', '=', 'gateway', '.', 'new_array', '(', 'cls', ',', 'len', '(', 'self', '.', 'getEstimatorParamMaps', '(', ')', ')', ')', 'for', 'idx', ',', 'epm', 'in', 'enumerate', '(', 'self', '.', 'getEstimatorParamMaps', '(', ')', ')', ':', 'java_epms', '[', 'idx', ']', '=', 'self', '.', 'getEstimator', '(', ')', '.', '_transfer_param_map_to_java', '(', 'epm', ')', 'java_estimator', '=', 'self', '.', 'getEstimator', '(', ')', '.', '_to_java', '(', ')', 'java_evaluator', '=', 'self', '.', 'getEvaluator', '(', ')', '.', '_to_java', '(', ')', 'return', 'java_estimator', ',', 'java_epms', ',', 'java_evaluator']
Docstring: Return Java estimator, estimatorParamMaps, and evaluator from this Python instance.
*******__*******
Code:def _from_java(cls, java_stage):\n        """\n        Given a Java CrossValidator, create and return a Python wrapper of it.\n        Used for ML persistence.\n        """\n\n        estimator, epms, evaluator = super(CrossValidator, cls)._from_java_impl(java_stage)\n        numFolds = java_stage.getNumFolds()\n        seed = java_stage.getSeed()\n        parallelism = java_stage.getParallelism()\n        collectSubModels = java_stage.getCollectSubModels()\n        # Create a new instance of this stage.\n        py_stage = cls(estimator=estimator, estimatorParamMaps=epms, evaluator=evaluator,\n                       numFolds=numFolds, seed=seed, parallelism=parallelism,\n                       collectSubModels=collectSubModels)\n        py_stage._resetUid(java_stage.uid())\n        return py_stage
Language: python
Code Tokens: ['def', '_from_java', '(', 'cls', ',', 'java_stage', ')', ':', 'estimator', ',', 'epms', ',', 'evaluator', '=', 'super', '(', 'CrossValidator', ',', 'cls', ')', '.', '_from_java_impl', '(', 'java_stage', ')', 'numFolds', '=', 'java_stage', '.', 'getNumFolds', '(', ')', 'seed', '=', 'java_stage', '.', 'getSeed', '(', ')', 'parallelism', '=', 'java_stage', '.', 'getParallelism', '(', ')', 'collectSubModels', '=', 'java_stage', '.', 'getCollectSubModels', '(', ')', '# Create a new instance of this stage.', 'py_stage', '=', 'cls', '(', 'estimator', '=', 'estimator', ',', 'estimatorParamMaps', '=', 'epms', ',', 'evaluator', '=', 'evaluator', ',', 'numFolds', '=', 'numFolds', ',', 'seed', '=', 'seed', ',', 'parallelism', '=', 'parallelism', ',', 'collectSubModels', '=', 'collectSubModels', ')', 'py_stage', '.', '_resetUid', '(', 'java_stage', '.', 'uid', '(', ')', ')', 'return', 'py_stage']
Docstring: Given a Java CrossValidator, create and return a Python wrapper of it.        Used for ML persistence.
*******__*******
Code:def _to_java(self):\n        """\n        Transfer this instance to a Java CrossValidator. Used for ML persistence.\n\n        :return: Java object equivalent to this instance.\n        """\n\n        estimator, epms, evaluator = super(CrossValidator, self)._to_java_impl()\n\n        _java_obj = JavaParams._new_java_obj("org.apache.spark.ml.tuning.CrossValidator", self.uid)\n        _java_obj.setEstimatorParamMaps(epms)\n        _java_obj.setEvaluator(evaluator)\n        _java_obj.setEstimator(estimator)\n        _java_obj.setSeed(self.getSeed())\n        _java_obj.setNumFolds(self.getNumFolds())\n        _java_obj.setParallelism(self.getParallelism())\n        _java_obj.setCollectSubModels(self.getCollectSubModels())\n\n        return _java_obj
Language: python
Code Tokens: ['def', '_to_java', '(', 'self', ')', ':', 'estimator', ',', 'epms', ',', 'evaluator', '=', 'super', '(', 'CrossValidator', ',', 'self', ')', '.', '_to_java_impl', '(', ')', '_java_obj', '=', 'JavaParams', '.', '_new_java_obj', '(', '"org.apache.spark.ml.tuning.CrossValidator"', ',', 'self', '.', 'uid', ')', '_java_obj', '.', 'setEstimatorParamMaps', '(', 'epms', ')', '_java_obj', '.', 'setEvaluator', '(', 'evaluator', ')', '_java_obj', '.', 'setEstimator', '(', 'estimator', ')', '_java_obj', '.', 'setSeed', '(', 'self', '.', 'getSeed', '(', ')', ')', '_java_obj', '.', 'setNumFolds', '(', 'self', '.', 'getNumFolds', '(', ')', ')', '_java_obj', '.', 'setParallelism', '(', 'self', '.', 'getParallelism', '(', ')', ')', '_java_obj', '.', 'setCollectSubModels', '(', 'self', '.', 'getCollectSubModels', '(', ')', ')', 'return', '_java_obj']
Docstring: Transfer this instance to a Java CrossValidator. Used for ML persistence.        :return: Java object equivalent to this instance.
*******__*******
Code:def copy(self, extra=None):\n        """\n        Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies the underlying bestModel,\n        creates a deep copy of the embedded paramMap, and\n        copies the embedded and extra parameters over.\n        It does not copy the extra Params into the subModels.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance\n        """\n        if extra is None:\n            extra = dict()\n        bestModel = self.bestModel.copy(extra)\n        avgMetrics = self.avgMetrics\n        subModels = self.subModels\n        return CrossValidatorModel(bestModel, avgMetrics, subModels)
Language: python
Code Tokens: ['def', 'copy', '(', 'self', ',', 'extra', '=', 'None', ')', ':', 'if', 'extra', 'is', 'None', ':', 'extra', '=', 'dict', '(', ')', 'bestModel', '=', 'self', '.', 'bestModel', '.', 'copy', '(', 'extra', ')', 'avgMetrics', '=', 'self', '.', 'avgMetrics', 'subModels', '=', 'self', '.', 'subModels', 'return', 'CrossValidatorModel', '(', 'bestModel', ',', 'avgMetrics', ',', 'subModels', ')']
Docstring: Creates a copy of this instance with a randomly generated uid        and some extra params. This copies the underlying bestModel,        creates a deep copy of the embedded paramMap, and        copies the embedded and extra parameters over.        It does not copy the extra Params into the subModels.        :param extra: Extra parameters to copy to the new instance        :return: Copy of this instance
*******__*******
Code:def setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\n                  parallelism=1, collectSubModels=False, seed=None):\n        """\n        setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\\n                  parallelism=1, collectSubModels=False, seed=None):\n        Sets params for the train validation split.\n        """\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)
Language: python
Code Tokens: ['def', 'setParams', '(', 'self', ',', 'estimator', '=', 'None', ',', 'estimatorParamMaps', '=', 'None', ',', 'evaluator', '=', 'None', ',', 'trainRatio', '=', '0.75', ',', 'parallelism', '=', '1', ',', 'collectSubModels', '=', 'False', ',', 'seed', '=', 'None', ')', ':', 'kwargs', '=', 'self', '.', '_input_kwargs', 'return', 'self', '.', '_set', '(', '*', '*', 'kwargs', ')']
Docstring: setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\                  parallelism=1, collectSubModels=False, seed=None):        Sets params for the train validation split.
*******__*******
Code:def copy(self, extra=None):\n        """\n        Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies creates a deep copy of\n        the embedded paramMap, and copies the embedded and extra parameters over.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance\n        """\n        if extra is None:\n            extra = dict()\n        newTVS = Params.copy(self, extra)\n        if self.isSet(self.estimator):\n            newTVS.setEstimator(self.getEstimator().copy(extra))\n        # estimatorParamMaps remain the same\n        if self.isSet(self.evaluator):\n            newTVS.setEvaluator(self.getEvaluator().copy(extra))\n        return newTVS
Language: python
Code Tokens: ['def', 'copy', '(', 'self', ',', 'extra', '=', 'None', ')', ':', 'if', 'extra', 'is', 'None', ':', 'extra', '=', 'dict', '(', ')', 'newTVS', '=', 'Params', '.', 'copy', '(', 'self', ',', 'extra', ')', 'if', 'self', '.', 'isSet', '(', 'self', '.', 'estimator', ')', ':', 'newTVS', '.', 'setEstimator', '(', 'self', '.', 'getEstimator', '(', ')', '.', 'copy', '(', 'extra', ')', ')', '# estimatorParamMaps remain the same', 'if', 'self', '.', 'isSet', '(', 'self', '.', 'evaluator', ')', ':', 'newTVS', '.', 'setEvaluator', '(', 'self', '.', 'getEvaluator', '(', ')', '.', 'copy', '(', 'extra', ')', ')', 'return', 'newTVS']
Docstring: Creates a copy of this instance with a randomly generated uid        and some extra params. This copies creates a deep copy of        the embedded paramMap, and copies the embedded and extra parameters over.        :param extra: Extra parameters to copy to the new instance        :return: Copy of this instance
*******__*******
Code:def _from_java(cls, java_stage):\n        """\n        Given a Java TrainValidationSplit, create and return a Python wrapper of it.\n        Used for ML persistence.\n        """\n\n        estimator, epms, evaluator = super(TrainValidationSplit, cls)._from_java_impl(java_stage)\n        trainRatio = java_stage.getTrainRatio()\n        seed = java_stage.getSeed()\n        parallelism = java_stage.getParallelism()\n        collectSubModels = java_stage.getCollectSubModels()\n        # Create a new instance of this stage.\n        py_stage = cls(estimator=estimator, estimatorParamMaps=epms, evaluator=evaluator,\n                       trainRatio=trainRatio, seed=seed, parallelism=parallelism,\n                       collectSubModels=collectSubModels)\n        py_stage._resetUid(java_stage.uid())\n        return py_stage
Language: python
Code Tokens: ['def', '_from_java', '(', 'cls', ',', 'java_stage', ')', ':', 'estimator', ',', 'epms', ',', 'evaluator', '=', 'super', '(', 'TrainValidationSplit', ',', 'cls', ')', '.', '_from_java_impl', '(', 'java_stage', ')', 'trainRatio', '=', 'java_stage', '.', 'getTrainRatio', '(', ')', 'seed', '=', 'java_stage', '.', 'getSeed', '(', ')', 'parallelism', '=', 'java_stage', '.', 'getParallelism', '(', ')', 'collectSubModels', '=', 'java_stage', '.', 'getCollectSubModels', '(', ')', '# Create a new instance of this stage.', 'py_stage', '=', 'cls', '(', 'estimator', '=', 'estimator', ',', 'estimatorParamMaps', '=', 'epms', ',', 'evaluator', '=', 'evaluator', ',', 'trainRatio', '=', 'trainRatio', ',', 'seed', '=', 'seed', ',', 'parallelism', '=', 'parallelism', ',', 'collectSubModels', '=', 'collectSubModels', ')', 'py_stage', '.', '_resetUid', '(', 'java_stage', '.', 'uid', '(', ')', ')', 'return', 'py_stage']
Docstring: Given a Java TrainValidationSplit, create and return a Python wrapper of it.        Used for ML persistence.
*******__*******
Code:def _to_java(self):\n        """\n        Transfer this instance to a Java TrainValidationSplit. Used for ML persistence.\n        :return: Java object equivalent to this instance.\n        """\n\n        estimator, epms, evaluator = super(TrainValidationSplit, self)._to_java_impl()\n\n        _java_obj = JavaParams._new_java_obj("org.apache.spark.ml.tuning.TrainValidationSplit",\n                                             self.uid)\n        _java_obj.setEstimatorParamMaps(epms)\n        _java_obj.setEvaluator(evaluator)\n        _java_obj.setEstimator(estimator)\n        _java_obj.setTrainRatio(self.getTrainRatio())\n        _java_obj.setSeed(self.getSeed())\n        _java_obj.setParallelism(self.getParallelism())\n        _java_obj.setCollectSubModels(self.getCollectSubModels())\n        return _java_obj
Language: python
Code Tokens: ['def', '_to_java', '(', 'self', ')', ':', 'estimator', ',', 'epms', ',', 'evaluator', '=', 'super', '(', 'TrainValidationSplit', ',', 'self', ')', '.', '_to_java_impl', '(', ')', '_java_obj', '=', 'JavaParams', '.', '_new_java_obj', '(', '"org.apache.spark.ml.tuning.TrainValidationSplit"', ',', 'self', '.', 'uid', ')', '_java_obj', '.', 'setEstimatorParamMaps', '(', 'epms', ')', '_java_obj', '.', 'setEvaluator', '(', 'evaluator', ')', '_java_obj', '.', 'setEstimator', '(', 'estimator', ')', '_java_obj', '.', 'setTrainRatio', '(', 'self', '.', 'getTrainRatio', '(', ')', ')', '_java_obj', '.', 'setSeed', '(', 'self', '.', 'getSeed', '(', ')', ')', '_java_obj', '.', 'setParallelism', '(', 'self', '.', 'getParallelism', '(', ')', ')', '_java_obj', '.', 'setCollectSubModels', '(', 'self', '.', 'getCollectSubModels', '(', ')', ')', 'return', '_java_obj']
Docstring: Transfer this instance to a Java TrainValidationSplit. Used for ML persistence.        :return: Java object equivalent to this instance.
*******__*******
Code:def copy(self, extra=None):\n        """\n        Creates a copy of this instance with a randomly generated uid\n        and some extra params. This copies the underlying bestModel,\n        creates a deep copy of the embedded paramMap, and\n        copies the embedded and extra parameters over.\n        And, this creates a shallow copy of the validationMetrics.\n        It does not copy the extra Params into the subModels.\n\n        :param extra: Extra parameters to copy to the new instance\n        :return: Copy of this instance\n        """\n        if extra is None:\n            extra = dict()\n        bestModel = self.bestModel.copy(extra)\n        validationMetrics = list(self.validationMetrics)\n        subModels = self.subModels\n        return TrainValidationSplitModel(bestModel, validationMetrics, subModels)
Language: python
Code Tokens: ['def', 'copy', '(', 'self', ',', 'extra', '=', 'None', ')', ':', 'if', 'extra', 'is', 'None', ':', 'extra', '=', 'dict', '(', ')', 'bestModel', '=', 'self', '.', 'bestModel', '.', 'copy', '(', 'extra', ')', 'validationMetrics', '=', 'list', '(', 'self', '.', 'validationMetrics', ')', 'subModels', '=', 'self', '.', 'subModels', 'return', 'TrainValidationSplitModel', '(', 'bestModel', ',', 'validationMetrics', ',', 'subModels', ')']
Docstring: Creates a copy of this instance with a randomly generated uid        and some extra params. This copies the underlying bestModel,        creates a deep copy of the embedded paramMap, and        copies the embedded and extra parameters over.        And, this creates a shallow copy of the validationMetrics.        It does not copy the extra Params into the subModels.        :param extra: Extra parameters to copy to the new instance        :return: Copy of this instance
*******__*******
Code:def _from_java(cls, java_stage):\n        """\n        Given a Java TrainValidationSplitModel, create and return a Python wrapper of it.\n        Used for ML persistence.\n        """\n\n        # Load information from java_stage to the instance.\n        bestModel = JavaParams._from_java(java_stage.bestModel())\n        estimator, epms, evaluator = super(TrainValidationSplitModel,\n                                           cls)._from_java_impl(java_stage)\n        # Create a new instance of this stage.\n        py_stage = cls(bestModel=bestModel).setEstimator(estimator)\n        py_stage = py_stage.setEstimatorParamMaps(epms).setEvaluator(evaluator)\n\n        if java_stage.hasSubModels():\n            py_stage.subModels = [JavaParams._from_java(sub_model)\n                                  for sub_model in java_stage.subModels()]\n\n        py_stage._resetUid(java_stage.uid())\n        return py_stage
Language: python
Code Tokens: ['def', '_from_java', '(', 'cls', ',', 'java_stage', ')', ':', '# Load information from java_stage to the instance.', 'bestModel', '=', 'JavaParams', '.', '_from_java', '(', 'java_stage', '.', 'bestModel', '(', ')', ')', 'estimator', ',', 'epms', ',', 'evaluator', '=', 'super', '(', 'TrainValidationSplitModel', ',', 'cls', ')', '.', '_from_java_impl', '(', 'java_stage', ')', '# Create a new instance of this stage.', 'py_stage', '=', 'cls', '(', 'bestModel', '=', 'bestModel', ')', '.', 'setEstimator', '(', 'estimator', ')', 'py_stage', '=', 'py_stage', '.', 'setEstimatorParamMaps', '(', 'epms', ')', '.', 'setEvaluator', '(', 'evaluator', ')', 'if', 'java_stage', '.', 'hasSubModels', '(', ')', ':', 'py_stage', '.', 'subModels', '=', '[', 'JavaParams', '.', '_from_java', '(', 'sub_model', ')', 'for', 'sub_model', 'in', 'java_stage', '.', 'subModels', '(', ')', ']', 'py_stage', '.', '_resetUid', '(', 'java_stage', '.', 'uid', '(', ')', ')', 'return', 'py_stage']
Docstring: Given a Java TrainValidationSplitModel, create and return a Python wrapper of it.        Used for ML persistence.
*******__*******
Code:def _to_java(self):\n        """\n        Transfer this instance to a Java TrainValidationSplitModel. Used for ML persistence.\n        :return: Java object equivalent to this instance.\n        """\n\n        sc = SparkContext._active_spark_context\n        # TODO: persst validation metrics as well\n        _java_obj = JavaParams._new_java_obj(\n            "org.apache.spark.ml.tuning.TrainValidationSplitModel",\n            self.uid,\n            self.bestModel._to_java(),\n            _py2java(sc, []))\n        estimator, epms, evaluator = super(TrainValidationSplitModel, self)._to_java_impl()\n\n        _java_obj.set("evaluator", evaluator)\n        _java_obj.set("estimator", estimator)\n        _java_obj.set("estimatorParamMaps", epms)\n\n        if self.subModels is not None:\n            java_sub_models = [sub_model._to_java() for sub_model in self.subModels]\n            _java_obj.setSubModels(java_sub_models)\n\n        return _java_obj
Language: python
Code Tokens: ['def', '_to_java', '(', 'self', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', '# TODO: persst validation metrics as well', '_java_obj', '=', 'JavaParams', '.', '_new_java_obj', '(', '"org.apache.spark.ml.tuning.TrainValidationSplitModel"', ',', 'self', '.', 'uid', ',', 'self', '.', 'bestModel', '.', '_to_java', '(', ')', ',', '_py2java', '(', 'sc', ',', '[', ']', ')', ')', 'estimator', ',', 'epms', ',', 'evaluator', '=', 'super', '(', 'TrainValidationSplitModel', ',', 'self', ')', '.', '_to_java_impl', '(', ')', '_java_obj', '.', 'set', '(', '"evaluator"', ',', 'evaluator', ')', '_java_obj', '.', 'set', '(', '"estimator"', ',', 'estimator', ')', '_java_obj', '.', 'set', '(', '"estimatorParamMaps"', ',', 'epms', ')', 'if', 'self', '.', 'subModels', 'is', 'not', 'None', ':', 'java_sub_models', '=', '[', 'sub_model', '.', '_to_java', '(', ')', 'for', 'sub_model', 'in', 'self', '.', 'subModels', ']', '_java_obj', '.', 'setSubModels', '(', 'java_sub_models', ')', 'return', '_java_obj']
Docstring: Transfer this instance to a Java TrainValidationSplitModel. Used for ML persistence.        :return: Java object equivalent to this instance.
*******__*******
Code:def get(self, key, default=_NoValue):\n        """Returns the value of Spark runtime configuration property for the given key,\n        assuming it is set.\n        """\n        self._checkType(key, "key")\n        if default is _NoValue:\n            return self._jconf.get(key)\n        else:\n            if default is not None:\n                self._checkType(default, "default")\n            return self._jconf.get(key, default)
Language: python
Code Tokens: ['def', 'get', '(', 'self', ',', 'key', ',', 'default', '=', '_NoValue', ')', ':', 'self', '.', '_checkType', '(', 'key', ',', '"key"', ')', 'if', 'default', 'is', '_NoValue', ':', 'return', 'self', '.', '_jconf', '.', 'get', '(', 'key', ')', 'else', ':', 'if', 'default', 'is', 'not', 'None', ':', 'self', '.', '_checkType', '(', 'default', ',', '"default"', ')', 'return', 'self', '.', '_jconf', '.', 'get', '(', 'key', ',', 'default', ')']
Docstring: Returns the value of Spark runtime configuration property for the given key,        assuming it is set.
*******__*******
Code:def _checkType(self, obj, identifier):\n        """Assert that an object is of type str."""\n        if not isinstance(obj, basestring):\n            raise TypeError("expected %s '%s' to be a string (was '%s')" %\n                            (identifier, obj, type(obj).__name__))
Language: python
Code Tokens: ['def', '_checkType', '(', 'self', ',', 'obj', ',', 'identifier', ')', ':', 'if', 'not', 'isinstance', '(', 'obj', ',', 'basestring', ')', ':', 'raise', 'TypeError', '(', '"expected %s \'%s\' to be a string (was \'%s\')"', '%', '(', 'identifier', ',', 'obj', ',', 'type', '(', 'obj', ')', '.', '__name__', ')', ')']
Docstring: Assert that an object is of type str.
*******__*******
Code:def _create_function(name, doc=""):\n    """Create a PySpark function by its name"""\n    def _(col):\n        sc = SparkContext._active_spark_context\n        jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = doc\n    return _
Language: python
Code Tokens: ['def', '_create_function', '(', 'name', ',', 'doc', '=', '""', ')', ':', 'def', '_', '(', 'col', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'getattr', '(', 'sc', '.', '_jvm', '.', 'functions', ',', 'name', ')', '(', 'col', '.', '_jc', 'if', 'isinstance', '(', 'col', ',', 'Column', ')', 'else', 'col', ')', 'return', 'Column', '(', 'jc', ')', '_', '.', '__name__', '=', 'name', '_', '.', '__doc__', '=', 'doc', 'return', '_']
Docstring: Create a PySpark function by its name
*******__*******
Code:def _create_function_over_column(name, doc=""):\n    """Similar with `_create_function` but creates a PySpark function that takes a column\n    (as string as well). This is mainly for PySpark functions to take strings as\n    column names.\n    """\n    def _(col):\n        sc = SparkContext._active_spark_context\n        jc = getattr(sc._jvm.functions, name)(_to_java_column(col))\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = doc\n    return _
Language: python
Code Tokens: ['def', '_create_function_over_column', '(', 'name', ',', 'doc', '=', '""', ')', ':', 'def', '_', '(', 'col', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'getattr', '(', 'sc', '.', '_jvm', '.', 'functions', ',', 'name', ')', '(', '_to_java_column', '(', 'col', ')', ')', 'return', 'Column', '(', 'jc', ')', '_', '.', '__name__', '=', 'name', '_', '.', '__doc__', '=', 'doc', 'return', '_']
Docstring: Similar with `_create_function` but creates a PySpark function that takes a column    (as string as well). This is mainly for PySpark functions to take strings as    column names.
*******__*******
Code:def _wrap_deprecated_function(func, message):\n    """ Wrap the deprecated function to print out deprecation warnings"""\n    def _(col):\n        warnings.warn(message, DeprecationWarning)\n        return func(col)\n    return functools.wraps(func)(_)
Language: python
Code Tokens: ['def', '_wrap_deprecated_function', '(', 'func', ',', 'message', ')', ':', 'def', '_', '(', 'col', ')', ':', 'warnings', '.', 'warn', '(', 'message', ',', 'DeprecationWarning', ')', 'return', 'func', '(', 'col', ')', 'return', 'functools', '.', 'wraps', '(', 'func', ')', '(', '_', ')']
Docstring: Wrap the deprecated function to print out deprecation warnings
*******__*******
Code:def _create_binary_mathfunction(name, doc=""):\n    """ Create a binary mathfunction by name"""\n    def _(col1, col2):\n        sc = SparkContext._active_spark_context\n        # For legacy reasons, the arguments here can be implicitly converted into floats,\n        # if they are not columns or strings.\n        if isinstance(col1, Column):\n            arg1 = col1._jc\n        elif isinstance(col1, basestring):\n            arg1 = _create_column_from_name(col1)\n        else:\n            arg1 = float(col1)\n\n        if isinstance(col2, Column):\n            arg2 = col2._jc\n        elif isinstance(col2, basestring):\n            arg2 = _create_column_from_name(col2)\n        else:\n            arg2 = float(col2)\n\n        jc = getattr(sc._jvm.functions, name)(arg1, arg2)\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = doc\n    return _
Language: python
Code Tokens: ['def', '_create_binary_mathfunction', '(', 'name', ',', 'doc', '=', '""', ')', ':', 'def', '_', '(', 'col1', ',', 'col2', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', '# For legacy reasons, the arguments here can be implicitly converted into floats,', '# if they are not columns or strings.', 'if', 'isinstance', '(', 'col1', ',', 'Column', ')', ':', 'arg1', '=', 'col1', '.', '_jc', 'elif', 'isinstance', '(', 'col1', ',', 'basestring', ')', ':', 'arg1', '=', '_create_column_from_name', '(', 'col1', ')', 'else', ':', 'arg1', '=', 'float', '(', 'col1', ')', 'if', 'isinstance', '(', 'col2', ',', 'Column', ')', ':', 'arg2', '=', 'col2', '.', '_jc', 'elif', 'isinstance', '(', 'col2', ',', 'basestring', ')', ':', 'arg2', '=', '_create_column_from_name', '(', 'col2', ')', 'else', ':', 'arg2', '=', 'float', '(', 'col2', ')', 'jc', '=', 'getattr', '(', 'sc', '.', '_jvm', '.', 'functions', ',', 'name', ')', '(', 'arg1', ',', 'arg2', ')', 'return', 'Column', '(', 'jc', ')', '_', '.', '__name__', '=', 'name', '_', '.', '__doc__', '=', 'doc', 'return', '_']
Docstring: Create a binary mathfunction by name
*******__*******
Code:def _create_window_function(name, doc=''):\n    """ Create a window function by name """\n    def _():\n        sc = SparkContext._active_spark_context\n        jc = getattr(sc._jvm.functions, name)()\n        return Column(jc)\n    _.__name__ = name\n    _.__doc__ = 'Window function: ' + doc\n    return _
Language: python
Code Tokens: ['def', '_create_window_function', '(', 'name', ',', 'doc', '=', "''", ')', ':', 'def', '_', '(', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'getattr', '(', 'sc', '.', '_jvm', '.', 'functions', ',', 'name', ')', '(', ')', 'return', 'Column', '(', 'jc', ')', '_', '.', '__name__', '=', 'name', '_', '.', '__doc__', '=', "'Window function: '", '+', 'doc', 'return', '_']
Docstring: Create a window function by name
*******__*******
Code:def approx_count_distinct(col, rsd=None):\n    """Aggregate function: returns a new :class:`Column` for approximate distinct count of\n    column `col`.\n\n    :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more\n        efficient to use :func:`countDistinct`\n\n    >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n    [Row(distinct_ages=2)]\n    """\n    sc = SparkContext._active_spark_context\n    if rsd is None:\n        jc = sc._jvm.functions.approx_count_distinct(_to_java_column(col))\n    else:\n        jc = sc._jvm.functions.approx_count_distinct(_to_java_column(col), rsd)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'approx_count_distinct', '(', 'col', ',', 'rsd', '=', 'None', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'rsd', 'is', 'None', ':', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'approx_count_distinct', '(', '_to_java_column', '(', 'col', ')', ')', 'else', ':', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'approx_count_distinct', '(', '_to_java_column', '(', 'col', ')', ',', 'rsd', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Aggregate function: returns a new :class:`Column` for approximate distinct count of    column `col`.    :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more        efficient to use :func:`countDistinct`    >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()    [Row(distinct_ages=2)]
*******__*******
Code:def broadcast(df):\n    """Marks a DataFrame as small enough for use in broadcast joins."""\n\n    sc = SparkContext._active_spark_context\n    return DataFrame(sc._jvm.functions.broadcast(df._jdf), df.sql_ctx)
Language: python
Code Tokens: ['def', 'broadcast', '(', 'df', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'DataFrame', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'broadcast', '(', 'df', '.', '_jdf', ')', ',', 'df', '.', 'sql_ctx', ')']
Docstring: Marks a DataFrame as small enough for use in broadcast joins.
*******__*******
Code:def countDistinct(col, *cols):\n    """Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n\n    >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n    [Row(c=2)]\n\n    >>> df.agg(countDistinct("age", "name").alias('c')).collect()\n    [Row(c=2)]\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.countDistinct(_to_java_column(col), _to_seq(sc, cols, _to_java_column))\n    return Column(jc)
Language: python
Code Tokens: ['def', 'countDistinct', '(', 'col', ',', '*', 'cols', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'countDistinct', '(', '_to_java_column', '(', 'col', ')', ',', '_to_seq', '(', 'sc', ',', 'cols', ',', '_to_java_column', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.    >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()    [Row(c=2)]    >>> df.agg(countDistinct("age", "name").alias('c')).collect()    [Row(c=2)]
*******__*******
Code:def last(col, ignorenulls=False):\n    """Aggregate function: returns the last value in a group.\n\n    The function by default returns the last values it sees. It will return the last non-null\n    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n\n    .. note:: The function is non-deterministic because its results depends on order of rows\n        which may be non-deterministic after a shuffle.\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.last(_to_java_column(col), ignorenulls)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'last', '(', 'col', ',', 'ignorenulls', '=', 'False', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'last', '(', '_to_java_column', '(', 'col', ')', ',', 'ignorenulls', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Aggregate function: returns the last value in a group.    The function by default returns the last values it sees. It will return the last non-null    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.    .. note:: The function is non-deterministic because its results depends on order of rows        which may be non-deterministic after a shuffle.
*******__*******
Code:def nanvl(col1, col2):\n    """Returns col1 if it is not NaN, or col2 if col1 is NaN.\n\n    Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n\n    >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], ("a", "b"))\n    >>> df.select(nanvl("a", "b").alias("r1"), nanvl(df.a, df.b).alias("r2")).collect()\n    [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.nanvl(_to_java_column(col1), _to_java_column(col2)))
Language: python
Code Tokens: ['def', 'nanvl', '(', 'col1', ',', 'col2', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'nanvl', '(', '_to_java_column', '(', 'col1', ')', ',', '_to_java_column', '(', 'col2', ')', ')', ')']
Docstring: Returns col1 if it is not NaN, or col2 if col1 is NaN.    Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).    >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], ("a", "b"))    >>> df.select(nanvl("a", "b").alias("r1"), nanvl(df.a, df.b).alias("r2")).collect()    [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]
*******__*******
Code:def rand(seed=None):\n    """Generates a random column with independent and identically distributed (i.i.d.) samples\n    from U[0.0, 1.0].\n\n    .. note:: The function is non-deterministic in general case.\n\n    >>> df.withColumn('rand', rand(seed=42) * 3).collect()\n    [Row(age=2, name=u'Alice', rand=2.4052597283576684),\n     Row(age=5, name=u'Bob', rand=2.3913904055683974)]\n    """\n    sc = SparkContext._active_spark_context\n    if seed is not None:\n        jc = sc._jvm.functions.rand(seed)\n    else:\n        jc = sc._jvm.functions.rand()\n    return Column(jc)
Language: python
Code Tokens: ['def', 'rand', '(', 'seed', '=', 'None', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'seed', 'is', 'not', 'None', ':', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'rand', '(', 'seed', ')', 'else', ':', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'rand', '(', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Generates a random column with independent and identically distributed (i.i.d.) samples    from U[0.0, 1.0].    .. note:: The function is non-deterministic in general case.    >>> df.withColumn('rand', rand(seed=42) * 3).collect()    [Row(age=2, name=u'Alice', rand=2.4052597283576684),     Row(age=5, name=u'Bob', rand=2.3913904055683974)]
*******__*******
Code:def round(col, scale=0):\n    """\n    Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n    or at integral part when `scale` < 0.\n\n    >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n    [Row(r=3.0)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.round(_to_java_column(col), scale))
Language: python
Code Tokens: ['def', 'round', '(', 'col', ',', 'scale', '=', '0', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'round', '(', '_to_java_column', '(', 'col', ')', ',', 'scale', ')', ')']
Docstring: Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0    or at integral part when `scale` < 0.    >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()    [Row(r=3.0)]
*******__*******
Code:def shiftLeft(col, numBits):\n    """Shift the given value numBits left.\n\n    >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n    [Row(r=42)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.shiftLeft(_to_java_column(col), numBits))
Language: python
Code Tokens: ['def', 'shiftLeft', '(', 'col', ',', 'numBits', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'shiftLeft', '(', '_to_java_column', '(', 'col', ')', ',', 'numBits', ')', ')']
Docstring: Shift the given value numBits left.    >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()    [Row(r=42)]
*******__*******
Code:def shiftRight(col, numBits):\n    """(Signed) shift the given value numBits right.\n\n    >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n    [Row(r=21)]\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.shiftRight(_to_java_column(col), numBits)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'shiftRight', '(', 'col', ',', 'numBits', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'shiftRight', '(', '_to_java_column', '(', 'col', ')', ',', 'numBits', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: (Signed) shift the given value numBits right.    >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()    [Row(r=21)]
*******__*******
Code:def expr(str):\n    """Parses the expression string into the column that it represents\n\n    >>> df.select(expr("length(name)")).collect()\n    [Row(length(name)=5), Row(length(name)=3)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.expr(str))
Language: python
Code Tokens: ['def', 'expr', '(', 'str', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'expr', '(', 'str', ')', ')']
Docstring: Parses the expression string into the column that it represents    >>> df.select(expr("length(name)")).collect()    [Row(length(name)=5), Row(length(name)=3)]
*******__*******
Code:def when(condition, value):\n    """Evaluates a list of conditions and returns one of multiple possible result expressions.\n    If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n\n    :param condition: a boolean :class:`Column` expression.\n    :param value: a literal value, or a :class:`Column` expression.\n\n    >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias("age")).collect()\n    [Row(age=3), Row(age=4)]\n\n    >>> df.select(when(df.age == 2, df.age + 1).alias("age")).collect()\n    [Row(age=3), Row(age=None)]\n    """\n    sc = SparkContext._active_spark_context\n    if not isinstance(condition, Column):\n        raise TypeError("condition should be a Column")\n    v = value._jc if isinstance(value, Column) else value\n    jc = sc._jvm.functions.when(condition._jc, v)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'when', '(', 'condition', ',', 'value', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'not', 'isinstance', '(', 'condition', ',', 'Column', ')', ':', 'raise', 'TypeError', '(', '"condition should be a Column"', ')', 'v', '=', 'value', '.', '_jc', 'if', 'isinstance', '(', 'value', ',', 'Column', ')', 'else', 'value', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'when', '(', 'condition', '.', '_jc', ',', 'v', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Evaluates a list of conditions and returns one of multiple possible result expressions.    If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.    :param condition: a boolean :class:`Column` expression.    :param value: a literal value, or a :class:`Column` expression.    >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias("age")).collect()    [Row(age=3), Row(age=4)]    >>> df.select(when(df.age == 2, df.age + 1).alias("age")).collect()    [Row(age=3), Row(age=None)]
*******__*******
Code:def log(arg1, arg2=None):\n    """Returns the first argument-based logarithm of the second argument.\n\n    If there is only one argument, then this takes the natural logarithm of the argument.\n\n    >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n    ['0.30102', '0.69897']\n\n    >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n    ['0.69314', '1.60943']\n    """\n    sc = SparkContext._active_spark_context\n    if arg2 is None:\n        jc = sc._jvm.functions.log(_to_java_column(arg1))\n    else:\n        jc = sc._jvm.functions.log(arg1, _to_java_column(arg2))\n    return Column(jc)
Language: python
Code Tokens: ['def', 'log', '(', 'arg1', ',', 'arg2', '=', 'None', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'arg2', 'is', 'None', ':', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'log', '(', '_to_java_column', '(', 'arg1', ')', ')', 'else', ':', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'log', '(', 'arg1', ',', '_to_java_column', '(', 'arg2', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Returns the first argument-based logarithm of the second argument.    If there is only one argument, then this takes the natural logarithm of the argument.    >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()    ['0.30102', '0.69897']    >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()    ['0.69314', '1.60943']
*******__*******
Code:def conv(col, fromBase, toBase):\n    """\n    Convert a number in a string column from one base to another.\n\n    >>> df = spark.createDataFrame([("010101",)], ['n'])\n    >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n    [Row(hex=u'15')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.conv(_to_java_column(col), fromBase, toBase))
Language: python
Code Tokens: ['def', 'conv', '(', 'col', ',', 'fromBase', ',', 'toBase', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'conv', '(', '_to_java_column', '(', 'col', ')', ',', 'fromBase', ',', 'toBase', ')', ')']
Docstring: Convert a number in a string column from one base to another.    >>> df = spark.createDataFrame([("010101",)], ['n'])    >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()    [Row(hex=u'15')]
*******__*******
Code:def lag(col, offset=1, default=None):\n    """\n    Window function: returns the value that is `offset` rows before the current row, and\n    `defaultValue` if there is less than `offset` rows before the current row. For example,\n    an `offset` of one will return the previous row at any given point in the window partition.\n\n    This is equivalent to the LAG function in SQL.\n\n    :param col: name of column or expression\n    :param offset: number of row to extend\n    :param default: default value\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.lag(_to_java_column(col), offset, default))
Language: python
Code Tokens: ['def', 'lag', '(', 'col', ',', 'offset', '=', '1', ',', 'default', '=', 'None', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'lag', '(', '_to_java_column', '(', 'col', ')', ',', 'offset', ',', 'default', ')', ')']
Docstring: Window function: returns the value that is `offset` rows before the current row, and    `defaultValue` if there is less than `offset` rows before the current row. For example,    an `offset` of one will return the previous row at any given point in the window partition.    This is equivalent to the LAG function in SQL.    :param col: name of column or expression    :param offset: number of row to extend    :param default: default value
*******__*******
Code:def ntile(n):\n    """\n    Window function: returns the ntile group id (from 1 to `n` inclusive)\n    in an ordered window partition. For example, if `n` is 4, the first\n    quarter of the rows will get value 1, the second quarter will get 2,\n    the third quarter will get 3, and the last quarter will get 4.\n\n    This is equivalent to the NTILE function in SQL.\n\n    :param n: an integer\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.ntile(int(n)))
Language: python
Code Tokens: ['def', 'ntile', '(', 'n', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'ntile', '(', 'int', '(', 'n', ')', ')', ')']
Docstring: Window function: returns the ntile group id (from 1 to `n` inclusive)    in an ordered window partition. For example, if `n` is 4, the first    quarter of the rows will get value 1, the second quarter will get 2,    the third quarter will get 3, and the last quarter will get 4.    This is equivalent to the NTILE function in SQL.    :param n: an integer
*******__*******
Code:def date_format(date, format):\n    """\n    Converts a date/timestamp/string to a value of string in the format specified by the date\n    format given by the second argument.\n\n    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n    pattern letters of the Java class `java.time.format.DateTimeFormatter` can be used.\n\n    .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n        specialized implementation.\n\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n    [Row(date=u'04/08/2015')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.date_format(_to_java_column(date), format))
Language: python
Code Tokens: ['def', 'date_format', '(', 'date', ',', 'format', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'date_format', '(', '_to_java_column', '(', 'date', ')', ',', 'format', ')', ')']
Docstring: Converts a date/timestamp/string to a value of string in the format specified by the date    format given by the second argument.    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All    pattern letters of the Java class `java.time.format.DateTimeFormatter` can be used.    .. note:: Use when ever possible specialized functions like `year`. These benefit from a        specialized implementation.    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()    [Row(date=u'04/08/2015')]
*******__*******
Code:def date_add(start, days):\n    """\n    Returns the date that is `days` days after `start`\n\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n    [Row(next_date=datetime.date(2015, 4, 9))]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.date_add(_to_java_column(start), days))
Language: python
Code Tokens: ['def', 'date_add', '(', 'start', ',', 'days', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'date_add', '(', '_to_java_column', '(', 'start', ')', ',', 'days', ')', ')']
Docstring: Returns the date that is `days` days after `start`    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()    [Row(next_date=datetime.date(2015, 4, 9))]
*******__*******
Code:def datediff(end, start):\n    """\n    Returns the number of days from `start` to `end`.\n\n    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n    >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n    [Row(diff=32)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.datediff(_to_java_column(end), _to_java_column(start)))
Language: python
Code Tokens: ['def', 'datediff', '(', 'end', ',', 'start', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'datediff', '(', '_to_java_column', '(', 'end', ')', ',', '_to_java_column', '(', 'start', ')', ')', ')']
Docstring: Returns the number of days from `start` to `end`.    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])    >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()    [Row(diff=32)]
*******__*******
Code:def add_months(start, months):\n    """\n    Returns the date that is `months` months after `start`\n\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n    [Row(next_month=datetime.date(2015, 5, 8))]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.add_months(_to_java_column(start), months))
Language: python
Code Tokens: ['def', 'add_months', '(', 'start', ',', 'months', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'add_months', '(', '_to_java_column', '(', 'start', ')', ',', 'months', ')', ')']
Docstring: Returns the date that is `months` months after `start`    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])    >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()    [Row(next_month=datetime.date(2015, 5, 8))]
*******__*******
Code:def months_between(date1, date2, roundOff=True):\n    """\n    Returns number of months between dates date1 and date2.\n    If date1 is later than date2, then the result is positive.\n    If date1 and date2 are on the same day of month, or both are the last day of month,\n    returns an integer (time of day will be ignored).\n    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n    [Row(months=3.94959677)]\n    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n    [Row(months=3.9495967741935485)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.months_between(\n        _to_java_column(date1), _to_java_column(date2), roundOff))
Language: python
Code Tokens: ['def', 'months_between', '(', 'date1', ',', 'date2', ',', 'roundOff', '=', 'True', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'months_between', '(', '_to_java_column', '(', 'date1', ')', ',', '_to_java_column', '(', 'date2', ')', ',', 'roundOff', ')', ')']
Docstring: Returns number of months between dates date1 and date2.    If date1 is later than date2, then the result is positive.    If date1 and date2 are on the same day of month, or both are the last day of month,    returns an integer (time of day will be ignored).    The result is rounded off to 8 digits unless `roundOff` is set to `False`.    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()    [Row(months=3.94959677)]    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()    [Row(months=3.9495967741935485)]
*******__*******
Code:def to_date(col, format=None):\n    """Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n    :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n    using the optionally specified format. Specify formats according to\n    `DateTimeFormatter <https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html>`_. # noqa\n    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n    is omitted (equivalent to ``col.cast("date")``).\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_date(df.t).alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n    """\n    sc = SparkContext._active_spark_context\n    if format is None:\n        jc = sc._jvm.functions.to_date(_to_java_column(col))\n    else:\n        jc = sc._jvm.functions.to_date(_to_java_column(col), format)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'to_date', '(', 'col', ',', 'format', '=', 'None', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'format', 'is', 'None', ':', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'to_date', '(', '_to_java_column', '(', 'col', ')', ')', 'else', ':', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'to_date', '(', '_to_java_column', '(', 'col', ')', ',', 'format', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or    :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`    using the optionally specified format. Specify formats according to    `DateTimeFormatter <https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html>`_. # noqa    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format    is omitted (equivalent to ``col.cast("date")``).    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])    >>> df.select(to_date(df.t).alias('date')).collect()    [Row(date=datetime.date(1997, 2, 28))]    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()    [Row(date=datetime.date(1997, 2, 28))]
*******__*******
Code:def date_trunc(format, timestamp):\n    """\n    Returns timestamp truncated to the unit specified by the format.\n\n    :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n        'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n\n    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.date_trunc(format, _to_java_column(timestamp)))
Language: python
Code Tokens: ['def', 'date_trunc', '(', 'format', ',', 'timestamp', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'date_trunc', '(', 'format', ',', '_to_java_column', '(', 'timestamp', ')', ')', ')']
Docstring: Returns timestamp truncated to the unit specified by the format.    :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',        'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])    >>> df.select(date_trunc('year', df.t).alias('year')).collect()    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]
*******__*******
Code:def next_day(date, dayOfWeek):\n    """\n    Returns the first date which is later than the value of the date column.\n\n    Day of the week parameter is case insensitive, and accepts:\n        "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun".\n\n    >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n    >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n    [Row(date=datetime.date(2015, 8, 2))]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.next_day(_to_java_column(date), dayOfWeek))
Language: python
Code Tokens: ['def', 'next_day', '(', 'date', ',', 'dayOfWeek', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'next_day', '(', '_to_java_column', '(', 'date', ')', ',', 'dayOfWeek', ')', ')']
Docstring: Returns the first date which is later than the value of the date column.    Day of the week parameter is case insensitive, and accepts:        "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun".    >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])    >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()    [Row(date=datetime.date(2015, 8, 2))]
*******__*******
Code:def last_day(date):\n    """\n    Returns the last day of the month which the given date belongs to.\n\n    >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n    >>> df.select(last_day(df.d).alias('date')).collect()\n    [Row(date=datetime.date(1997, 2, 28))]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.last_day(_to_java_column(date)))
Language: python
Code Tokens: ['def', 'last_day', '(', 'date', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'last_day', '(', '_to_java_column', '(', 'date', ')', ')', ')']
Docstring: Returns the last day of the month which the given date belongs to.    >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])    >>> df.select(last_day(df.d).alias('date')).collect()    [Row(date=datetime.date(1997, 2, 28))]
*******__*******
Code:def unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss'):\n    """\n    Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n    to Unix time stamp (in seconds), using the default timezone and the default\n    locale, return null if fail.\n\n    if `timestamp` is None, then it returns current timestamp.\n\n    >>> spark.conf.set("spark.sql.session.timeZone", "America/Los_Angeles")\n    >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n    >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n    [Row(unix_time=1428476400)]\n    >>> spark.conf.unset("spark.sql.session.timeZone")\n    """\n    sc = SparkContext._active_spark_context\n    if timestamp is None:\n        return Column(sc._jvm.functions.unix_timestamp())\n    return Column(sc._jvm.functions.unix_timestamp(_to_java_column(timestamp), format))
Language: python
Code Tokens: ['def', 'unix_timestamp', '(', 'timestamp', '=', 'None', ',', 'format', '=', "'yyyy-MM-dd HH:mm:ss'", ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'timestamp', 'is', 'None', ':', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'unix_timestamp', '(', ')', ')', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'unix_timestamp', '(', '_to_java_column', '(', 'timestamp', ')', ',', 'format', ')', ')']
Docstring: Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)    to Unix time stamp (in seconds), using the default timezone and the default    locale, return null if fail.    if `timestamp` is None, then it returns current timestamp.    >>> spark.conf.set("spark.sql.session.timeZone", "America/Los_Angeles")    >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])    >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()    [Row(unix_time=1428476400)]    >>> spark.conf.unset("spark.sql.session.timeZone")
*******__*******
Code:def from_utc_timestamp(timestamp, tz):\n    """\n    This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n    renders that timestamp as a timestamp in the given time zone.\n\n    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n    timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n    the given timezone.\n\n    This function may return confusing result if the input is a string with timezone, e.g.\n    '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n    according to the timezone in the string, and finally display the result by converting the\n    timestamp to string according to the session local timezone.\n\n    :param timestamp: the column that contains timestamps\n    :param tz: a string that has the ID of timezone, e.g. "GMT", "America/Los_Angeles", etc\n\n    .. versionchanged:: 2.4\n       `tz` can take a :class:`Column` containing timezone ID strings.\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n    >>> df.select(from_utc_timestamp(df.ts, "PST").alias('local_time')).collect()\n    [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n    >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n    [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n\n    .. note:: Deprecated in 3.0. See SPARK-25496\n    """\n    warnings.warn("Deprecated in 3.0. See SPARK-25496", DeprecationWarning)\n    sc = SparkContext._active_spark_context\n    if isinstance(tz, Column):\n        tz = _to_java_column(tz)\n    return Column(sc._jvm.functions.from_utc_timestamp(_to_java_column(timestamp), tz))
Language: python
Code Tokens: ['def', 'from_utc_timestamp', '(', 'timestamp', ',', 'tz', ')', ':', 'warnings', '.', 'warn', '(', '"Deprecated in 3.0. See SPARK-25496"', ',', 'DeprecationWarning', ')', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'isinstance', '(', 'tz', ',', 'Column', ')', ':', 'tz', '=', '_to_java_column', '(', 'tz', ')', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'from_utc_timestamp', '(', '_to_java_column', '(', 'timestamp', ')', ',', 'tz', ')', ')']
Docstring: This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and    renders that timestamp as a timestamp in the given time zone.    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not    timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to    the given timezone.    This function may return confusing result if the input is a string with timezone, e.g.    '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp    according to the timezone in the string, and finally display the result by converting the    timestamp to string according to the session local timezone.    :param timestamp: the column that contains timestamps    :param tz: a string that has the ID of timezone, e.g. "GMT", "America/Los_Angeles", etc    .. versionchanged:: 2.4       `tz` can take a :class:`Column` containing timezone ID strings.    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])    >>> df.select(from_utc_timestamp(df.ts, "PST").alias('local_time')).collect()    [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]    >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()    [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]    .. note:: Deprecated in 3.0. See SPARK-25496
*******__*******
Code:def window(timeColumn, windowDuration, slideDuration=None, startTime=None):\n    """Bucketize rows into one or more time windows given a timestamp specifying column. Window\n    starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n    [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n    the order of months are not supported.\n\n    The time column must be of :class:`pyspark.sql.types.TimestampType`.\n\n    Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n    interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n    If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n\n    The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n    window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n    past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n\n    The output column will be a struct called 'window' by default with the nested columns 'start'\n    and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n\n    >>> df = spark.createDataFrame([("2016-03-11 09:00:07", 1)]).toDF("date", "val")\n    >>> w = df.groupBy(window("date", "5 seconds")).agg(sum("val").alias("sum"))\n    >>> w.select(w.window.start.cast("string").alias("start"),\n    ...          w.window.end.cast("string").alias("end"), "sum").collect()\n    [Row(start=u'2016-03-11 09:00:05', end=u'2016-03-11 09:00:10', sum=1)]\n    """\n    def check_string_field(field, fieldName):\n        if not field or type(field) is not str:\n            raise TypeError("%s should be provided as a string" % fieldName)\n\n    sc = SparkContext._active_spark_context\n    time_col = _to_java_column(timeColumn)\n    check_string_field(windowDuration, "windowDuration")\n    if slideDuration and startTime:\n        check_string_field(slideDuration, "slideDuration")\n        check_string_field(startTime, "startTime")\n        res = sc._jvm.functions.window(time_col, windowDuration, slideDuration, startTime)\n    elif slideDuration:\n        check_string_field(slideDuration, "slideDuration")\n        res = sc._jvm.functions.window(time_col, windowDuration, slideDuration)\n    elif startTime:\n        check_string_field(startTime, "startTime")\n        res = sc._jvm.functions.window(time_col, windowDuration, windowDuration, startTime)\n    else:\n        res = sc._jvm.functions.window(time_col, windowDuration)\n    return Column(res)
Language: python
Code Tokens: ['def', 'window', '(', 'timeColumn', ',', 'windowDuration', ',', 'slideDuration', '=', 'None', ',', 'startTime', '=', 'None', ')', ':', 'def', 'check_string_field', '(', 'field', ',', 'fieldName', ')', ':', 'if', 'not', 'field', 'or', 'type', '(', 'field', ')', 'is', 'not', 'str', ':', 'raise', 'TypeError', '(', '"%s should be provided as a string"', '%', 'fieldName', ')', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'time_col', '=', '_to_java_column', '(', 'timeColumn', ')', 'check_string_field', '(', 'windowDuration', ',', '"windowDuration"', ')', 'if', 'slideDuration', 'and', 'startTime', ':', 'check_string_field', '(', 'slideDuration', ',', '"slideDuration"', ')', 'check_string_field', '(', 'startTime', ',', '"startTime"', ')', 'res', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'window', '(', 'time_col', ',', 'windowDuration', ',', 'slideDuration', ',', 'startTime', ')', 'elif', 'slideDuration', ':', 'check_string_field', '(', 'slideDuration', ',', '"slideDuration"', ')', 'res', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'window', '(', 'time_col', ',', 'windowDuration', ',', 'slideDuration', ')', 'elif', 'startTime', ':', 'check_string_field', '(', 'startTime', ',', '"startTime"', ')', 'res', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'window', '(', 'time_col', ',', 'windowDuration', ',', 'windowDuration', ',', 'startTime', ')', 'else', ':', 'res', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'window', '(', 'time_col', ',', 'windowDuration', ')', 'return', 'Column', '(', 'res', ')']
Docstring: Bucketize rows into one or more time windows given a timestamp specifying column. Window    starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window    [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in    the order of months are not supported.    The time column must be of :class:`pyspark.sql.types.TimestampType`.    Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid    interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.    If the ``slideDuration`` is not provided, the windows will be tumbling windows.    The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start    window intervals. For example, in order to have hourly tumbling windows that start 15 minutes    past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.    The output column will be a struct called 'window' by default with the nested columns 'start'    and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.    >>> df = spark.createDataFrame([("2016-03-11 09:00:07", 1)]).toDF("date", "val")    >>> w = df.groupBy(window("date", "5 seconds")).agg(sum("val").alias("sum"))    >>> w.select(w.window.start.cast("string").alias("start"),    ...          w.window.end.cast("string").alias("end"), "sum").collect()    [Row(start=u'2016-03-11 09:00:05', end=u'2016-03-11 09:00:10', sum=1)]
*******__*******
Code:def hash(*cols):\n    """Calculates the hash code of given columns, and returns the result as an int column.\n\n    >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n    [Row(hash=-757602832)]\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.hash(_to_seq(sc, cols, _to_java_column))\n    return Column(jc)
Language: python
Code Tokens: ['def', 'hash', '(', '*', 'cols', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'hash', '(', '_to_seq', '(', 'sc', ',', 'cols', ',', '_to_java_column', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Calculates the hash code of given columns, and returns the result as an int column.    >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()    [Row(hash=-757602832)]
*******__*******
Code:def concat_ws(sep, *cols):\n    """\n    Concatenates multiple input string columns together into a single string column,\n    using the given separator.\n\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n    [Row(s=u'abcd-123')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.concat_ws(sep, _to_seq(sc, cols, _to_java_column)))
Language: python
Code Tokens: ['def', 'concat_ws', '(', 'sep', ',', '*', 'cols', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'concat_ws', '(', 'sep', ',', '_to_seq', '(', 'sc', ',', 'cols', ',', '_to_java_column', ')', ')', ')']
Docstring: Concatenates multiple input string columns together into a single string column,    using the given separator.    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()    [Row(s=u'abcd-123')]
*******__*******
Code:def decode(col, charset):\n    """\n    Computes the first argument into a string from a binary using the provided character set\n    (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.decode(_to_java_column(col), charset))
Language: python
Code Tokens: ['def', 'decode', '(', 'col', ',', 'charset', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'decode', '(', '_to_java_column', '(', 'col', ')', ',', 'charset', ')', ')']
Docstring: Computes the first argument into a string from a binary using the provided character set    (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').
*******__*******
Code:def format_number(col, d):\n    """\n    Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n    with HALF_EVEN round mode, and returns the result as a string.\n\n    :param col: the column name of the numeric value to be formatted\n    :param d: the N decimal places\n\n    >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n    [Row(v=u'5.0000')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.format_number(_to_java_column(col), d))
Language: python
Code Tokens: ['def', 'format_number', '(', 'col', ',', 'd', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'format_number', '(', '_to_java_column', '(', 'col', ')', ',', 'd', ')', ')']
Docstring: Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places    with HALF_EVEN round mode, and returns the result as a string.    :param col: the column name of the numeric value to be formatted    :param d: the N decimal places    >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()    [Row(v=u'5.0000')]
*******__*******
Code:def format_string(format, *cols):\n    """\n    Formats the arguments in printf-style and returns the result as a string column.\n\n    :param col: the column name of the numeric value to be formatted\n    :param d: the N decimal places\n\n    >>> df = spark.createDataFrame([(5, "hello")], ['a', 'b'])\n    >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n    [Row(v=u'5 hello')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.format_string(format, _to_seq(sc, cols, _to_java_column)))
Language: python
Code Tokens: ['def', 'format_string', '(', 'format', ',', '*', 'cols', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'format_string', '(', 'format', ',', '_to_seq', '(', 'sc', ',', 'cols', ',', '_to_java_column', ')', ')', ')']
Docstring: Formats the arguments in printf-style and returns the result as a string column.    :param col: the column name of the numeric value to be formatted    :param d: the N decimal places    >>> df = spark.createDataFrame([(5, "hello")], ['a', 'b'])    >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()    [Row(v=u'5 hello')]
*******__*******
Code:def instr(str, substr):\n    """\n    Locate the position of the first occurrence of substr column in the given string.\n    Returns null if either of the arguments are null.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(instr(df.s, 'b').alias('s')).collect()\n    [Row(s=2)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.instr(_to_java_column(str), substr))
Language: python
Code Tokens: ['def', 'instr', '(', 'str', ',', 'substr', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'instr', '(', '_to_java_column', '(', 'str', ')', ',', 'substr', ')', ')']
Docstring: Locate the position of the first occurrence of substr column in the given string.    Returns null if either of the arguments are null.    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr        could not be found in str.    >>> df = spark.createDataFrame([('abcd',)], ['s',])    >>> df.select(instr(df.s, 'b').alias('s')).collect()    [Row(s=2)]
*******__*******
Code:def substring(str, pos, len):\n    """\n    Substring starts at `pos` and is of length `len` when str is String type or\n    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n    when str is Binary type.\n\n    .. note:: The position is not zero based, but 1 based index.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n    [Row(s=u'ab')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.substring(_to_java_column(str), pos, len))
Language: python
Code Tokens: ['def', 'substring', '(', 'str', ',', 'pos', ',', 'len', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'substring', '(', '_to_java_column', '(', 'str', ')', ',', 'pos', ',', 'len', ')', ')']
Docstring: Substring starts at `pos` and is of length `len` when str is String type or    returns the slice of byte array that starts at `pos` in byte and is of length `len`    when str is Binary type.    .. note:: The position is not zero based, but 1 based index.    >>> df = spark.createDataFrame([('abcd',)], ['s',])    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()    [Row(s=u'ab')]
*******__*******
Code:def substring_index(str, delim, count):\n    """\n    Returns the substring from string str before count occurrences of the delimiter delim.\n    If count is positive, everything the left of the final delimiter (counting from left) is\n    returned. If count is negative, every to the right of the final delimiter (counting from the\n    right) is returned. substring_index performs a case-sensitive match when searching for delim.\n\n    >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n    >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n    [Row(s=u'a.b')]\n    >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n    [Row(s=u'b.c.d')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.substring_index(_to_java_column(str), delim, count))
Language: python
Code Tokens: ['def', 'substring_index', '(', 'str', ',', 'delim', ',', 'count', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'substring_index', '(', '_to_java_column', '(', 'str', ')', ',', 'delim', ',', 'count', ')', ')']
Docstring: Returns the substring from string str before count occurrences of the delimiter delim.    If count is positive, everything the left of the final delimiter (counting from left) is    returned. If count is negative, every to the right of the final delimiter (counting from the    right) is returned. substring_index performs a case-sensitive match when searching for delim.    >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])    >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()    [Row(s=u'a.b')]    >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()    [Row(s=u'b.c.d')]
*******__*******
Code:def levenshtein(left, right):\n    """Computes the Levenshtein distance of the two given strings.\n\n    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n    [Row(d=3)]\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))\n    return Column(jc)
Language: python
Code Tokens: ['def', 'levenshtein', '(', 'left', ',', 'right', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'levenshtein', '(', '_to_java_column', '(', 'left', ')', ',', '_to_java_column', '(', 'right', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Computes the Levenshtein distance of the two given strings.    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()    [Row(d=3)]
*******__*******
Code:def locate(substr, str, pos=1):\n    """\n    Locate the position of the first occurrence of substr in a string column, after position pos.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    :param substr: a string\n    :param str: a Column of :class:`pyspark.sql.types.StringType`\n    :param pos: start position (zero based)\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n    [Row(s=2)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.locate(substr, _to_java_column(str), pos))
Language: python
Code Tokens: ['def', 'locate', '(', 'substr', ',', 'str', ',', 'pos', '=', '1', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'locate', '(', 'substr', ',', '_to_java_column', '(', 'str', ')', ',', 'pos', ')', ')']
Docstring: Locate the position of the first occurrence of substr in a string column, after position pos.    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr        could not be found in str.    :param substr: a string    :param str: a Column of :class:`pyspark.sql.types.StringType`    :param pos: start position (zero based)    >>> df = spark.createDataFrame([('abcd',)], ['s',])    >>> df.select(locate('b', df.s, 1).alias('s')).collect()    [Row(s=2)]
*******__*******
Code:def lpad(col, len, pad):\n    """\n    Left-pad the string column to width `len` with `pad`.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n    [Row(s=u'##abcd')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.lpad(_to_java_column(col), len, pad))
Language: python
Code Tokens: ['def', 'lpad', '(', 'col', ',', 'len', ',', 'pad', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'lpad', '(', '_to_java_column', '(', 'col', ')', ',', 'len', ',', 'pad', ')', ')']
Docstring: Left-pad the string column to width `len` with `pad`.    >>> df = spark.createDataFrame([('abcd',)], ['s',])    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()    [Row(s=u'##abcd')]
*******__*******
Code:def repeat(col, n):\n    """\n    Repeats a string column n times, and returns it as a new string column.\n\n    >>> df = spark.createDataFrame([('ab',)], ['s',])\n    >>> df.select(repeat(df.s, 3).alias('s')).collect()\n    [Row(s=u'ababab')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.repeat(_to_java_column(col), n))
Language: python
Code Tokens: ['def', 'repeat', '(', 'col', ',', 'n', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'repeat', '(', '_to_java_column', '(', 'col', ')', ',', 'n', ')', ')']
Docstring: Repeats a string column n times, and returns it as a new string column.    >>> df = spark.createDataFrame([('ab',)], ['s',])    >>> df.select(repeat(df.s, 3).alias('s')).collect()    [Row(s=u'ababab')]
*******__*******
Code:def split(str, pattern, limit=-1):\n    """\n    Splits str around matches of the given pattern.\n\n    :param str: a string expression to split\n    :param pattern: a string representing a regular expression. The regex string should be\n        a Java regular expression.\n    :param limit: an integer which controls the number of times `pattern` is applied.\n\n        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n                         resulting array's last entry will contain all input beyond the last\n                         matched pattern.\n        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n                          array can be of any size.\n\n    .. versionchanged:: 3.0\n       `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n\n    >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n    >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n    [Row(s=[u'one', u'twoBthreeC'])]\n    >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n    [Row(s=[u'one', u'two', u'three', u''])]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.split(_to_java_column(str), pattern, limit))
Language: python
Code Tokens: ['def', 'split', '(', 'str', ',', 'pattern', ',', 'limit', '=', '-', '1', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'split', '(', '_to_java_column', '(', 'str', ')', ',', 'pattern', ',', 'limit', ')', ')']
Docstring: Splits str around matches of the given pattern.    :param str: a string expression to split    :param pattern: a string representing a regular expression. The regex string should be        a Java regular expression.    :param limit: an integer which controls the number of times `pattern` is applied.        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the                         resulting array's last entry will contain all input beyond the last                         matched pattern.        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting                          array can be of any size.    .. versionchanged:: 3.0       `split` now takes an optional `limit` field. If not provided, default limit value is -1.    >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])    >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()    [Row(s=[u'one', u'twoBthreeC'])]    >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()    [Row(s=[u'one', u'two', u'three', u''])]
*******__*******
Code:def regexp_extract(str, pattern, idx):\n    r"""Extract a specific group matched by a Java regex, from the specified string column.\n    If the regex did not match, or the specified group did not match, an empty string is returned.\n\n    >>> df = spark.createDataFrame([('100-200',)], ['str'])\n    >>> df.select(regexp_extract('str', r'(\d+)-(\d+)', 1).alias('d')).collect()\n    [Row(d=u'100')]\n    >>> df = spark.createDataFrame([('foo',)], ['str'])\n    >>> df.select(regexp_extract('str', r'(\d+)', 1).alias('d')).collect()\n    [Row(d=u'')]\n    >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n    >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n    [Row(d=u'')]\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.regexp_extract(_to_java_column(str), pattern, idx)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'regexp_extract', '(', 'str', ',', 'pattern', ',', 'idx', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'regexp_extract', '(', '_to_java_column', '(', 'str', ')', ',', 'pattern', ',', 'idx', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: r"""Extract a specific group matched by a Java regex, from the specified string column.    If the regex did not match, or the specified group did not match, an empty string is returned.    >>> df = spark.createDataFrame([('100-200',)], ['str'])    >>> df.select(regexp_extract('str', r'(\d+)-(\d+)', 1).alias('d')).collect()    [Row(d=u'100')]    >>> df = spark.createDataFrame([('foo',)], ['str'])    >>> df.select(regexp_extract('str', r'(\d+)', 1).alias('d')).collect()    [Row(d=u'')]    >>> df = spark.createDataFrame([('aaaac',)], ['str'])    >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()    [Row(d=u'')]
*******__*******
Code:def regexp_replace(str, pattern, replacement):\n    r"""Replace all substrings of the specified string value that match regexp with rep.\n\n    >>> df = spark.createDataFrame([('100-200',)], ['str'])\n    >>> df.select(regexp_replace('str', r'(\d+)', '--').alias('d')).collect()\n    [Row(d=u'-----')]\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.regexp_replace(_to_java_column(str), pattern, replacement)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'regexp_replace', '(', 'str', ',', 'pattern', ',', 'replacement', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'regexp_replace', '(', '_to_java_column', '(', 'str', ')', ',', 'pattern', ',', 'replacement', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: r"""Replace all substrings of the specified string value that match regexp with rep.    >>> df = spark.createDataFrame([('100-200',)], ['str'])    >>> df.select(regexp_replace('str', r'(\d+)', '--').alias('d')).collect()    [Row(d=u'-----')]
*******__*******
Code:def translate(srcCol, matching, replace):\n    """A function translate any character in the `srcCol` by a character in `matching`.\n    The characters in `replace` is corresponding to the characters in `matching`.\n    The translate will happen when any character in the string matching with the character\n    in the `matching`.\n\n    >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', "rnlt", "123") \\\n    ...     .alias('r')).collect()\n    [Row(r=u'1a2s3ae')]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.translate(_to_java_column(srcCol), matching, replace))
Language: python
Code Tokens: ['def', 'translate', '(', 'srcCol', ',', 'matching', ',', 'replace', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'translate', '(', '_to_java_column', '(', 'srcCol', ')', ',', 'matching', ',', 'replace', ')', ')']
Docstring: A function translate any character in the `srcCol` by a character in `matching`.    The characters in `replace` is corresponding to the characters in `matching`.    The translate will happen when any character in the string matching with the character    in the `matching`.    >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', "rnlt", "123") \\    ...     .alias('r')).collect()    [Row(r=u'1a2s3ae')]
*******__*******
Code:def arrays_overlap(a1, a2):\n    """\n    Collection function: returns true if the arrays contain any common non-null element; if not,\n    returns null if both the arrays are non-empty and any of them contains a null element; returns\n    false otherwise.\n\n    >>> df = spark.createDataFrame([(["a", "b"], ["b", "c"]), (["a"], ["b", "c"])], ['x', 'y'])\n    >>> df.select(arrays_overlap(df.x, df.y).alias("overlap")).collect()\n    [Row(overlap=True), Row(overlap=False)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.arrays_overlap(_to_java_column(a1), _to_java_column(a2)))
Language: python
Code Tokens: ['def', 'arrays_overlap', '(', 'a1', ',', 'a2', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'arrays_overlap', '(', '_to_java_column', '(', 'a1', ')', ',', '_to_java_column', '(', 'a2', ')', ')', ')']
Docstring: Collection function: returns true if the arrays contain any common non-null element; if not,    returns null if both the arrays are non-empty and any of them contains a null element; returns    false otherwise.    >>> df = spark.createDataFrame([(["a", "b"], ["b", "c"]), (["a"], ["b", "c"])], ['x', 'y'])    >>> df.select(arrays_overlap(df.x, df.y).alias("overlap")).collect()    [Row(overlap=True), Row(overlap=False)]
*******__*******
Code:def slice(x, start, length):\n    """\n    Collection function: returns an array containing  all the elements in `x` from index `start`\n    (or starting from the end if `start` is negative) with the specified `length`.\n    >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n    >>> df.select(slice(df.x, 2, 2).alias("sliced")).collect()\n    [Row(sliced=[2, 3]), Row(sliced=[5])]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.slice(_to_java_column(x), start, length))
Language: python
Code Tokens: ['def', 'slice', '(', 'x', ',', 'start', ',', 'length', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'slice', '(', '_to_java_column', '(', 'x', ')', ',', 'start', ',', 'length', ')', ')']
Docstring: Collection function: returns an array containing  all the elements in `x` from index `start`    (or starting from the end if `start` is negative) with the specified `length`.    >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])    >>> df.select(slice(df.x, 2, 2).alias("sliced")).collect()    [Row(sliced=[2, 3]), Row(sliced=[5])]
*******__*******
Code:def array_join(col, delimiter, null_replacement=None):\n    """\n    Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n    `null_replacement` if set, otherwise they are ignored.\n\n    >>> df = spark.createDataFrame([(["a", "b", "c"],), (["a", None],)], ['data'])\n    >>> df.select(array_join(df.data, ",").alias("joined")).collect()\n    [Row(joined=u'a,b,c'), Row(joined=u'a')]\n    >>> df.select(array_join(df.data, ",", "NULL").alias("joined")).collect()\n    [Row(joined=u'a,b,c'), Row(joined=u'a,NULL')]\n    """\n    sc = SparkContext._active_spark_context\n    if null_replacement is None:\n        return Column(sc._jvm.functions.array_join(_to_java_column(col), delimiter))\n    else:\n        return Column(sc._jvm.functions.array_join(\n            _to_java_column(col), delimiter, null_replacement))
Language: python
Code Tokens: ['def', 'array_join', '(', 'col', ',', 'delimiter', ',', 'null_replacement', '=', 'None', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'null_replacement', 'is', 'None', ':', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'array_join', '(', '_to_java_column', '(', 'col', ')', ',', 'delimiter', ')', ')', 'else', ':', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'array_join', '(', '_to_java_column', '(', 'col', ')', ',', 'delimiter', ',', 'null_replacement', ')', ')']
Docstring: Concatenates the elements of `column` using the `delimiter`. Null values are replaced with    `null_replacement` if set, otherwise they are ignored.    >>> df = spark.createDataFrame([(["a", "b", "c"],), (["a", None],)], ['data'])    >>> df.select(array_join(df.data, ",").alias("joined")).collect()    [Row(joined=u'a,b,c'), Row(joined=u'a')]    >>> df.select(array_join(df.data, ",", "NULL").alias("joined")).collect()    [Row(joined=u'a,b,c'), Row(joined=u'a,NULL')]
*******__*******
Code:def concat(*cols):\n    """\n    Concatenates multiple input columns together into a single column.\n    The function works with strings, binary and compatible array columns.\n\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n    >>> df.select(concat(df.s, df.d).alias('s')).collect()\n    [Row(s=u'abcd123')]\n\n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n    >>> df.select(concat(df.a, df.b, df.c).alias("arr")).collect()\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.concat(_to_seq(sc, cols, _to_java_column)))
Language: python
Code Tokens: ['def', 'concat', '(', '*', 'cols', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'concat', '(', '_to_seq', '(', 'sc', ',', 'cols', ',', '_to_java_column', ')', ')', ')']
Docstring: Concatenates multiple input columns together into a single column.    The function works with strings, binary and compatible array columns.    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])    >>> df.select(concat(df.s, df.d).alias('s')).collect()    [Row(s=u'abcd123')]    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])    >>> df.select(concat(df.a, df.b, df.c).alias("arr")).collect()    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]
*******__*******
Code:def array_position(col, value):\n    """\n    Collection function: Locates the position of the first occurrence of the given value\n    in the given array. Returns null if either of the arguments are null.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if the given\n        value could not be found in the array.\n\n    >>> df = spark.createDataFrame([(["c", "b", "a"],), ([],)], ['data'])\n    >>> df.select(array_position(df.data, "a")).collect()\n    [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.array_position(_to_java_column(col), value))
Language: python
Code Tokens: ['def', 'array_position', '(', 'col', ',', 'value', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'array_position', '(', '_to_java_column', '(', 'col', ')', ',', 'value', ')', ')']
Docstring: Collection function: Locates the position of the first occurrence of the given value    in the given array. Returns null if either of the arguments are null.    .. note:: The position is not zero based, but 1 based index. Returns 0 if the given        value could not be found in the array.    >>> df = spark.createDataFrame([(["c", "b", "a"],), ([],)], ['data'])    >>> df.select(array_position(df.data, "a")).collect()    [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]
*******__*******
Code:def element_at(col, extraction):\n    """\n    Collection function: Returns element of array at given index in extraction if col is array.\n    Returns value for the given key in extraction if col is map.\n\n    :param col: name of column containing array or map\n    :param extraction: index to check for in array or key to check for in map\n\n    .. note:: The position is not zero based, but 1 based index.\n\n    >>> df = spark.createDataFrame([(["a", "b", "c"],), ([],)], ['data'])\n    >>> df.select(element_at(df.data, 1)).collect()\n    [Row(element_at(data, 1)=u'a'), Row(element_at(data, 1)=None)]\n\n    >>> df = spark.createDataFrame([({"a": 1.0, "b": 2.0},), ({},)], ['data'])\n    >>> df.select(element_at(df.data, "a")).collect()\n    [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.element_at(_to_java_column(col), extraction))
Language: python
Code Tokens: ['def', 'element_at', '(', 'col', ',', 'extraction', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'element_at', '(', '_to_java_column', '(', 'col', ')', ',', 'extraction', ')', ')']
Docstring: Collection function: Returns element of array at given index in extraction if col is array.    Returns value for the given key in extraction if col is map.    :param col: name of column containing array or map    :param extraction: index to check for in array or key to check for in map    .. note:: The position is not zero based, but 1 based index.    >>> df = spark.createDataFrame([(["a", "b", "c"],), ([],)], ['data'])    >>> df.select(element_at(df.data, 1)).collect()    [Row(element_at(data, 1)=u'a'), Row(element_at(data, 1)=None)]    >>> df = spark.createDataFrame([({"a": 1.0, "b": 2.0},), ({},)], ['data'])    >>> df.select(element_at(df.data, "a")).collect()    [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]
*******__*******
Code:def array_remove(col, element):\n    """\n    Collection function: Remove all elements that equal to element from the given array.\n\n    :param col: name of column containing array\n    :param element: element to be removed from the array\n\n    >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n    >>> df.select(array_remove(df.data, 1)).collect()\n    [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n    """\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.array_remove(_to_java_column(col), element))
Language: python
Code Tokens: ['def', 'array_remove', '(', 'col', ',', 'element', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'return', 'Column', '(', 'sc', '.', '_jvm', '.', 'functions', '.', 'array_remove', '(', '_to_java_column', '(', 'col', ')', ',', 'element', ')', ')']
Docstring: Collection function: Remove all elements that equal to element from the given array.    :param col: name of column containing array    :param element: element to be removed from the array    >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])    >>> df.select(array_remove(df.data, 1)).collect()    [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]
*******__*******
Code:def explode(col):\n    """\n    Returns a new row for each element in the given array or map.\n    Uses the default column name `col` for elements in the array and\n    `key` and `value` for elements in the map unless specified otherwise.\n\n    >>> from pyspark.sql import Row\n    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={"a": "b"})])\n    >>> eDF.select(explode(eDF.intlist).alias("anInt")).collect()\n    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n\n    >>> eDF.select(explode(eDF.mapfield).alias("key", "value")).show()\n    +---+-----+\n    |key|value|\n    +---+-----+\n    |  a|    b|\n    +---+-----+\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.explode(_to_java_column(col))\n    return Column(jc)
Language: python
Code Tokens: ['def', 'explode', '(', 'col', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'explode', '(', '_to_java_column', '(', 'col', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Returns a new row for each element in the given array or map.    Uses the default column name `col` for elements in the array and    `key` and `value` for elements in the map unless specified otherwise.    >>> from pyspark.sql import Row    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={"a": "b"})])    >>> eDF.select(explode(eDF.intlist).alias("anInt")).collect()    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]    >>> eDF.select(explode(eDF.mapfield).alias("key", "value")).show()    +---+-----+    |key|value|    +---+-----+    |  a|    b|    +---+-----+
*******__*******
Code:def get_json_object(col, path):\n    """\n    Extracts json object from a json string based on json path specified, and returns json string\n    of the extracted json object. It will return null if the input json string is invalid.\n\n    :param col: string column in json format\n    :param path: path to the json object to extract\n\n    >>> data = [("1", '''{"f1": "value1", "f2": "value2"}'''), ("2", '''{"f1": "value12"}''')]\n    >>> df = spark.createDataFrame(data, ("key", "jstring"))\n    >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias("c0"), \\\n    ...                   get_json_object(df.jstring, '$.f2').alias("c1") ).collect()\n    [Row(key=u'1', c0=u'value1', c1=u'value2'), Row(key=u'2', c0=u'value12', c1=None)]\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.get_json_object(_to_java_column(col), path)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'get_json_object', '(', 'col', ',', 'path', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'get_json_object', '(', '_to_java_column', '(', 'col', ')', ',', 'path', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Extracts json object from a json string based on json path specified, and returns json string    of the extracted json object. It will return null if the input json string is invalid.    :param col: string column in json format    :param path: path to the json object to extract    >>> data = [("1", '''{"f1": "value1", "f2": "value2"}'''), ("2", '''{"f1": "value12"}''')]    >>> df = spark.createDataFrame(data, ("key", "jstring"))    >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias("c0"), \\    ...                   get_json_object(df.jstring, '$.f2').alias("c1") ).collect()    [Row(key=u'1', c0=u'value1', c1=u'value2'), Row(key=u'2', c0=u'value12', c1=None)]
*******__*******
Code:def json_tuple(col, *fields):\n    """Creates a new row for a json column according to the given field names.\n\n    :param col: string column in json format\n    :param fields: list of fields to extract\n\n    >>> data = [("1", '''{"f1": "value1", "f2": "value2"}'''), ("2", '''{"f1": "value12"}''')]\n    >>> df = spark.createDataFrame(data, ("key", "jstring"))\n    >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n    [Row(key=u'1', c0=u'value1', c1=u'value2'), Row(key=u'2', c0=u'value12', c1=None)]\n    """\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.json_tuple(_to_java_column(col), _to_seq(sc, fields))\n    return Column(jc)
Language: python
Code Tokens: ['def', 'json_tuple', '(', 'col', ',', '*', 'fields', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'json_tuple', '(', '_to_java_column', '(', 'col', ')', ',', '_to_seq', '(', 'sc', ',', 'fields', ')', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Creates a new row for a json column according to the given field names.    :param col: string column in json format    :param fields: list of fields to extract    >>> data = [("1", '''{"f1": "value1", "f2": "value2"}'''), ("2", '''{"f1": "value12"}''')]    >>> df = spark.createDataFrame(data, ("key", "jstring"))    >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()    [Row(key=u'1', c0=u'value1', c1=u'value2'), Row(key=u'2', c0=u'value12', c1=None)]
*******__*******
Code:def from_json(col, schema, options={}):\n    """\n    Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\n    as keys type, :class:`StructType` or :class:`ArrayType` with\n    the specified schema. Returns `null`, in the case of an unparseable string.\n\n    :param col: string column in json format\n    :param schema: a StructType or ArrayType of StructType to use when parsing the json column.\n    :param options: options to control parsing. accepts the same options as the json datasource\n\n    .. note:: Since Spark 2.3, the DDL-formatted string or a JSON format string is also\n              supported for ``schema``.\n\n    >>> from pyspark.sql.types import *\n    >>> data = [(1, '''{"a": 1}''')]\n    >>> schema = StructType([StructField("a", IntegerType())])\n    >>> df = spark.createDataFrame(data, ("key", "value"))\n    >>> df.select(from_json(df.value, schema).alias("json")).collect()\n    [Row(json=Row(a=1))]\n    >>> df.select(from_json(df.value, "a INT").alias("json")).collect()\n    [Row(json=Row(a=1))]\n    >>> df.select(from_json(df.value, "MAP<STRING,INT>").alias("json")).collect()\n    [Row(json={u'a': 1})]\n    >>> data = [(1, '''[{"a": 1}]''')]\n    >>> schema = ArrayType(StructType([StructField("a", IntegerType())]))\n    >>> df = spark.createDataFrame(data, ("key", "value"))\n    >>> df.select(from_json(df.value, schema).alias("json")).collect()\n    [Row(json=[Row(a=1)])]\n    >>> schema = schema_of_json(lit('''{"a": 0}'''))\n    >>> df.select(from_json(df.value, schema).alias("json")).collect()\n    [Row(json=Row(a=None))]\n    >>> data = [(1, '''[1, 2, 3]''')]\n    >>> schema = ArrayType(IntegerType())\n    >>> df = spark.createDataFrame(data, ("key", "value"))\n    >>> df.select(from_json(df.value, schema).alias("json")).collect()\n    [Row(json=[1, 2, 3])]\n    """\n\n    sc = SparkContext._active_spark_context\n    if isinstance(schema, DataType):\n        schema = schema.json()\n    elif isinstance(schema, Column):\n        schema = _to_java_column(schema)\n    jc = sc._jvm.functions.from_json(_to_java_column(col), schema, options)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'from_json', '(', 'col', ',', 'schema', ',', 'options', '=', '{', '}', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'isinstance', '(', 'schema', ',', 'DataType', ')', ':', 'schema', '=', 'schema', '.', 'json', '(', ')', 'elif', 'isinstance', '(', 'schema', ',', 'Column', ')', ':', 'schema', '=', '_to_java_column', '(', 'schema', ')', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'from_json', '(', '_to_java_column', '(', 'col', ')', ',', 'schema', ',', 'options', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`    as keys type, :class:`StructType` or :class:`ArrayType` with    the specified schema. Returns `null`, in the case of an unparseable string.    :param col: string column in json format    :param schema: a StructType or ArrayType of StructType to use when parsing the json column.    :param options: options to control parsing. accepts the same options as the json datasource    .. note:: Since Spark 2.3, the DDL-formatted string or a JSON format string is also              supported for ``schema``.    >>> from pyspark.sql.types import *    >>> data = [(1, '''{"a": 1}''')]    >>> schema = StructType([StructField("a", IntegerType())])    >>> df = spark.createDataFrame(data, ("key", "value"))    >>> df.select(from_json(df.value, schema).alias("json")).collect()    [Row(json=Row(a=1))]    >>> df.select(from_json(df.value, "a INT").alias("json")).collect()    [Row(json=Row(a=1))]    >>> df.select(from_json(df.value, "MAP<STRING,INT>").alias("json")).collect()    [Row(json={u'a': 1})]    >>> data = [(1, '''[{"a": 1}]''')]    >>> schema = ArrayType(StructType([StructField("a", IntegerType())]))    >>> df = spark.createDataFrame(data, ("key", "value"))    >>> df.select(from_json(df.value, schema).alias("json")).collect()    [Row(json=[Row(a=1)])]    >>> schema = schema_of_json(lit('''{"a": 0}'''))    >>> df.select(from_json(df.value, schema).alias("json")).collect()    [Row(json=Row(a=None))]    >>> data = [(1, '''[1, 2, 3]''')]    >>> schema = ArrayType(IntegerType())    >>> df = spark.createDataFrame(data, ("key", "value"))    >>> df.select(from_json(df.value, schema).alias("json")).collect()    [Row(json=[1, 2, 3])]
*******__*******
Code:def schema_of_json(json, options={}):\n    """\n    Parses a JSON string and infers its schema in DDL format.\n\n    :param json: a JSON string or a string literal containing a JSON string.\n    :param options: options to control parsing. accepts the same options as the JSON datasource\n\n    .. versionchanged:: 3.0\n       It accepts `options` parameter to control schema inferring.\n\n    >>> df = spark.range(1)\n    >>> df.select(schema_of_json(lit('{"a": 0}')).alias("json")).collect()\n    [Row(json=u'struct<a:bigint>')]\n    >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n    >>> df.select(schema.alias("json")).collect()\n    [Row(json=u'struct<a:bigint>')]\n    """\n    if isinstance(json, basestring):\n        col = _create_column_from_literal(json)\n    elif isinstance(json, Column):\n        col = _to_java_column(json)\n    else:\n        raise TypeError("schema argument should be a column or string")\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.schema_of_json(col, options)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'schema_of_json', '(', 'json', ',', 'options', '=', '{', '}', ')', ':', 'if', 'isinstance', '(', 'json', ',', 'basestring', ')', ':', 'col', '=', '_create_column_from_literal', '(', 'json', ')', 'elif', 'isinstance', '(', 'json', ',', 'Column', ')', ':', 'col', '=', '_to_java_column', '(', 'json', ')', 'else', ':', 'raise', 'TypeError', '(', '"schema argument should be a column or string"', ')', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'schema_of_json', '(', 'col', ',', 'options', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Parses a JSON string and infers its schema in DDL format.    :param json: a JSON string or a string literal containing a JSON string.    :param options: options to control parsing. accepts the same options as the JSON datasource    .. versionchanged:: 3.0       It accepts `options` parameter to control schema inferring.    >>> df = spark.range(1)    >>> df.select(schema_of_json(lit('{"a": 0}')).alias("json")).collect()    [Row(json=u'struct<a:bigint>')]    >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})    >>> df.select(schema.alias("json")).collect()    [Row(json=u'struct<a:bigint>')]
*******__*******
Code:def schema_of_csv(csv, options={}):\n    """\n    Parses a CSV string and infers its schema in DDL format.\n\n    :param col: a CSV string or a string literal containing a CSV string.\n    :param options: options to control parsing. accepts the same options as the CSV datasource\n\n    >>> df = spark.range(1)\n    >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias("csv")).collect()\n    [Row(csv=u'struct<_c0:int,_c1:string>')]\n    >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias("csv")).collect()\n    [Row(csv=u'struct<_c0:int,_c1:string>')]\n    """\n    if isinstance(csv, basestring):\n        col = _create_column_from_literal(csv)\n    elif isinstance(csv, Column):\n        col = _to_java_column(csv)\n    else:\n        raise TypeError("schema argument should be a column or string")\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.schema_of_csv(col, options)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'schema_of_csv', '(', 'csv', ',', 'options', '=', '{', '}', ')', ':', 'if', 'isinstance', '(', 'csv', ',', 'basestring', ')', ':', 'col', '=', '_create_column_from_literal', '(', 'csv', ')', 'elif', 'isinstance', '(', 'csv', ',', 'Column', ')', ':', 'col', '=', '_to_java_column', '(', 'csv', ')', 'else', ':', 'raise', 'TypeError', '(', '"schema argument should be a column or string"', ')', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'schema_of_csv', '(', 'col', ',', 'options', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Parses a CSV string and infers its schema in DDL format.    :param col: a CSV string or a string literal containing a CSV string.    :param options: options to control parsing. accepts the same options as the CSV datasource    >>> df = spark.range(1)    >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias("csv")).collect()    [Row(csv=u'struct<_c0:int,_c1:string>')]    >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias("csv")).collect()    [Row(csv=u'struct<_c0:int,_c1:string>')]
*******__*******
Code:def to_csv(col, options={}):\n    """\n    Converts a column containing a :class:`StructType` into a CSV string.\n    Throws an exception, in the case of an unsupported type.\n\n    :param col: name of column containing a struct.\n    :param options: options to control converting. accepts the same options as the CSV datasource.\n\n    >>> from pyspark.sql import Row\n    >>> data = [(1, Row(name='Alice', age=2))]\n    >>> df = spark.createDataFrame(data, ("key", "value"))\n    >>> df.select(to_csv(df.value).alias("csv")).collect()\n    [Row(csv=u'2,Alice')]\n    """\n\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.to_csv(_to_java_column(col), options)\n    return Column(jc)
Language: python
Code Tokens: ['def', 'to_csv', '(', 'col', ',', 'options', '=', '{', '}', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'sc', '.', '_jvm', '.', 'functions', '.', 'to_csv', '(', '_to_java_column', '(', 'col', ')', ',', 'options', ')', 'return', 'Column', '(', 'jc', ')']
Docstring: Converts a column containing a :class:`StructType` into a CSV string.    Throws an exception, in the case of an unsupported type.    :param col: name of column containing a struct.    :param options: options to control converting. accepts the same options as the CSV datasource.    >>> from pyspark.sql import Row    >>> data = [(1, Row(name='Alice', age=2))]    >>> df = spark.createDataFrame(data, ("key", "value"))    >>> df.select(to_csv(df.value).alias("csv")).collect()    [Row(csv=u'2,Alice')]
*******__*******
